{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heliocarrara/Python-IA/blob/main/redesneurais_classificacao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY7yOKvocMjz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('emprego.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "S5gz5OD_cXMy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "49fc78a9-1099-4032-b3db-e3244f99f2e1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sl_no gender  ssc_p    ssc_b  hsc_p    hsc_b     hsc_s  degree_p  \\\n",
              "0      1      M  67.00   Others  91.00   Others  Commerce     58.00   \n",
              "1      2      M  79.33  Central  78.33   Others   Science     77.48   \n",
              "2      3      M  65.00  Central  68.00  Central      Arts     64.00   \n",
              "3      4      M  56.00  Central  52.00  Central   Science     52.00   \n",
              "4      5      M  85.80  Central  73.60  Central  Commerce     73.30   \n",
              "\n",
              "    degree_t workex  etest_p specialisation  mba_p      status    salary  \n",
              "0   Sci&Tech     No     55.0         Mkt&HR  58.80      Placed  270000.0  \n",
              "1   Sci&Tech    Yes     86.5        Mkt&Fin  66.28      Placed  200000.0  \n",
              "2  Comm&Mgmt     No     75.0        Mkt&Fin  57.80      Placed  250000.0  \n",
              "3   Sci&Tech     No     66.0         Mkt&HR  59.43  Not Placed       NaN  \n",
              "4  Comm&Mgmt     No     96.8        Mkt&Fin  55.50      Placed  425000.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-026da478-af0a-44db-ad61-c09dc948bdb0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sl_no</th>\n",
              "      <th>gender</th>\n",
              "      <th>ssc_p</th>\n",
              "      <th>ssc_b</th>\n",
              "      <th>hsc_p</th>\n",
              "      <th>hsc_b</th>\n",
              "      <th>hsc_s</th>\n",
              "      <th>degree_p</th>\n",
              "      <th>degree_t</th>\n",
              "      <th>workex</th>\n",
              "      <th>etest_p</th>\n",
              "      <th>specialisation</th>\n",
              "      <th>mba_p</th>\n",
              "      <th>status</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>M</td>\n",
              "      <td>67.00</td>\n",
              "      <td>Others</td>\n",
              "      <td>91.00</td>\n",
              "      <td>Others</td>\n",
              "      <td>Commerce</td>\n",
              "      <td>58.00</td>\n",
              "      <td>Sci&amp;Tech</td>\n",
              "      <td>No</td>\n",
              "      <td>55.0</td>\n",
              "      <td>Mkt&amp;HR</td>\n",
              "      <td>58.80</td>\n",
              "      <td>Placed</td>\n",
              "      <td>270000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>M</td>\n",
              "      <td>79.33</td>\n",
              "      <td>Central</td>\n",
              "      <td>78.33</td>\n",
              "      <td>Others</td>\n",
              "      <td>Science</td>\n",
              "      <td>77.48</td>\n",
              "      <td>Sci&amp;Tech</td>\n",
              "      <td>Yes</td>\n",
              "      <td>86.5</td>\n",
              "      <td>Mkt&amp;Fin</td>\n",
              "      <td>66.28</td>\n",
              "      <td>Placed</td>\n",
              "      <td>200000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>M</td>\n",
              "      <td>65.00</td>\n",
              "      <td>Central</td>\n",
              "      <td>68.00</td>\n",
              "      <td>Central</td>\n",
              "      <td>Arts</td>\n",
              "      <td>64.00</td>\n",
              "      <td>Comm&amp;Mgmt</td>\n",
              "      <td>No</td>\n",
              "      <td>75.0</td>\n",
              "      <td>Mkt&amp;Fin</td>\n",
              "      <td>57.80</td>\n",
              "      <td>Placed</td>\n",
              "      <td>250000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>M</td>\n",
              "      <td>56.00</td>\n",
              "      <td>Central</td>\n",
              "      <td>52.00</td>\n",
              "      <td>Central</td>\n",
              "      <td>Science</td>\n",
              "      <td>52.00</td>\n",
              "      <td>Sci&amp;Tech</td>\n",
              "      <td>No</td>\n",
              "      <td>66.0</td>\n",
              "      <td>Mkt&amp;HR</td>\n",
              "      <td>59.43</td>\n",
              "      <td>Not Placed</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>M</td>\n",
              "      <td>85.80</td>\n",
              "      <td>Central</td>\n",
              "      <td>73.60</td>\n",
              "      <td>Central</td>\n",
              "      <td>Commerce</td>\n",
              "      <td>73.30</td>\n",
              "      <td>Comm&amp;Mgmt</td>\n",
              "      <td>No</td>\n",
              "      <td>96.8</td>\n",
              "      <td>Mkt&amp;Fin</td>\n",
              "      <td>55.50</td>\n",
              "      <td>Placed</td>\n",
              "      <td>425000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-026da478-af0a-44db-ad61-c09dc948bdb0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-026da478-af0a-44db-ad61-c09dc948bdb0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-026da478-af0a-44db-ad61-c09dc948bdb0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a2229814-c800-446b-abc3-c065dd1118e1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2229814-c800-446b-abc3-c065dd1118e1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a2229814-c800-446b-abc3-c065dd1118e1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 215,\n  \"fields\": [\n    {\n      \"column\": \"sl_no\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62,\n        \"min\": 1,\n        \"max\": 215,\n        \"num_unique_values\": 215,\n        \"samples\": [\n          201,\n          213,\n          139\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"F\",\n          \"M\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ssc_p\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.827205398231452,\n        \"min\": 40.89,\n        \"max\": 89.4,\n        \"num_unique_values\": 103,\n        \"samples\": [\n          74.0,\n          73.96\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ssc_b\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Central\",\n          \"Others\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hsc_p\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.89750915750298,\n        \"min\": 37.0,\n        \"max\": 97.7,\n        \"num_unique_values\": 97,\n        \"samples\": [\n          82.0,\n          73.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hsc_b\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Central\",\n          \"Others\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hsc_s\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Commerce\",\n          \"Science\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"degree_p\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.358743287339439,\n        \"min\": 50.0,\n        \"max\": 91.0,\n        \"num_unique_values\": 89,\n        \"samples\": [\n          71.72,\n          76.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"degree_t\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Sci&Tech\",\n          \"Comm&Mgmt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"workex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Yes\",\n          \"No\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"etest_p\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.275956401653835,\n        \"min\": 50.0,\n        \"max\": 98.0,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          93.4,\n          69.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"specialisation\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Mkt&Fin\",\n          \"Mkt&HR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mba_p\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.8333845806838,\n        \"min\": 51.21,\n        \"max\": 77.89,\n        \"num_unique_values\": 205,\n        \"samples\": [\n          64.66,\n          52.21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Not Placed\",\n          \"Placed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"salary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 93457.45241958875,\n        \"min\": 200000.0,\n        \"max\": 940000.0,\n        \"num_unique_values\": 45,\n        \"samples\": [\n          255000.0,\n          336000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p']]\n",
        "y = df['status']"
      ],
      "metadata": {
        "id": "fg51DUmycjUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=18)"
      ],
      "metadata": {
        "id": "5HIgQXP_cYHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "PDIqIIHZcxRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = MLPClassifier(\n",
        "    hidden_layer_sizes=(10, 5, 3, 2),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    learning_rate='constant',\n",
        "    learning_rate_init=0.0001,\n",
        "    max_iter=2000,\n",
        "    tol=1e-5,\n",
        "    shuffle=True,\n",
        "    random_state=20,\n",
        "    validation_fraction=0.2,\n",
        "    verbose=True)"
      ],
      "metadata": {
        "id": "yb3wk7OdcyNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "E41WHxcQc4Ay",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0918f649-3565-4391-e6df-8a9549c5bebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 9400, loss = 0.32422456\n",
            "Iteration 9401, loss = 0.32421498\n",
            "Iteration 9402, loss = 0.32419188\n",
            "Iteration 9403, loss = 0.32418216\n",
            "Iteration 9404, loss = 0.32416810\n",
            "Iteration 9405, loss = 0.32415435\n",
            "Iteration 9406, loss = 0.32414352\n",
            "Iteration 9407, loss = 0.32414091\n",
            "Iteration 9408, loss = 0.32411780\n",
            "Iteration 9409, loss = 0.32411119\n",
            "Iteration 9410, loss = 0.32410268\n",
            "Iteration 9411, loss = 0.32407642\n",
            "Iteration 9412, loss = 0.32407071\n",
            "Iteration 9413, loss = 0.32405163\n",
            "Iteration 9414, loss = 0.32403784\n",
            "Iteration 9415, loss = 0.32402341\n",
            "Iteration 9416, loss = 0.32401687\n",
            "Iteration 9417, loss = 0.32399272\n",
            "Iteration 9418, loss = 0.32397966\n",
            "Iteration 9419, loss = 0.32398623\n",
            "Iteration 9420, loss = 0.32396106\n",
            "Iteration 9421, loss = 0.32394001\n",
            "Iteration 9422, loss = 0.32394564\n",
            "Iteration 9423, loss = 0.32393239\n",
            "Iteration 9424, loss = 0.32390015\n",
            "Iteration 9425, loss = 0.32390820\n",
            "Iteration 9426, loss = 0.32388616\n",
            "Iteration 9427, loss = 0.32387300\n",
            "Iteration 9428, loss = 0.32385859\n",
            "Iteration 9429, loss = 0.32383717\n",
            "Iteration 9430, loss = 0.32384009\n",
            "Iteration 9431, loss = 0.32382964\n",
            "Iteration 9432, loss = 0.32380820\n",
            "Iteration 9433, loss = 0.32379440\n",
            "Iteration 9434, loss = 0.32378255\n",
            "Iteration 9435, loss = 0.32375825\n",
            "Iteration 9436, loss = 0.32374283\n",
            "Iteration 9437, loss = 0.32375037\n",
            "Iteration 9438, loss = 0.32372256\n",
            "Iteration 9439, loss = 0.32371328\n",
            "Iteration 9440, loss = 0.32370087\n",
            "Iteration 9441, loss = 0.32369419\n",
            "Iteration 9442, loss = 0.32368231\n",
            "Iteration 9443, loss = 0.32367066\n",
            "Iteration 9444, loss = 0.32365582\n",
            "Iteration 9445, loss = 0.32365482\n",
            "Iteration 9446, loss = 0.32364247\n",
            "Iteration 9447, loss = 0.32361937\n",
            "Iteration 9448, loss = 0.32360742\n",
            "Iteration 9449, loss = 0.32358873\n",
            "Iteration 9450, loss = 0.32357642\n",
            "Iteration 9451, loss = 0.32356103\n",
            "Iteration 9452, loss = 0.32354506\n",
            "Iteration 9453, loss = 0.32353432\n",
            "Iteration 9454, loss = 0.32351320\n",
            "Iteration 9455, loss = 0.32350600\n",
            "Iteration 9456, loss = 0.32349870\n",
            "Iteration 9457, loss = 0.32346705\n",
            "Iteration 9458, loss = 0.32345766\n",
            "Iteration 9459, loss = 0.32344574\n",
            "Iteration 9460, loss = 0.32343180\n",
            "Iteration 9461, loss = 0.32342450\n",
            "Iteration 9462, loss = 0.32341478\n",
            "Iteration 9463, loss = 0.32340056\n",
            "Iteration 9464, loss = 0.32339243\n",
            "Iteration 9465, loss = 0.32339256\n",
            "Iteration 9466, loss = 0.32336872\n",
            "Iteration 9467, loss = 0.32335102\n",
            "Iteration 9468, loss = 0.32334839\n",
            "Iteration 9469, loss = 0.32331508\n",
            "Iteration 9470, loss = 0.32332283\n",
            "Iteration 9471, loss = 0.32331946\n",
            "Iteration 9472, loss = 0.32329014\n",
            "Iteration 9473, loss = 0.32326835\n",
            "Iteration 9474, loss = 0.32326950\n",
            "Iteration 9475, loss = 0.32323726\n",
            "Iteration 9476, loss = 0.32322921\n",
            "Iteration 9477, loss = 0.32322046\n",
            "Iteration 9478, loss = 0.32319892\n",
            "Iteration 9479, loss = 0.32319303\n",
            "Iteration 9480, loss = 0.32318106\n",
            "Iteration 9481, loss = 0.32316685\n",
            "Iteration 9482, loss = 0.32314626\n",
            "Iteration 9483, loss = 0.32313434\n",
            "Iteration 9484, loss = 0.32312711\n",
            "Iteration 9485, loss = 0.32311394\n",
            "Iteration 9486, loss = 0.32309665\n",
            "Iteration 9487, loss = 0.32308165\n",
            "Iteration 9488, loss = 0.32306998\n",
            "Iteration 9489, loss = 0.32307137\n",
            "Iteration 9490, loss = 0.32305209\n",
            "Iteration 9491, loss = 0.32302687\n",
            "Iteration 9492, loss = 0.32303508\n",
            "Iteration 9493, loss = 0.32302229\n",
            "Iteration 9494, loss = 0.32299586\n",
            "Iteration 9495, loss = 0.32298894\n",
            "Iteration 9496, loss = 0.32297507\n",
            "Iteration 9497, loss = 0.32296994\n",
            "Iteration 9498, loss = 0.32294168\n",
            "Iteration 9499, loss = 0.32293378\n",
            "Iteration 9500, loss = 0.32295730\n",
            "Iteration 9501, loss = 0.32294879\n",
            "Iteration 9502, loss = 0.32290662\n",
            "Iteration 9503, loss = 0.32288351\n",
            "Iteration 9504, loss = 0.32290815\n",
            "Iteration 9505, loss = 0.32290709\n",
            "Iteration 9506, loss = 0.32286895\n",
            "Iteration 9507, loss = 0.32282712\n",
            "Iteration 9508, loss = 0.32283803\n",
            "Iteration 9509, loss = 0.32284817\n",
            "Iteration 9510, loss = 0.32282231\n",
            "Iteration 9511, loss = 0.32277072\n",
            "Iteration 9512, loss = 0.32276430\n",
            "Iteration 9513, loss = 0.32276381\n",
            "Iteration 9514, loss = 0.32273913\n",
            "Iteration 9515, loss = 0.32272880\n",
            "Iteration 9516, loss = 0.32270430\n",
            "Iteration 9517, loss = 0.32269742\n",
            "Iteration 9518, loss = 0.32269591\n",
            "Iteration 9519, loss = 0.32266708\n",
            "Iteration 9520, loss = 0.32265092\n",
            "Iteration 9521, loss = 0.32264474\n",
            "Iteration 9522, loss = 0.32262958\n",
            "Iteration 9523, loss = 0.32262526\n",
            "Iteration 9524, loss = 0.32261088\n",
            "Iteration 9525, loss = 0.32259522\n",
            "Iteration 9526, loss = 0.32257536\n",
            "Iteration 9527, loss = 0.32256710\n",
            "Iteration 9528, loss = 0.32255500\n",
            "Iteration 9529, loss = 0.32253308\n",
            "Iteration 9530, loss = 0.32252547\n",
            "Iteration 9531, loss = 0.32251575\n",
            "Iteration 9532, loss = 0.32250067\n",
            "Iteration 9533, loss = 0.32248283\n",
            "Iteration 9534, loss = 0.32247635\n",
            "Iteration 9535, loss = 0.32245950\n",
            "Iteration 9536, loss = 0.32244687\n",
            "Iteration 9537, loss = 0.32243507\n",
            "Iteration 9538, loss = 0.32241761\n",
            "Iteration 9539, loss = 0.32240635\n",
            "Iteration 9540, loss = 0.32239324\n",
            "Iteration 9541, loss = 0.32237014\n",
            "Iteration 9542, loss = 0.32236888\n",
            "Iteration 9543, loss = 0.32234588\n",
            "Iteration 9544, loss = 0.32234614\n",
            "Iteration 9545, loss = 0.32232516\n",
            "Iteration 9546, loss = 0.32232587\n",
            "Iteration 9547, loss = 0.32231362\n",
            "Iteration 9548, loss = 0.32228471\n",
            "Iteration 9549, loss = 0.32228809\n",
            "Iteration 9550, loss = 0.32225788\n",
            "Iteration 9551, loss = 0.32225413\n",
            "Iteration 9552, loss = 0.32223984\n",
            "Iteration 9553, loss = 0.32221865\n",
            "Iteration 9554, loss = 0.32220342\n",
            "Iteration 9555, loss = 0.32218886\n",
            "Iteration 9556, loss = 0.32218315\n",
            "Iteration 9557, loss = 0.32216806\n",
            "Iteration 9558, loss = 0.32215462\n",
            "Iteration 9559, loss = 0.32214347\n",
            "Iteration 9560, loss = 0.32213643\n",
            "Iteration 9561, loss = 0.32210702\n",
            "Iteration 9562, loss = 0.32209650\n",
            "Iteration 9563, loss = 0.32208448\n",
            "Iteration 9564, loss = 0.32207122\n",
            "Iteration 9565, loss = 0.32206272\n",
            "Iteration 9566, loss = 0.32204927\n",
            "Iteration 9567, loss = 0.32203689\n",
            "Iteration 9568, loss = 0.32202360\n",
            "Iteration 9569, loss = 0.32201264\n",
            "Iteration 9570, loss = 0.32199759\n",
            "Iteration 9571, loss = 0.32198322\n",
            "Iteration 9572, loss = 0.32196294\n",
            "Iteration 9573, loss = 0.32195384\n",
            "Iteration 9574, loss = 0.32193805\n",
            "Iteration 9575, loss = 0.32191971\n",
            "Iteration 9576, loss = 0.32191230\n",
            "Iteration 9577, loss = 0.32189532\n",
            "Iteration 9578, loss = 0.32189711\n",
            "Iteration 9579, loss = 0.32186891\n",
            "Iteration 9580, loss = 0.32185829\n",
            "Iteration 9581, loss = 0.32184547\n",
            "Iteration 9582, loss = 0.32183707\n",
            "Iteration 9583, loss = 0.32181886\n",
            "Iteration 9584, loss = 0.32179823\n",
            "Iteration 9585, loss = 0.32179538\n",
            "Iteration 9586, loss = 0.32178180\n",
            "Iteration 9587, loss = 0.32175813\n",
            "Iteration 9588, loss = 0.32178138\n",
            "Iteration 9589, loss = 0.32174309\n",
            "Iteration 9590, loss = 0.32173729\n",
            "Iteration 9591, loss = 0.32172025\n",
            "Iteration 9592, loss = 0.32169342\n",
            "Iteration 9593, loss = 0.32168220\n",
            "Iteration 9594, loss = 0.32167101\n",
            "Iteration 9595, loss = 0.32165063\n",
            "Iteration 9596, loss = 0.32164802\n",
            "Iteration 9597, loss = 0.32162720\n",
            "Iteration 9598, loss = 0.32161868\n",
            "Iteration 9599, loss = 0.32163170\n",
            "Iteration 9600, loss = 0.32161110\n",
            "Iteration 9601, loss = 0.32159420\n",
            "Iteration 9602, loss = 0.32156465\n",
            "Iteration 9603, loss = 0.32158011\n",
            "Iteration 9604, loss = 0.32154746\n",
            "Iteration 9605, loss = 0.32152742\n",
            "Iteration 9606, loss = 0.32153058\n",
            "Iteration 9607, loss = 0.32149956\n",
            "Iteration 9608, loss = 0.32148863\n",
            "Iteration 9609, loss = 0.32147420\n",
            "Iteration 9610, loss = 0.32145812\n",
            "Iteration 9611, loss = 0.32145607\n",
            "Iteration 9612, loss = 0.32143039\n",
            "Iteration 9613, loss = 0.32142934\n",
            "Iteration 9614, loss = 0.32139934\n",
            "Iteration 9615, loss = 0.32140319\n",
            "Iteration 9616, loss = 0.32137809\n",
            "Iteration 9617, loss = 0.32137226\n",
            "Iteration 9618, loss = 0.32135076\n",
            "Iteration 9619, loss = 0.32133833\n",
            "Iteration 9620, loss = 0.32132291\n",
            "Iteration 9621, loss = 0.32130674\n",
            "Iteration 9622, loss = 0.32130668\n",
            "Iteration 9623, loss = 0.32129254\n",
            "Iteration 9624, loss = 0.32126077\n",
            "Iteration 9625, loss = 0.32128162\n",
            "Iteration 9626, loss = 0.32124125\n",
            "Iteration 9627, loss = 0.32123540\n",
            "Iteration 9628, loss = 0.32121180\n",
            "Iteration 9629, loss = 0.32121550\n",
            "Iteration 9630, loss = 0.32118784\n",
            "Iteration 9631, loss = 0.32117853\n",
            "Iteration 9632, loss = 0.32118373\n",
            "Iteration 9633, loss = 0.32115392\n",
            "Iteration 9634, loss = 0.32116077\n",
            "Iteration 9635, loss = 0.32111909\n",
            "Iteration 9636, loss = 0.32112658\n",
            "Iteration 9637, loss = 0.32109179\n",
            "Iteration 9638, loss = 0.32108237\n",
            "Iteration 9639, loss = 0.32106028\n",
            "Iteration 9640, loss = 0.32106024\n",
            "Iteration 9641, loss = 0.32105044\n",
            "Iteration 9642, loss = 0.32102694\n",
            "Iteration 9643, loss = 0.32104127\n",
            "Iteration 9644, loss = 0.32100988\n",
            "Iteration 9645, loss = 0.32099784\n",
            "Iteration 9646, loss = 0.32098846\n",
            "Iteration 9647, loss = 0.32097776\n",
            "Iteration 9648, loss = 0.32095949\n",
            "Iteration 9649, loss = 0.32094652\n",
            "Iteration 9650, loss = 0.32093090\n",
            "Iteration 9651, loss = 0.32092980\n",
            "Iteration 9652, loss = 0.32088852\n",
            "Iteration 9653, loss = 0.32089004\n",
            "Iteration 9654, loss = 0.32087658\n",
            "Iteration 9655, loss = 0.32086575\n",
            "Iteration 9656, loss = 0.32087815\n",
            "Iteration 9657, loss = 0.32082161\n",
            "Iteration 9658, loss = 0.32082429\n",
            "Iteration 9659, loss = 0.32080754\n",
            "Iteration 9660, loss = 0.32079696\n",
            "Iteration 9661, loss = 0.32078302\n",
            "Iteration 9662, loss = 0.32075520\n",
            "Iteration 9663, loss = 0.32076829\n",
            "Iteration 9664, loss = 0.32073510\n",
            "Iteration 9665, loss = 0.32072983\n",
            "Iteration 9666, loss = 0.32071673\n",
            "Iteration 9667, loss = 0.32069119\n",
            "Iteration 9668, loss = 0.32068559\n",
            "Iteration 9669, loss = 0.32067423\n",
            "Iteration 9670, loss = 0.32066487\n",
            "Iteration 9671, loss = 0.32068152\n",
            "Iteration 9672, loss = 0.32062911\n",
            "Iteration 9673, loss = 0.32064081\n",
            "Iteration 9674, loss = 0.32061752\n",
            "Iteration 9675, loss = 0.32059259\n",
            "Iteration 9676, loss = 0.32058057\n",
            "Iteration 9677, loss = 0.32057518\n",
            "Iteration 9678, loss = 0.32057972\n",
            "Iteration 9679, loss = 0.32053743\n",
            "Iteration 9680, loss = 0.32054892\n",
            "Iteration 9681, loss = 0.32051713\n",
            "Iteration 9682, loss = 0.32049970\n",
            "Iteration 9683, loss = 0.32048719\n",
            "Iteration 9684, loss = 0.32046911\n",
            "Iteration 9685, loss = 0.32046136\n",
            "Iteration 9686, loss = 0.32045231\n",
            "Iteration 9687, loss = 0.32042951\n",
            "Iteration 9688, loss = 0.32041891\n",
            "Iteration 9689, loss = 0.32039579\n",
            "Iteration 9690, loss = 0.32038772\n",
            "Iteration 9691, loss = 0.32038282\n",
            "Iteration 9692, loss = 0.32035632\n",
            "Iteration 9693, loss = 0.32036286\n",
            "Iteration 9694, loss = 0.32032586\n",
            "Iteration 9695, loss = 0.32031352\n",
            "Iteration 9696, loss = 0.32029884\n",
            "Iteration 9697, loss = 0.32029301\n",
            "Iteration 9698, loss = 0.32028725\n",
            "Iteration 9699, loss = 0.32026211\n",
            "Iteration 9700, loss = 0.32025147\n",
            "Iteration 9701, loss = 0.32024970\n",
            "Iteration 9702, loss = 0.32022286\n",
            "Iteration 9703, loss = 0.32020812\n",
            "Iteration 9704, loss = 0.32020355\n",
            "Iteration 9705, loss = 0.32019768\n",
            "Iteration 9706, loss = 0.32016249\n",
            "Iteration 9707, loss = 0.32018603\n",
            "Iteration 9708, loss = 0.32014376\n",
            "Iteration 9709, loss = 0.32013930\n",
            "Iteration 9710, loss = 0.32011457\n",
            "Iteration 9711, loss = 0.32012740\n",
            "Iteration 9712, loss = 0.32008421\n",
            "Iteration 9713, loss = 0.32009386\n",
            "Iteration 9714, loss = 0.32008390\n",
            "Iteration 9715, loss = 0.32004543\n",
            "Iteration 9716, loss = 0.32008171\n",
            "Iteration 9717, loss = 0.32004245\n",
            "Iteration 9718, loss = 0.32004075\n",
            "Iteration 9719, loss = 0.32002748\n",
            "Iteration 9720, loss = 0.31997891\n",
            "Iteration 9721, loss = 0.32006662\n",
            "Iteration 9722, loss = 0.32002818\n",
            "Iteration 9723, loss = 0.31995418\n",
            "Iteration 9724, loss = 0.32000744\n",
            "Iteration 9725, loss = 0.31997173\n",
            "Iteration 9726, loss = 0.31991026\n",
            "Iteration 9727, loss = 0.31995013\n",
            "Iteration 9728, loss = 0.31991000\n",
            "Iteration 9729, loss = 0.31990032\n",
            "Iteration 9730, loss = 0.31996027\n",
            "Iteration 9731, loss = 0.31989821\n",
            "Iteration 9732, loss = 0.31984509\n",
            "Iteration 9733, loss = 0.31992330\n",
            "Iteration 9734, loss = 0.31988717\n",
            "Iteration 9735, loss = 0.31978235\n",
            "Iteration 9736, loss = 0.31985540\n",
            "Iteration 9737, loss = 0.31983838\n",
            "Iteration 9738, loss = 0.31974537\n",
            "Iteration 9739, loss = 0.31981283\n",
            "Iteration 9740, loss = 0.31980423\n",
            "Iteration 9741, loss = 0.31971112\n",
            "Iteration 9742, loss = 0.31975403\n",
            "Iteration 9743, loss = 0.31975247\n",
            "Iteration 9744, loss = 0.31966907\n",
            "Iteration 9745, loss = 0.31971304\n",
            "Iteration 9746, loss = 0.31972154\n",
            "Iteration 9747, loss = 0.31963308\n",
            "Iteration 9748, loss = 0.31963356\n",
            "Iteration 9749, loss = 0.31963989\n",
            "Iteration 9750, loss = 0.31958692\n",
            "Iteration 9751, loss = 0.31962462\n",
            "Iteration 9752, loss = 0.31959336\n",
            "Iteration 9753, loss = 0.31954597\n",
            "Iteration 9754, loss = 0.31955193\n",
            "Iteration 9755, loss = 0.31953913\n",
            "Iteration 9756, loss = 0.31950903\n",
            "Iteration 9757, loss = 0.31950627\n",
            "Iteration 9758, loss = 0.31951229\n",
            "Iteration 9759, loss = 0.31946957\n",
            "Iteration 9760, loss = 0.31947988\n",
            "Iteration 9761, loss = 0.31946635\n",
            "Iteration 9762, loss = 0.31943469\n",
            "Iteration 9763, loss = 0.31943131\n",
            "Iteration 9764, loss = 0.31944098\n",
            "Iteration 9765, loss = 0.31939443\n",
            "Iteration 9766, loss = 0.31939565\n",
            "Iteration 9767, loss = 0.31940936\n",
            "Iteration 9768, loss = 0.31935847\n",
            "Iteration 9769, loss = 0.31934129\n",
            "Iteration 9770, loss = 0.31937602\n",
            "Iteration 9771, loss = 0.31931202\n",
            "Iteration 9772, loss = 0.31930981\n",
            "Iteration 9773, loss = 0.31931675\n",
            "Iteration 9774, loss = 0.31930160\n",
            "Iteration 9775, loss = 0.31926928\n",
            "Iteration 9776, loss = 0.31928335\n",
            "Iteration 9777, loss = 0.31926192\n",
            "Iteration 9778, loss = 0.31923021\n",
            "Iteration 9779, loss = 0.31922855\n",
            "Iteration 9780, loss = 0.31921961\n",
            "Iteration 9781, loss = 0.31919426\n",
            "Iteration 9782, loss = 0.31918304\n",
            "Iteration 9783, loss = 0.31915771\n",
            "Iteration 9784, loss = 0.31916107\n",
            "Iteration 9785, loss = 0.31915574\n",
            "Iteration 9786, loss = 0.31912486\n",
            "Iteration 9787, loss = 0.31909628\n",
            "Iteration 9788, loss = 0.31909546\n",
            "Iteration 9789, loss = 0.31906043\n",
            "Iteration 9790, loss = 0.31907035\n",
            "Iteration 9791, loss = 0.31905234\n",
            "Iteration 9792, loss = 0.31903224\n",
            "Iteration 9793, loss = 0.31901388\n",
            "Iteration 9794, loss = 0.31901734\n",
            "Iteration 9795, loss = 0.31899498\n",
            "Iteration 9796, loss = 0.31899219\n",
            "Iteration 9797, loss = 0.31900189\n",
            "Iteration 9798, loss = 0.31897321\n",
            "Iteration 9799, loss = 0.31894752\n",
            "Iteration 9800, loss = 0.31894964\n",
            "Iteration 9801, loss = 0.31894142\n",
            "Iteration 9802, loss = 0.31889824\n",
            "Iteration 9803, loss = 0.31889329\n",
            "Iteration 9804, loss = 0.31889008\n",
            "Iteration 9805, loss = 0.31886494\n",
            "Iteration 9806, loss = 0.31883198\n",
            "Iteration 9807, loss = 0.31884342\n",
            "Iteration 9808, loss = 0.31885899\n",
            "Iteration 9809, loss = 0.31880675\n",
            "Iteration 9810, loss = 0.31878514\n",
            "Iteration 9811, loss = 0.31880086\n",
            "Iteration 9812, loss = 0.31879042\n",
            "Iteration 9813, loss = 0.31874831\n",
            "Iteration 9814, loss = 0.31871976\n",
            "Iteration 9815, loss = 0.31873642\n",
            "Iteration 9816, loss = 0.31875648\n",
            "Iteration 9817, loss = 0.31869765\n",
            "Iteration 9818, loss = 0.31868776\n",
            "Iteration 9819, loss = 0.31868796\n",
            "Iteration 9820, loss = 0.31867350\n",
            "Iteration 9821, loss = 0.31864234\n",
            "Iteration 9822, loss = 0.31862176\n",
            "Iteration 9823, loss = 0.31862874\n",
            "Iteration 9824, loss = 0.31860219\n",
            "Iteration 9825, loss = 0.31857562\n",
            "Iteration 9826, loss = 0.31859971\n",
            "Iteration 9827, loss = 0.31856462\n",
            "Iteration 9828, loss = 0.31854360\n",
            "Iteration 9829, loss = 0.31852839\n",
            "Iteration 9830, loss = 0.31853079\n",
            "Iteration 9831, loss = 0.31851137\n",
            "Iteration 9832, loss = 0.31850126\n",
            "Iteration 9833, loss = 0.31849231\n",
            "Iteration 9834, loss = 0.31847807\n",
            "Iteration 9835, loss = 0.31845907\n",
            "Iteration 9836, loss = 0.31844166\n",
            "Iteration 9837, loss = 0.31843184\n",
            "Iteration 9838, loss = 0.31841895\n",
            "Iteration 9839, loss = 0.31840604\n",
            "Iteration 9840, loss = 0.31838860\n",
            "Iteration 9841, loss = 0.31837847\n",
            "Iteration 9842, loss = 0.31837100\n",
            "Iteration 9843, loss = 0.31834139\n",
            "Iteration 9844, loss = 0.31832155\n",
            "Iteration 9845, loss = 0.31830259\n",
            "Iteration 9846, loss = 0.31830491\n",
            "Iteration 9847, loss = 0.31827548\n",
            "Iteration 9848, loss = 0.31828155\n",
            "Iteration 9849, loss = 0.31824983\n",
            "Iteration 9850, loss = 0.31824735\n",
            "Iteration 9851, loss = 0.31824606\n",
            "Iteration 9852, loss = 0.31822052\n",
            "Iteration 9853, loss = 0.31820165\n",
            "Iteration 9854, loss = 0.31820807\n",
            "Iteration 9855, loss = 0.31819073\n",
            "Iteration 9856, loss = 0.31819229\n",
            "Iteration 9857, loss = 0.31816387\n",
            "Iteration 9858, loss = 0.31815208\n",
            "Iteration 9859, loss = 0.31812843\n",
            "Iteration 9860, loss = 0.31811538\n",
            "Iteration 9861, loss = 0.31813383\n",
            "Iteration 9862, loss = 0.31813519\n",
            "Iteration 9863, loss = 0.31807807\n",
            "Iteration 9864, loss = 0.31807824\n",
            "Iteration 9865, loss = 0.31807700\n",
            "Iteration 9866, loss = 0.31805918\n",
            "Iteration 9867, loss = 0.31803427\n",
            "Iteration 9868, loss = 0.31800973\n",
            "Iteration 9869, loss = 0.31798711\n",
            "Iteration 9870, loss = 0.31802115\n",
            "Iteration 9871, loss = 0.31797496\n",
            "Iteration 9872, loss = 0.31797227\n",
            "Iteration 9873, loss = 0.31798564\n",
            "Iteration 9874, loss = 0.31792389\n",
            "Iteration 9875, loss = 0.31793498\n",
            "Iteration 9876, loss = 0.31790518\n",
            "Iteration 9877, loss = 0.31787430\n",
            "Iteration 9878, loss = 0.31788097\n",
            "Iteration 9879, loss = 0.31786059\n",
            "Iteration 9880, loss = 0.31786443\n",
            "Iteration 9881, loss = 0.31784813\n",
            "Iteration 9882, loss = 0.31780921\n",
            "Iteration 9883, loss = 0.31781574\n",
            "Iteration 9884, loss = 0.31778931\n",
            "Iteration 9885, loss = 0.31777964\n",
            "Iteration 9886, loss = 0.31777180\n",
            "Iteration 9887, loss = 0.31774121\n",
            "Iteration 9888, loss = 0.31772965\n",
            "Iteration 9889, loss = 0.31771514\n",
            "Iteration 9890, loss = 0.31769933\n",
            "Iteration 9891, loss = 0.31768840\n",
            "Iteration 9892, loss = 0.31768079\n",
            "Iteration 9893, loss = 0.31766340\n",
            "Iteration 9894, loss = 0.31764579\n",
            "Iteration 9895, loss = 0.31763283\n",
            "Iteration 9896, loss = 0.31761635\n",
            "Iteration 9897, loss = 0.31760946\n",
            "Iteration 9898, loss = 0.31759351\n",
            "Iteration 9899, loss = 0.31757973\n",
            "Iteration 9900, loss = 0.31756486\n",
            "Iteration 9901, loss = 0.31755684\n",
            "Iteration 9902, loss = 0.31753549\n",
            "Iteration 9903, loss = 0.31753579\n",
            "Iteration 9904, loss = 0.31752397\n",
            "Iteration 9905, loss = 0.31752027\n",
            "Iteration 9906, loss = 0.31749021\n",
            "Iteration 9907, loss = 0.31750238\n",
            "Iteration 9908, loss = 0.31745540\n",
            "Iteration 9909, loss = 0.31745365\n",
            "Iteration 9910, loss = 0.31742869\n",
            "Iteration 9911, loss = 0.31743181\n",
            "Iteration 9912, loss = 0.31740883\n",
            "Iteration 9913, loss = 0.31741262\n",
            "Iteration 9914, loss = 0.31742562\n",
            "Iteration 9915, loss = 0.31739329\n",
            "Iteration 9916, loss = 0.31735921\n",
            "Iteration 9917, loss = 0.31735206\n",
            "Iteration 9918, loss = 0.31737061\n",
            "Iteration 9919, loss = 0.31736553\n",
            "Iteration 9920, loss = 0.31729883\n",
            "Iteration 9921, loss = 0.31730333\n",
            "Iteration 9922, loss = 0.31729733\n",
            "Iteration 9923, loss = 0.31726167\n",
            "Iteration 9924, loss = 0.31724602\n",
            "Iteration 9925, loss = 0.31725592\n",
            "Iteration 9926, loss = 0.31723102\n",
            "Iteration 9927, loss = 0.31719575\n",
            "Iteration 9928, loss = 0.31720736\n",
            "Iteration 9929, loss = 0.31717766\n",
            "Iteration 9930, loss = 0.31715514\n",
            "Iteration 9931, loss = 0.31717523\n",
            "Iteration 9932, loss = 0.31716012\n",
            "Iteration 9933, loss = 0.31711583\n",
            "Iteration 9934, loss = 0.31711479\n",
            "Iteration 9935, loss = 0.31709437\n",
            "Iteration 9936, loss = 0.31708551\n",
            "Iteration 9937, loss = 0.31708508\n",
            "Iteration 9938, loss = 0.31706907\n",
            "Iteration 9939, loss = 0.31705183\n",
            "Iteration 9940, loss = 0.31703200\n",
            "Iteration 9941, loss = 0.31705076\n",
            "Iteration 9942, loss = 0.31702377\n",
            "Iteration 9943, loss = 0.31699307\n",
            "Iteration 9944, loss = 0.31697022\n",
            "Iteration 9945, loss = 0.31695975\n",
            "Iteration 9946, loss = 0.31697053\n",
            "Iteration 9947, loss = 0.31697254\n",
            "Iteration 9948, loss = 0.31691084\n",
            "Iteration 9949, loss = 0.31692118\n",
            "Iteration 9950, loss = 0.31690387\n",
            "Iteration 9951, loss = 0.31688343\n",
            "Iteration 9952, loss = 0.31690986\n",
            "Iteration 9953, loss = 0.31686765\n",
            "Iteration 9954, loss = 0.31683623\n",
            "Iteration 9955, loss = 0.31686112\n",
            "Iteration 9956, loss = 0.31681087\n",
            "Iteration 9957, loss = 0.31679885\n",
            "Iteration 9958, loss = 0.31680489\n",
            "Iteration 9959, loss = 0.31676904\n",
            "Iteration 9960, loss = 0.31676452\n",
            "Iteration 9961, loss = 0.31673134\n",
            "Iteration 9962, loss = 0.31675201\n",
            "Iteration 9963, loss = 0.31674031\n",
            "Iteration 9964, loss = 0.31669079\n",
            "Iteration 9965, loss = 0.31672746\n",
            "Iteration 9966, loss = 0.31669240\n",
            "Iteration 9967, loss = 0.31666479\n",
            "Iteration 9968, loss = 0.31666384\n",
            "Iteration 9969, loss = 0.31663022\n",
            "Iteration 9970, loss = 0.31661453\n",
            "Iteration 9971, loss = 0.31661926\n",
            "Iteration 9972, loss = 0.31659760\n",
            "Iteration 9973, loss = 0.31656990\n",
            "Iteration 9974, loss = 0.31657522\n",
            "Iteration 9975, loss = 0.31657472\n",
            "Iteration 9976, loss = 0.31653051\n",
            "Iteration 9977, loss = 0.31652020\n",
            "Iteration 9978, loss = 0.31652445\n",
            "Iteration 9979, loss = 0.31650003\n",
            "Iteration 9980, loss = 0.31648748\n",
            "Iteration 9981, loss = 0.31646322\n",
            "Iteration 9982, loss = 0.31645890\n",
            "Iteration 9983, loss = 0.31646590\n",
            "Iteration 9984, loss = 0.31641184\n",
            "Iteration 9985, loss = 0.31644424\n",
            "Iteration 9986, loss = 0.31641285\n",
            "Iteration 9987, loss = 0.31638395\n",
            "Iteration 9988, loss = 0.31637252\n",
            "Iteration 9989, loss = 0.31635473\n",
            "Iteration 9990, loss = 0.31633857\n",
            "Iteration 9991, loss = 0.31632870\n",
            "Iteration 9992, loss = 0.31631571\n",
            "Iteration 9993, loss = 0.31629667\n",
            "Iteration 9994, loss = 0.31628132\n",
            "Iteration 9995, loss = 0.31627517\n",
            "Iteration 9996, loss = 0.31627345\n",
            "Iteration 9997, loss = 0.31625910\n",
            "Iteration 9998, loss = 0.31622754\n",
            "Iteration 9999, loss = 0.31621765\n",
            "Iteration 10000, loss = 0.31620516\n",
            "Iteration 10001, loss = 0.31619387\n",
            "Iteration 10002, loss = 0.31616872\n",
            "Iteration 10003, loss = 0.31615775\n",
            "Iteration 10004, loss = 0.31614854\n",
            "Iteration 10005, loss = 0.31612412\n",
            "Iteration 10006, loss = 0.31612286\n",
            "Iteration 10007, loss = 0.31609849\n",
            "Iteration 10008, loss = 0.31609813\n",
            "Iteration 10009, loss = 0.31607520\n",
            "Iteration 10010, loss = 0.31606489\n",
            "Iteration 10011, loss = 0.31604204\n",
            "Iteration 10012, loss = 0.31603983\n",
            "Iteration 10013, loss = 0.31601697\n",
            "Iteration 10014, loss = 0.31600934\n",
            "Iteration 10015, loss = 0.31598772\n",
            "Iteration 10016, loss = 0.31598654\n",
            "Iteration 10017, loss = 0.31596254\n",
            "Iteration 10018, loss = 0.31595519\n",
            "Iteration 10019, loss = 0.31593929\n",
            "Iteration 10020, loss = 0.31593331\n",
            "Iteration 10021, loss = 0.31590405\n",
            "Iteration 10022, loss = 0.31589384\n",
            "Iteration 10023, loss = 0.31587747\n",
            "Iteration 10024, loss = 0.31586427\n",
            "Iteration 10025, loss = 0.31585013\n",
            "Iteration 10026, loss = 0.31583805\n",
            "Iteration 10027, loss = 0.31582591\n",
            "Iteration 10028, loss = 0.31581179\n",
            "Iteration 10029, loss = 0.31579524\n",
            "Iteration 10030, loss = 0.31578098\n",
            "Iteration 10031, loss = 0.31576670\n",
            "Iteration 10032, loss = 0.31575205\n",
            "Iteration 10033, loss = 0.31575573\n",
            "Iteration 10034, loss = 0.31572209\n",
            "Iteration 10035, loss = 0.31571481\n",
            "Iteration 10036, loss = 0.31570929\n",
            "Iteration 10037, loss = 0.31568784\n",
            "Iteration 10038, loss = 0.31570101\n",
            "Iteration 10039, loss = 0.31565517\n",
            "Iteration 10040, loss = 0.31566838\n",
            "Iteration 10041, loss = 0.31564053\n",
            "Iteration 10042, loss = 0.31564623\n",
            "Iteration 10043, loss = 0.31561630\n",
            "Iteration 10044, loss = 0.31562755\n",
            "Iteration 10045, loss = 0.31562657\n",
            "Iteration 10046, loss = 0.31557751\n",
            "Iteration 10047, loss = 0.31558136\n",
            "Iteration 10048, loss = 0.31556003\n",
            "Iteration 10049, loss = 0.31555238\n",
            "Iteration 10050, loss = 0.31553305\n",
            "Iteration 10051, loss = 0.31552014\n",
            "Iteration 10052, loss = 0.31551234\n",
            "Iteration 10053, loss = 0.31549419\n",
            "Iteration 10054, loss = 0.31546827\n",
            "Iteration 10055, loss = 0.31549153\n",
            "Iteration 10056, loss = 0.31542349\n",
            "Iteration 10057, loss = 0.31545435\n",
            "Iteration 10058, loss = 0.31544813\n",
            "Iteration 10059, loss = 0.31542257\n",
            "Iteration 10060, loss = 0.31539149\n",
            "Iteration 10061, loss = 0.31536953\n",
            "Iteration 10062, loss = 0.31536945\n",
            "Iteration 10063, loss = 0.31533776\n",
            "Iteration 10064, loss = 0.31532722\n",
            "Iteration 10065, loss = 0.31535521\n",
            "Iteration 10066, loss = 0.31531264\n",
            "Iteration 10067, loss = 0.31527183\n",
            "Iteration 10068, loss = 0.31527708\n",
            "Iteration 10069, loss = 0.31529122\n",
            "Iteration 10070, loss = 0.31524391\n",
            "Iteration 10071, loss = 0.31522390\n",
            "Iteration 10072, loss = 0.31522411\n",
            "Iteration 10073, loss = 0.31520963\n",
            "Iteration 10074, loss = 0.31518186\n",
            "Iteration 10075, loss = 0.31516719\n",
            "Iteration 10076, loss = 0.31517422\n",
            "Iteration 10077, loss = 0.31513565\n",
            "Iteration 10078, loss = 0.31512360\n",
            "Iteration 10079, loss = 0.31512981\n",
            "Iteration 10080, loss = 0.31511010\n",
            "Iteration 10081, loss = 0.31508344\n",
            "Iteration 10082, loss = 0.31505869\n",
            "Iteration 10083, loss = 0.31506622\n",
            "Iteration 10084, loss = 0.31505099\n",
            "Iteration 10085, loss = 0.31503067\n",
            "Iteration 10086, loss = 0.31502496\n",
            "Iteration 10087, loss = 0.31501040\n",
            "Iteration 10088, loss = 0.31497973\n",
            "Iteration 10089, loss = 0.31497960\n",
            "Iteration 10090, loss = 0.31496926\n",
            "Iteration 10091, loss = 0.31494790\n",
            "Iteration 10092, loss = 0.31493072\n",
            "Iteration 10093, loss = 0.31493807\n",
            "Iteration 10094, loss = 0.31491504\n",
            "Iteration 10095, loss = 0.31488716\n",
            "Iteration 10096, loss = 0.31487138\n",
            "Iteration 10097, loss = 0.31490418\n",
            "Iteration 10098, loss = 0.31485694\n",
            "Iteration 10099, loss = 0.31481989\n",
            "Iteration 10100, loss = 0.31484840\n",
            "Iteration 10101, loss = 0.31481148\n",
            "Iteration 10102, loss = 0.31480402\n",
            "Iteration 10103, loss = 0.31477314\n",
            "Iteration 10104, loss = 0.31477636\n",
            "Iteration 10105, loss = 0.31475377\n",
            "Iteration 10106, loss = 0.31472569\n",
            "Iteration 10107, loss = 0.31472005\n",
            "Iteration 10108, loss = 0.31471767\n",
            "Iteration 10109, loss = 0.31468936\n",
            "Iteration 10110, loss = 0.31468024\n",
            "Iteration 10111, loss = 0.31465958\n",
            "Iteration 10112, loss = 0.31464915\n",
            "Iteration 10113, loss = 0.31463191\n",
            "Iteration 10114, loss = 0.31461236\n",
            "Iteration 10115, loss = 0.31459954\n",
            "Iteration 10116, loss = 0.31458542\n",
            "Iteration 10117, loss = 0.31456950\n",
            "Iteration 10118, loss = 0.31455860\n",
            "Iteration 10119, loss = 0.31454692\n",
            "Iteration 10120, loss = 0.31452692\n",
            "Iteration 10121, loss = 0.31451499\n",
            "Iteration 10122, loss = 0.31451836\n",
            "Iteration 10123, loss = 0.31449800\n",
            "Iteration 10124, loss = 0.31449421\n",
            "Iteration 10125, loss = 0.31446522\n",
            "Iteration 10126, loss = 0.31447003\n",
            "Iteration 10127, loss = 0.31443459\n",
            "Iteration 10128, loss = 0.31441899\n",
            "Iteration 10129, loss = 0.31442089\n",
            "Iteration 10130, loss = 0.31440655\n",
            "Iteration 10131, loss = 0.31437790\n",
            "Iteration 10132, loss = 0.31437437\n",
            "Iteration 10133, loss = 0.31434639\n",
            "Iteration 10134, loss = 0.31432659\n",
            "Iteration 10135, loss = 0.31433724\n",
            "Iteration 10136, loss = 0.31431120\n",
            "Iteration 10137, loss = 0.31428082\n",
            "Iteration 10138, loss = 0.31431270\n",
            "Iteration 10139, loss = 0.31426525\n",
            "Iteration 10140, loss = 0.31430172\n",
            "Iteration 10141, loss = 0.31426797\n",
            "Iteration 10142, loss = 0.31426556\n",
            "Iteration 10143, loss = 0.31428048\n",
            "Iteration 10144, loss = 0.31420391\n",
            "Iteration 10145, loss = 0.31423253\n",
            "Iteration 10146, loss = 0.31421522\n",
            "Iteration 10147, loss = 0.31418008\n",
            "Iteration 10148, loss = 0.31419912\n",
            "Iteration 10149, loss = 0.31412089\n",
            "Iteration 10150, loss = 0.31418400\n",
            "Iteration 10151, loss = 0.31421764\n",
            "Iteration 10152, loss = 0.31407203\n",
            "Iteration 10153, loss = 0.31414401\n",
            "Iteration 10154, loss = 0.31412009\n",
            "Iteration 10155, loss = 0.31403533\n",
            "Iteration 10156, loss = 0.31408665\n",
            "Iteration 10157, loss = 0.31402674\n",
            "Iteration 10158, loss = 0.31402675\n",
            "Iteration 10159, loss = 0.31400834\n",
            "Iteration 10160, loss = 0.31398771\n",
            "Iteration 10161, loss = 0.31401714\n",
            "Iteration 10162, loss = 0.31398872\n",
            "Iteration 10163, loss = 0.31393493\n",
            "Iteration 10164, loss = 0.31395028\n",
            "Iteration 10165, loss = 0.31391534\n",
            "Iteration 10166, loss = 0.31396080\n",
            "Iteration 10167, loss = 0.31390235\n",
            "Iteration 10168, loss = 0.31387660\n",
            "Iteration 10169, loss = 0.31392404\n",
            "Iteration 10170, loss = 0.31386894\n",
            "Iteration 10171, loss = 0.31381942\n",
            "Iteration 10172, loss = 0.31380957\n",
            "Iteration 10173, loss = 0.31382024\n",
            "Iteration 10174, loss = 0.31379593\n",
            "Iteration 10175, loss = 0.31379837\n",
            "Iteration 10176, loss = 0.31375477\n",
            "Iteration 10177, loss = 0.31374112\n",
            "Iteration 10178, loss = 0.31373103\n",
            "Iteration 10179, loss = 0.31371678\n",
            "Iteration 10180, loss = 0.31369013\n",
            "Iteration 10181, loss = 0.31369456\n",
            "Iteration 10182, loss = 0.31366194\n",
            "Iteration 10183, loss = 0.31369834\n",
            "Iteration 10184, loss = 0.31363553\n",
            "Iteration 10185, loss = 0.31366201\n",
            "Iteration 10186, loss = 0.31363011\n",
            "Iteration 10187, loss = 0.31359506\n",
            "Iteration 10188, loss = 0.31358075\n",
            "Iteration 10189, loss = 0.31357444\n",
            "Iteration 10190, loss = 0.31356163\n",
            "Iteration 10191, loss = 0.31353696\n",
            "Iteration 10192, loss = 0.31353258\n",
            "Iteration 10193, loss = 0.31351949\n",
            "Iteration 10194, loss = 0.31350886\n",
            "Iteration 10195, loss = 0.31347546\n",
            "Iteration 10196, loss = 0.31347016\n",
            "Iteration 10197, loss = 0.31345756\n",
            "Iteration 10198, loss = 0.31344161\n",
            "Iteration 10199, loss = 0.31342338\n",
            "Iteration 10200, loss = 0.31341690\n",
            "Iteration 10201, loss = 0.31339713\n",
            "Iteration 10202, loss = 0.31338502\n",
            "Iteration 10203, loss = 0.31336948\n",
            "Iteration 10204, loss = 0.31335039\n",
            "Iteration 10205, loss = 0.31333200\n",
            "Iteration 10206, loss = 0.31332989\n",
            "Iteration 10207, loss = 0.31332012\n",
            "Iteration 10208, loss = 0.31331161\n",
            "Iteration 10209, loss = 0.31330658\n",
            "Iteration 10210, loss = 0.31326669\n",
            "Iteration 10211, loss = 0.31328642\n",
            "Iteration 10212, loss = 0.31324819\n",
            "Iteration 10213, loss = 0.31326414\n",
            "Iteration 10214, loss = 0.31324325\n",
            "Iteration 10215, loss = 0.31321956\n",
            "Iteration 10216, loss = 0.31326621\n",
            "Iteration 10217, loss = 0.31320420\n",
            "Iteration 10218, loss = 0.31320941\n",
            "Iteration 10219, loss = 0.31316357\n",
            "Iteration 10220, loss = 0.31312767\n",
            "Iteration 10221, loss = 0.31315856\n",
            "Iteration 10222, loss = 0.31309611\n",
            "Iteration 10223, loss = 0.31311089\n",
            "Iteration 10224, loss = 0.31308833\n",
            "Iteration 10225, loss = 0.31305853\n",
            "Iteration 10226, loss = 0.31307311\n",
            "Iteration 10227, loss = 0.31303311\n",
            "Iteration 10228, loss = 0.31303000\n",
            "Iteration 10229, loss = 0.31300702\n",
            "Iteration 10230, loss = 0.31299671\n",
            "Iteration 10231, loss = 0.31297285\n",
            "Iteration 10232, loss = 0.31298103\n",
            "Iteration 10233, loss = 0.31295167\n",
            "Iteration 10234, loss = 0.31295704\n",
            "Iteration 10235, loss = 0.31292023\n",
            "Iteration 10236, loss = 0.31290247\n",
            "Iteration 10237, loss = 0.31290380\n",
            "Iteration 10238, loss = 0.31289080\n",
            "Iteration 10239, loss = 0.31286460\n",
            "Iteration 10240, loss = 0.31285096\n",
            "Iteration 10241, loss = 0.31285502\n",
            "Iteration 10242, loss = 0.31281799\n",
            "Iteration 10243, loss = 0.31284352\n",
            "Iteration 10244, loss = 0.31280759\n",
            "Iteration 10245, loss = 0.31280960\n",
            "Iteration 10246, loss = 0.31278124\n",
            "Iteration 10247, loss = 0.31275169\n",
            "Iteration 10248, loss = 0.31274827\n",
            "Iteration 10249, loss = 0.31273797\n",
            "Iteration 10250, loss = 0.31272742\n",
            "Iteration 10251, loss = 0.31269775\n",
            "Iteration 10252, loss = 0.31270243\n",
            "Iteration 10253, loss = 0.31266995\n",
            "Iteration 10254, loss = 0.31265518\n",
            "Iteration 10255, loss = 0.31266514\n",
            "Iteration 10256, loss = 0.31263578\n",
            "Iteration 10257, loss = 0.31261915\n",
            "Iteration 10258, loss = 0.31261677\n",
            "Iteration 10259, loss = 0.31258780\n",
            "Iteration 10260, loss = 0.31256638\n",
            "Iteration 10261, loss = 0.31256340\n",
            "Iteration 10262, loss = 0.31254216\n",
            "Iteration 10263, loss = 0.31252359\n",
            "Iteration 10264, loss = 0.31251704\n",
            "Iteration 10265, loss = 0.31249404\n",
            "Iteration 10266, loss = 0.31248746\n",
            "Iteration 10267, loss = 0.31246054\n",
            "Iteration 10268, loss = 0.31244622\n",
            "Iteration 10269, loss = 0.31243203\n",
            "Iteration 10270, loss = 0.31242551\n",
            "Iteration 10271, loss = 0.31241130\n",
            "Iteration 10272, loss = 0.31239939\n",
            "Iteration 10273, loss = 0.31238796\n",
            "Iteration 10274, loss = 0.31236215\n",
            "Iteration 10275, loss = 0.31237173\n",
            "Iteration 10276, loss = 0.31237704\n",
            "Iteration 10277, loss = 0.31232848\n",
            "Iteration 10278, loss = 0.31231965\n",
            "Iteration 10279, loss = 0.31229481\n",
            "Iteration 10280, loss = 0.31228974\n",
            "Iteration 10281, loss = 0.31226531\n",
            "Iteration 10282, loss = 0.31225588\n",
            "Iteration 10283, loss = 0.31224161\n",
            "Iteration 10284, loss = 0.31222352\n",
            "Iteration 10285, loss = 0.31220815\n",
            "Iteration 10286, loss = 0.31220080\n",
            "Iteration 10287, loss = 0.31218146\n",
            "Iteration 10288, loss = 0.31216981\n",
            "Iteration 10289, loss = 0.31214916\n",
            "Iteration 10290, loss = 0.31213779\n",
            "Iteration 10291, loss = 0.31212468\n",
            "Iteration 10292, loss = 0.31210464\n",
            "Iteration 10293, loss = 0.31208459\n",
            "Iteration 10294, loss = 0.31207450\n",
            "Iteration 10295, loss = 0.31206379\n",
            "Iteration 10296, loss = 0.31205011\n",
            "Iteration 10297, loss = 0.31203840\n",
            "Iteration 10298, loss = 0.31202251\n",
            "Iteration 10299, loss = 0.31200739\n",
            "Iteration 10300, loss = 0.31200812\n",
            "Iteration 10301, loss = 0.31198956\n",
            "Iteration 10302, loss = 0.31196767\n",
            "Iteration 10303, loss = 0.31194430\n",
            "Iteration 10304, loss = 0.31192712\n",
            "Iteration 10305, loss = 0.31191779\n",
            "Iteration 10306, loss = 0.31189592\n",
            "Iteration 10307, loss = 0.31188878\n",
            "Iteration 10308, loss = 0.31187645\n",
            "Iteration 10309, loss = 0.31186587\n",
            "Iteration 10310, loss = 0.31189728\n",
            "Iteration 10311, loss = 0.31185396\n",
            "Iteration 10312, loss = 0.31182488\n",
            "Iteration 10313, loss = 0.31180571\n",
            "Iteration 10314, loss = 0.31178478\n",
            "Iteration 10315, loss = 0.31176871\n",
            "Iteration 10316, loss = 0.31175031\n",
            "Iteration 10317, loss = 0.31173889\n",
            "Iteration 10318, loss = 0.31172209\n",
            "Iteration 10319, loss = 0.31172465\n",
            "Iteration 10320, loss = 0.31169067\n",
            "Iteration 10321, loss = 0.31169397\n",
            "Iteration 10322, loss = 0.31167132\n",
            "Iteration 10323, loss = 0.31165917\n",
            "Iteration 10324, loss = 0.31168224\n",
            "Iteration 10325, loss = 0.31163078\n",
            "Iteration 10326, loss = 0.31168902\n",
            "Iteration 10327, loss = 0.31169419\n",
            "Iteration 10328, loss = 0.31160498\n",
            "Iteration 10329, loss = 0.31166078\n",
            "Iteration 10330, loss = 0.31164438\n",
            "Iteration 10331, loss = 0.31158377\n",
            "Iteration 10332, loss = 0.31161854\n",
            "Iteration 10333, loss = 0.31160740\n",
            "Iteration 10334, loss = 0.31150821\n",
            "Iteration 10335, loss = 0.31160070\n",
            "Iteration 10336, loss = 0.31154059\n",
            "Iteration 10337, loss = 0.31148283\n",
            "Iteration 10338, loss = 0.31151736\n",
            "Iteration 10339, loss = 0.31148033\n",
            "Iteration 10340, loss = 0.31140947\n",
            "Iteration 10341, loss = 0.31150694\n",
            "Iteration 10342, loss = 0.31139640\n",
            "Iteration 10343, loss = 0.31141204\n",
            "Iteration 10344, loss = 0.31141529\n",
            "Iteration 10345, loss = 0.31134188\n",
            "Iteration 10346, loss = 0.31137488\n",
            "Iteration 10347, loss = 0.31132458\n",
            "Iteration 10348, loss = 0.31131276\n",
            "Iteration 10349, loss = 0.31131093\n",
            "Iteration 10350, loss = 0.31126436\n",
            "Iteration 10351, loss = 0.31126495\n",
            "Iteration 10352, loss = 0.31122895\n",
            "Iteration 10353, loss = 0.31124289\n",
            "Iteration 10354, loss = 0.31121192\n",
            "Iteration 10355, loss = 0.31122642\n",
            "Iteration 10356, loss = 0.31117864\n",
            "Iteration 10357, loss = 0.31119861\n",
            "Iteration 10358, loss = 0.31118639\n",
            "Iteration 10359, loss = 0.31112211\n",
            "Iteration 10360, loss = 0.31112233\n",
            "Iteration 10361, loss = 0.31112838\n",
            "Iteration 10362, loss = 0.31108367\n",
            "Iteration 10363, loss = 0.31110006\n",
            "Iteration 10364, loss = 0.31109770\n",
            "Iteration 10365, loss = 0.31105025\n",
            "Iteration 10366, loss = 0.31101472\n",
            "Iteration 10367, loss = 0.31105630\n",
            "Iteration 10368, loss = 0.31105524\n",
            "Iteration 10369, loss = 0.31099134\n",
            "Iteration 10370, loss = 0.31099227\n",
            "Iteration 10371, loss = 0.31102609\n",
            "Iteration 10372, loss = 0.31100150\n",
            "Iteration 10373, loss = 0.31094655\n",
            "Iteration 10374, loss = 0.31090492\n",
            "Iteration 10375, loss = 0.31095433\n",
            "Iteration 10376, loss = 0.31091127\n",
            "Iteration 10377, loss = 0.31086099\n",
            "Iteration 10378, loss = 0.31089882\n",
            "Iteration 10379, loss = 0.31092320\n",
            "Iteration 10380, loss = 0.31087590\n",
            "Iteration 10381, loss = 0.31079256\n",
            "Iteration 10382, loss = 0.31084281\n",
            "Iteration 10383, loss = 0.31083569\n",
            "Iteration 10384, loss = 0.31084910\n",
            "Iteration 10385, loss = 0.31075886\n",
            "Iteration 10386, loss = 0.31075054\n",
            "Iteration 10387, loss = 0.31080224\n",
            "Iteration 10388, loss = 0.31079987\n",
            "Iteration 10389, loss = 0.31074791\n",
            "Iteration 10390, loss = 0.31067282\n",
            "Iteration 10391, loss = 0.31065160\n",
            "Iteration 10392, loss = 0.31069374\n",
            "Iteration 10393, loss = 0.31064056\n",
            "Iteration 10394, loss = 0.31059821\n",
            "Iteration 10395, loss = 0.31059584\n",
            "Iteration 10396, loss = 0.31059415\n",
            "Iteration 10397, loss = 0.31056081\n",
            "Iteration 10398, loss = 0.31053151\n",
            "Iteration 10399, loss = 0.31053138\n",
            "Iteration 10400, loss = 0.31050689\n",
            "Iteration 10401, loss = 0.31048522\n",
            "Iteration 10402, loss = 0.31047895\n",
            "Iteration 10403, loss = 0.31047136\n",
            "Iteration 10404, loss = 0.31045499\n",
            "Iteration 10405, loss = 0.31042991\n",
            "Iteration 10406, loss = 0.31042594\n",
            "Iteration 10407, loss = 0.31041163\n",
            "Iteration 10408, loss = 0.31040814\n",
            "Iteration 10409, loss = 0.31037228\n",
            "Iteration 10410, loss = 0.31038752\n",
            "Iteration 10411, loss = 0.31033852\n",
            "Iteration 10412, loss = 0.31034813\n",
            "Iteration 10413, loss = 0.31034273\n",
            "Iteration 10414, loss = 0.31029499\n",
            "Iteration 10415, loss = 0.31034990\n",
            "Iteration 10416, loss = 0.31030294\n",
            "Iteration 10417, loss = 0.31024613\n",
            "Iteration 10418, loss = 0.31027570\n",
            "Iteration 10419, loss = 0.31022929\n",
            "Iteration 10420, loss = 0.31020367\n",
            "Iteration 10421, loss = 0.31023309\n",
            "Iteration 10422, loss = 0.31019815\n",
            "Iteration 10423, loss = 0.31016704\n",
            "Iteration 10424, loss = 0.31018867\n",
            "Iteration 10425, loss = 0.31013284\n",
            "Iteration 10426, loss = 0.31012369\n",
            "Iteration 10427, loss = 0.31009569\n",
            "Iteration 10428, loss = 0.31009130\n",
            "Iteration 10429, loss = 0.31010496\n",
            "Iteration 10430, loss = 0.31009236\n",
            "Iteration 10431, loss = 0.31004163\n",
            "Iteration 10432, loss = 0.31001570\n",
            "Iteration 10433, loss = 0.31007520\n",
            "Iteration 10434, loss = 0.30999733\n",
            "Iteration 10435, loss = 0.30999393\n",
            "Iteration 10436, loss = 0.31000098\n",
            "Iteration 10437, loss = 0.30996170\n",
            "Iteration 10438, loss = 0.30992923\n",
            "Iteration 10439, loss = 0.30996686\n",
            "Iteration 10440, loss = 0.30989465\n",
            "Iteration 10441, loss = 0.30988525\n",
            "Iteration 10442, loss = 0.30987337\n",
            "Iteration 10443, loss = 0.30984357\n",
            "Iteration 10444, loss = 0.30983832\n",
            "Iteration 10445, loss = 0.30982386\n",
            "Iteration 10446, loss = 0.30980411\n",
            "Iteration 10447, loss = 0.30979288\n",
            "Iteration 10448, loss = 0.30978161\n",
            "Iteration 10449, loss = 0.30976345\n",
            "Iteration 10450, loss = 0.30974728\n",
            "Iteration 10451, loss = 0.30973861\n",
            "Iteration 10452, loss = 0.30972275\n",
            "Iteration 10453, loss = 0.30970478\n",
            "Iteration 10454, loss = 0.30968469\n",
            "Iteration 10455, loss = 0.30966840\n",
            "Iteration 10456, loss = 0.30965954\n",
            "Iteration 10457, loss = 0.30964195\n",
            "Iteration 10458, loss = 0.30962705\n",
            "Iteration 10459, loss = 0.30960611\n",
            "Iteration 10460, loss = 0.30958669\n",
            "Iteration 10461, loss = 0.30959202\n",
            "Iteration 10462, loss = 0.30956475\n",
            "Iteration 10463, loss = 0.30954704\n",
            "Iteration 10464, loss = 0.30953017\n",
            "Iteration 10465, loss = 0.30951402\n",
            "Iteration 10466, loss = 0.30950847\n",
            "Iteration 10467, loss = 0.30950084\n",
            "Iteration 10468, loss = 0.30949772\n",
            "Iteration 10469, loss = 0.30947714\n",
            "Iteration 10470, loss = 0.30945038\n",
            "Iteration 10471, loss = 0.30949593\n",
            "Iteration 10472, loss = 0.30950664\n",
            "Iteration 10473, loss = 0.30941362\n",
            "Iteration 10474, loss = 0.30947377\n",
            "Iteration 10475, loss = 0.30938558\n",
            "Iteration 10476, loss = 0.30941832\n",
            "Iteration 10477, loss = 0.30936189\n",
            "Iteration 10478, loss = 0.30938889\n",
            "Iteration 10479, loss = 0.30936878\n",
            "Iteration 10480, loss = 0.30933893\n",
            "Iteration 10481, loss = 0.30937314\n",
            "Iteration 10482, loss = 0.30929866\n",
            "Iteration 10483, loss = 0.30937579\n",
            "Iteration 10484, loss = 0.30930666\n",
            "Iteration 10485, loss = 0.30929386\n",
            "Iteration 10486, loss = 0.30926077\n",
            "Iteration 10487, loss = 0.30927438\n",
            "Iteration 10488, loss = 0.30924110\n",
            "Iteration 10489, loss = 0.30920169\n",
            "Iteration 10490, loss = 0.30922103\n",
            "Iteration 10491, loss = 0.30918775\n",
            "Iteration 10492, loss = 0.30917342\n",
            "Iteration 10493, loss = 0.30916076\n",
            "Iteration 10494, loss = 0.30914996\n",
            "Iteration 10495, loss = 0.30912951\n",
            "Iteration 10496, loss = 0.30914176\n",
            "Iteration 10497, loss = 0.30910575\n",
            "Iteration 10498, loss = 0.30909894\n",
            "Iteration 10499, loss = 0.30909260\n",
            "Iteration 10500, loss = 0.30906946\n",
            "Iteration 10501, loss = 0.30909076\n",
            "Iteration 10502, loss = 0.30903833\n",
            "Iteration 10503, loss = 0.30907334\n",
            "Iteration 10504, loss = 0.30900885\n",
            "Iteration 10505, loss = 0.30900430\n",
            "Iteration 10506, loss = 0.30898910\n",
            "Iteration 10507, loss = 0.30897226\n",
            "Iteration 10508, loss = 0.30898079\n",
            "Iteration 10509, loss = 0.30897058\n",
            "Iteration 10510, loss = 0.30895797\n",
            "Iteration 10511, loss = 0.30895809\n",
            "Iteration 10512, loss = 0.30892566\n",
            "Iteration 10513, loss = 0.30891852\n",
            "Iteration 10514, loss = 0.30888879\n",
            "Iteration 10515, loss = 0.30893191\n",
            "Iteration 10516, loss = 0.30889206\n",
            "Iteration 10517, loss = 0.30889342\n",
            "Iteration 10518, loss = 0.30884491\n",
            "Iteration 10519, loss = 0.30885481\n",
            "Iteration 10520, loss = 0.30882040\n",
            "Iteration 10521, loss = 0.30882550\n",
            "Iteration 10522, loss = 0.30883022\n",
            "Iteration 10523, loss = 0.30879972\n",
            "Iteration 10524, loss = 0.30881174\n",
            "Iteration 10525, loss = 0.30877944\n",
            "Iteration 10526, loss = 0.30874590\n",
            "Iteration 10527, loss = 0.30879604\n",
            "Iteration 10528, loss = 0.30875249\n",
            "Iteration 10529, loss = 0.30879652\n",
            "Iteration 10530, loss = 0.30875155\n",
            "Iteration 10531, loss = 0.30869799\n",
            "Iteration 10532, loss = 0.30873531\n",
            "Iteration 10533, loss = 0.30865047\n",
            "Iteration 10534, loss = 0.30870876\n",
            "Iteration 10535, loss = 0.30867636\n",
            "Iteration 10536, loss = 0.30866020\n",
            "Iteration 10537, loss = 0.30865321\n",
            "Iteration 10538, loss = 0.30864028\n",
            "Iteration 10539, loss = 0.30867260\n",
            "Iteration 10540, loss = 0.30860370\n",
            "Iteration 10541, loss = 0.30865824\n",
            "Iteration 10542, loss = 0.30856954\n",
            "Iteration 10543, loss = 0.30861171\n",
            "Iteration 10544, loss = 0.30860762\n",
            "Iteration 10545, loss = 0.30853474\n",
            "Iteration 10546, loss = 0.30854237\n",
            "Iteration 10547, loss = 0.30849893\n",
            "Iteration 10548, loss = 0.30852847\n",
            "Iteration 10549, loss = 0.30850512\n",
            "Iteration 10550, loss = 0.30848530\n",
            "Iteration 10551, loss = 0.30847644\n",
            "Iteration 10552, loss = 0.30842996\n",
            "Iteration 10553, loss = 0.30845648\n",
            "Iteration 10554, loss = 0.30843628\n",
            "Iteration 10555, loss = 0.30838756\n",
            "Iteration 10556, loss = 0.30843922\n",
            "Iteration 10557, loss = 0.30839586\n",
            "Iteration 10558, loss = 0.30836261\n",
            "Iteration 10559, loss = 0.30837932\n",
            "Iteration 10560, loss = 0.30839862\n",
            "Iteration 10561, loss = 0.30834491\n",
            "Iteration 10562, loss = 0.30835956\n",
            "Iteration 10563, loss = 0.30840482\n",
            "Iteration 10564, loss = 0.30838732\n",
            "Iteration 10565, loss = 0.30833696\n",
            "Iteration 10566, loss = 0.30827204\n",
            "Iteration 10567, loss = 0.30835166\n",
            "Iteration 10568, loss = 0.30835747\n",
            "Iteration 10569, loss = 0.30827145\n",
            "Iteration 10570, loss = 0.30823314\n",
            "Iteration 10571, loss = 0.30826119\n",
            "Iteration 10572, loss = 0.30831516\n",
            "Iteration 10573, loss = 0.30822158\n",
            "Iteration 10574, loss = 0.30816902\n",
            "Iteration 10575, loss = 0.30822351\n",
            "Iteration 10576, loss = 0.30818378\n",
            "Iteration 10577, loss = 0.30815289\n",
            "Iteration 10578, loss = 0.30818151\n",
            "Iteration 10579, loss = 0.30816005\n",
            "Iteration 10580, loss = 0.30809647\n",
            "Iteration 10581, loss = 0.30812782\n",
            "Iteration 10582, loss = 0.30815927\n",
            "Iteration 10583, loss = 0.30808395\n",
            "Iteration 10584, loss = 0.30807531\n",
            "Iteration 10585, loss = 0.30811102\n",
            "Iteration 10586, loss = 0.30811036\n",
            "Iteration 10587, loss = 0.30807314\n",
            "Iteration 10588, loss = 0.30803519\n",
            "Iteration 10589, loss = 0.30801375\n",
            "Iteration 10590, loss = 0.30807405\n",
            "Iteration 10591, loss = 0.30800784\n",
            "Iteration 10592, loss = 0.30798718\n",
            "Iteration 10593, loss = 0.30801480\n",
            "Iteration 10594, loss = 0.30800354\n",
            "Iteration 10595, loss = 0.30794973\n",
            "Iteration 10596, loss = 0.30791479\n",
            "Iteration 10597, loss = 0.30793072\n",
            "Iteration 10598, loss = 0.30790438\n",
            "Iteration 10599, loss = 0.30789283\n",
            "Iteration 10600, loss = 0.30787451\n",
            "Iteration 10601, loss = 0.30787034\n",
            "Iteration 10602, loss = 0.30783420\n",
            "Iteration 10603, loss = 0.30784690\n",
            "Iteration 10604, loss = 0.30788038\n",
            "Iteration 10605, loss = 0.30783588\n",
            "Iteration 10606, loss = 0.30780976\n",
            "Iteration 10607, loss = 0.30784999\n",
            "Iteration 10608, loss = 0.30782489\n",
            "Iteration 10609, loss = 0.30777252\n",
            "Iteration 10610, loss = 0.30778151\n",
            "Iteration 10611, loss = 0.30775366\n",
            "Iteration 10612, loss = 0.30774469\n",
            "Iteration 10613, loss = 0.30770229\n",
            "Iteration 10614, loss = 0.30769163\n",
            "Iteration 10615, loss = 0.30767254\n",
            "Iteration 10616, loss = 0.30766954\n",
            "Iteration 10617, loss = 0.30764857\n",
            "Iteration 10618, loss = 0.30765129\n",
            "Iteration 10619, loss = 0.30763826\n",
            "Iteration 10620, loss = 0.30761101\n",
            "Iteration 10621, loss = 0.30763080\n",
            "Iteration 10622, loss = 0.30760095\n",
            "Iteration 10623, loss = 0.30759166\n",
            "Iteration 10624, loss = 0.30760378\n",
            "Iteration 10625, loss = 0.30759069\n",
            "Iteration 10626, loss = 0.30759148\n",
            "Iteration 10627, loss = 0.30758116\n",
            "Iteration 10628, loss = 0.30754769\n",
            "Iteration 10629, loss = 0.30755311\n",
            "Iteration 10630, loss = 0.30755445\n",
            "Iteration 10631, loss = 0.30754088\n",
            "Iteration 10632, loss = 0.30749770\n",
            "Iteration 10633, loss = 0.30749787\n",
            "Iteration 10634, loss = 0.30753576\n",
            "Iteration 10635, loss = 0.30746893\n",
            "Iteration 10636, loss = 0.30744031\n",
            "Iteration 10637, loss = 0.30746062\n",
            "Iteration 10638, loss = 0.30747376\n",
            "Iteration 10639, loss = 0.30740201\n",
            "Iteration 10640, loss = 0.30740427\n",
            "Iteration 10641, loss = 0.30741463\n",
            "Iteration 10642, loss = 0.30736535\n",
            "Iteration 10643, loss = 0.30736610\n",
            "Iteration 10644, loss = 0.30735583\n",
            "Iteration 10645, loss = 0.30731971\n",
            "Iteration 10646, loss = 0.30735325\n",
            "Iteration 10647, loss = 0.30731763\n",
            "Iteration 10648, loss = 0.30729204\n",
            "Iteration 10649, loss = 0.30732559\n",
            "Iteration 10650, loss = 0.30730226\n",
            "Iteration 10651, loss = 0.30726002\n",
            "Iteration 10652, loss = 0.30726847\n",
            "Iteration 10653, loss = 0.30722539\n",
            "Iteration 10654, loss = 0.30727135\n",
            "Iteration 10655, loss = 0.30724684\n",
            "Iteration 10656, loss = 0.30719563\n",
            "Iteration 10657, loss = 0.30721037\n",
            "Iteration 10658, loss = 0.30721411\n",
            "Iteration 10659, loss = 0.30717195\n",
            "Iteration 10660, loss = 0.30717425\n",
            "Iteration 10661, loss = 0.30718215\n",
            "Iteration 10662, loss = 0.30714994\n",
            "Iteration 10663, loss = 0.30716007\n",
            "Iteration 10664, loss = 0.30716691\n",
            "Iteration 10665, loss = 0.30713709\n",
            "Iteration 10666, loss = 0.30711861\n",
            "Iteration 10667, loss = 0.30711626\n",
            "Iteration 10668, loss = 0.30712318\n",
            "Iteration 10669, loss = 0.30707157\n",
            "Iteration 10670, loss = 0.30707764\n",
            "Iteration 10671, loss = 0.30709674\n",
            "Iteration 10672, loss = 0.30706488\n",
            "Iteration 10673, loss = 0.30700541\n",
            "Iteration 10674, loss = 0.30703354\n",
            "Iteration 10675, loss = 0.30703523\n",
            "Iteration 10676, loss = 0.30697756\n",
            "Iteration 10677, loss = 0.30697555\n",
            "Iteration 10678, loss = 0.30698069\n",
            "Iteration 10679, loss = 0.30692999\n",
            "Iteration 10680, loss = 0.30693208\n",
            "Iteration 10681, loss = 0.30690977\n",
            "Iteration 10682, loss = 0.30690197\n",
            "Iteration 10683, loss = 0.30689437\n",
            "Iteration 10684, loss = 0.30689991\n",
            "Iteration 10685, loss = 0.30690276\n",
            "Iteration 10686, loss = 0.30691130\n",
            "Iteration 10687, loss = 0.30684512\n",
            "Iteration 10688, loss = 0.30686836\n",
            "Iteration 10689, loss = 0.30692170\n",
            "Iteration 10690, loss = 0.30687030\n",
            "Iteration 10691, loss = 0.30682732\n",
            "Iteration 10692, loss = 0.30687191\n",
            "Iteration 10693, loss = 0.30689114\n",
            "Iteration 10694, loss = 0.30683847\n",
            "Iteration 10695, loss = 0.30676855\n",
            "Iteration 10696, loss = 0.30681888\n",
            "Iteration 10697, loss = 0.30685011\n",
            "Iteration 10698, loss = 0.30676962\n",
            "Iteration 10699, loss = 0.30673347\n",
            "Iteration 10700, loss = 0.30677180\n",
            "Iteration 10701, loss = 0.30677702\n",
            "Iteration 10702, loss = 0.30670653\n",
            "Iteration 10703, loss = 0.30668640\n",
            "Iteration 10704, loss = 0.30673474\n",
            "Iteration 10705, loss = 0.30675070\n",
            "Iteration 10706, loss = 0.30663021\n",
            "Iteration 10707, loss = 0.30667356\n",
            "Iteration 10708, loss = 0.30667540\n",
            "Iteration 10709, loss = 0.30668434\n",
            "Iteration 10710, loss = 0.30662577\n",
            "Iteration 10711, loss = 0.30656609\n",
            "Iteration 10712, loss = 0.30660280\n",
            "Iteration 10713, loss = 0.30658328\n",
            "Iteration 10714, loss = 0.30653949\n",
            "Iteration 10715, loss = 0.30654938\n",
            "Iteration 10716, loss = 0.30654426\n",
            "Iteration 10717, loss = 0.30651443\n",
            "Iteration 10718, loss = 0.30648853\n",
            "Iteration 10719, loss = 0.30649522\n",
            "Iteration 10720, loss = 0.30647790\n",
            "Iteration 10721, loss = 0.30648273\n",
            "Iteration 10722, loss = 0.30646992\n",
            "Iteration 10723, loss = 0.30642478\n",
            "Iteration 10724, loss = 0.30643110\n",
            "Iteration 10725, loss = 0.30643470\n",
            "Iteration 10726, loss = 0.30639329\n",
            "Iteration 10727, loss = 0.30640770\n",
            "Iteration 10728, loss = 0.30639976\n",
            "Iteration 10729, loss = 0.30636341\n",
            "Iteration 10730, loss = 0.30636728\n",
            "Iteration 10731, loss = 0.30636879\n",
            "Iteration 10732, loss = 0.30632275\n",
            "Iteration 10733, loss = 0.30631254\n",
            "Iteration 10734, loss = 0.30631916\n",
            "Iteration 10735, loss = 0.30629742\n",
            "Iteration 10736, loss = 0.30629224\n",
            "Iteration 10737, loss = 0.30629621\n",
            "Iteration 10738, loss = 0.30627856\n",
            "Iteration 10739, loss = 0.30625933\n",
            "Iteration 10740, loss = 0.30624728\n",
            "Iteration 10741, loss = 0.30626381\n",
            "Iteration 10742, loss = 0.30622860\n",
            "Iteration 10743, loss = 0.30625276\n",
            "Iteration 10744, loss = 0.30621616\n",
            "Iteration 10745, loss = 0.30618725\n",
            "Iteration 10746, loss = 0.30619895\n",
            "Iteration 10747, loss = 0.30619621\n",
            "Iteration 10748, loss = 0.30614959\n",
            "Iteration 10749, loss = 0.30617335\n",
            "Iteration 10750, loss = 0.30616296\n",
            "Iteration 10751, loss = 0.30612754\n",
            "Iteration 10752, loss = 0.30611421\n",
            "Iteration 10753, loss = 0.30614738\n",
            "Iteration 10754, loss = 0.30608314\n",
            "Iteration 10755, loss = 0.30609278\n",
            "Iteration 10756, loss = 0.30610651\n",
            "Iteration 10757, loss = 0.30607330\n",
            "Iteration 10758, loss = 0.30604289\n",
            "Iteration 10759, loss = 0.30606517\n",
            "Iteration 10760, loss = 0.30601002\n",
            "Iteration 10761, loss = 0.30603714\n",
            "Iteration 10762, loss = 0.30604795\n",
            "Iteration 10763, loss = 0.30601892\n",
            "Iteration 10764, loss = 0.30596384\n",
            "Iteration 10765, loss = 0.30599804\n",
            "Iteration 10766, loss = 0.30597433\n",
            "Iteration 10767, loss = 0.30593898\n",
            "Iteration 10768, loss = 0.30596503\n",
            "Iteration 10769, loss = 0.30596031\n",
            "Iteration 10770, loss = 0.30595525\n",
            "Iteration 10771, loss = 0.30592125\n",
            "Iteration 10772, loss = 0.30596229\n",
            "Iteration 10773, loss = 0.30594959\n",
            "Iteration 10774, loss = 0.30586959\n",
            "Iteration 10775, loss = 0.30588284\n",
            "Iteration 10776, loss = 0.30588121\n",
            "Iteration 10777, loss = 0.30584997\n",
            "Iteration 10778, loss = 0.30584164\n",
            "Iteration 10779, loss = 0.30584481\n",
            "Iteration 10780, loss = 0.30582568\n",
            "Iteration 10781, loss = 0.30582684\n",
            "Iteration 10782, loss = 0.30579349\n",
            "Iteration 10783, loss = 0.30582963\n",
            "Iteration 10784, loss = 0.30583489\n",
            "Iteration 10785, loss = 0.30576631\n",
            "Iteration 10786, loss = 0.30575348\n",
            "Iteration 10787, loss = 0.30574602\n",
            "Iteration 10788, loss = 0.30573501\n",
            "Iteration 10789, loss = 0.30571126\n",
            "Iteration 10790, loss = 0.30568359\n",
            "Iteration 10791, loss = 0.30570968\n",
            "Iteration 10792, loss = 0.30568536\n",
            "Iteration 10793, loss = 0.30568149\n",
            "Iteration 10794, loss = 0.30566442\n",
            "Iteration 10795, loss = 0.30564089\n",
            "Iteration 10796, loss = 0.30561921\n",
            "Iteration 10797, loss = 0.30560923\n",
            "Iteration 10798, loss = 0.30558867\n",
            "Iteration 10799, loss = 0.30557606\n",
            "Iteration 10800, loss = 0.30556592\n",
            "Iteration 10801, loss = 0.30555410\n",
            "Iteration 10802, loss = 0.30554268\n",
            "Iteration 10803, loss = 0.30552031\n",
            "Iteration 10804, loss = 0.30553585\n",
            "Iteration 10805, loss = 0.30552697\n",
            "Iteration 10806, loss = 0.30549271\n",
            "Iteration 10807, loss = 0.30554921\n",
            "Iteration 10808, loss = 0.30550110\n",
            "Iteration 10809, loss = 0.30554892\n",
            "Iteration 10810, loss = 0.30546007\n",
            "Iteration 10811, loss = 0.30552996\n",
            "Iteration 10812, loss = 0.30550179\n",
            "Iteration 10813, loss = 0.30545125\n",
            "Iteration 10814, loss = 0.30547344\n",
            "Iteration 10815, loss = 0.30541883\n",
            "Iteration 10816, loss = 0.30542003\n",
            "Iteration 10817, loss = 0.30543943\n",
            "Iteration 10818, loss = 0.30539086\n",
            "Iteration 10819, loss = 0.30540821\n",
            "Iteration 10820, loss = 0.30542617\n",
            "Iteration 10821, loss = 0.30541531\n",
            "Iteration 10822, loss = 0.30536152\n",
            "Iteration 10823, loss = 0.30536918\n",
            "Iteration 10824, loss = 0.30534280\n",
            "Iteration 10825, loss = 0.30535739\n",
            "Iteration 10826, loss = 0.30532559\n",
            "Iteration 10827, loss = 0.30531014\n",
            "Iteration 10828, loss = 0.30535614\n",
            "Iteration 10829, loss = 0.30525602\n",
            "Iteration 10830, loss = 0.30532827\n",
            "Iteration 10831, loss = 0.30531460\n",
            "Iteration 10832, loss = 0.30528476\n",
            "Iteration 10833, loss = 0.30533072\n",
            "Iteration 10834, loss = 0.30520374\n",
            "Iteration 10835, loss = 0.30524365\n",
            "Iteration 10836, loss = 0.30523844\n",
            "Iteration 10837, loss = 0.30518706\n",
            "Iteration 10838, loss = 0.30526953\n",
            "Iteration 10839, loss = 0.30514455\n",
            "Iteration 10840, loss = 0.30518283\n",
            "Iteration 10841, loss = 0.30515640\n",
            "Iteration 10842, loss = 0.30511994\n",
            "Iteration 10843, loss = 0.30513792\n",
            "Iteration 10844, loss = 0.30510365\n",
            "Iteration 10845, loss = 0.30510157\n",
            "Iteration 10846, loss = 0.30509464\n",
            "Iteration 10847, loss = 0.30506985\n",
            "Iteration 10848, loss = 0.30506879\n",
            "Iteration 10849, loss = 0.30507582\n",
            "Iteration 10850, loss = 0.30505787\n",
            "Iteration 10851, loss = 0.30502792\n",
            "Iteration 10852, loss = 0.30505086\n",
            "Iteration 10853, loss = 0.30501710\n",
            "Iteration 10854, loss = 0.30509564\n",
            "Iteration 10855, loss = 0.30504562\n",
            "Iteration 10856, loss = 0.30497907\n",
            "Iteration 10857, loss = 0.30498245\n",
            "Iteration 10858, loss = 0.30493782\n",
            "Iteration 10859, loss = 0.30497790\n",
            "Iteration 10860, loss = 0.30494965\n",
            "Iteration 10861, loss = 0.30492298\n",
            "Iteration 10862, loss = 0.30490199\n",
            "Iteration 10863, loss = 0.30489963\n",
            "Iteration 10864, loss = 0.30489913\n",
            "Iteration 10865, loss = 0.30485985\n",
            "Iteration 10866, loss = 0.30490719\n",
            "Iteration 10867, loss = 0.30483694\n",
            "Iteration 10868, loss = 0.30489271\n",
            "Iteration 10869, loss = 0.30481980\n",
            "Iteration 10870, loss = 0.30487266\n",
            "Iteration 10871, loss = 0.30480944\n",
            "Iteration 10872, loss = 0.30481665\n",
            "Iteration 10873, loss = 0.30481913\n",
            "Iteration 10874, loss = 0.30475801\n",
            "Iteration 10875, loss = 0.30481785\n",
            "Iteration 10876, loss = 0.30474786\n",
            "Iteration 10877, loss = 0.30478431\n",
            "Iteration 10878, loss = 0.30477654\n",
            "Iteration 10879, loss = 0.30475350\n",
            "Iteration 10880, loss = 0.30473394\n",
            "Iteration 10881, loss = 0.30473696\n",
            "Iteration 10882, loss = 0.30469505\n",
            "Iteration 10883, loss = 0.30470709\n",
            "Iteration 10884, loss = 0.30466206\n",
            "Iteration 10885, loss = 0.30468966\n",
            "Iteration 10886, loss = 0.30465042\n",
            "Iteration 10887, loss = 0.30462909\n",
            "Iteration 10888, loss = 0.30462450\n",
            "Iteration 10889, loss = 0.30461777\n",
            "Iteration 10890, loss = 0.30458102\n",
            "Iteration 10891, loss = 0.30457838\n",
            "Iteration 10892, loss = 0.30456443\n",
            "Iteration 10893, loss = 0.30456551\n",
            "Iteration 10894, loss = 0.30456782\n",
            "Iteration 10895, loss = 0.30454048\n",
            "Iteration 10896, loss = 0.30453816\n",
            "Iteration 10897, loss = 0.30452699\n",
            "Iteration 10898, loss = 0.30449978\n",
            "Iteration 10899, loss = 0.30453942\n",
            "Iteration 10900, loss = 0.30452973\n",
            "Iteration 10901, loss = 0.30447599\n",
            "Iteration 10902, loss = 0.30448956\n",
            "Iteration 10903, loss = 0.30446814\n",
            "Iteration 10904, loss = 0.30448415\n",
            "Iteration 10905, loss = 0.30441016\n",
            "Iteration 10906, loss = 0.30445970\n",
            "Iteration 10907, loss = 0.30443308\n",
            "Iteration 10908, loss = 0.30444231\n",
            "Iteration 10909, loss = 0.30440901\n",
            "Iteration 10910, loss = 0.30441243\n",
            "Iteration 10911, loss = 0.30446203\n",
            "Iteration 10912, loss = 0.30434858\n",
            "Iteration 10913, loss = 0.30442272\n",
            "Iteration 10914, loss = 0.30438781\n",
            "Iteration 10915, loss = 0.30438514\n",
            "Iteration 10916, loss = 0.30435927\n",
            "Iteration 10917, loss = 0.30431764\n",
            "Iteration 10918, loss = 0.30438061\n",
            "Iteration 10919, loss = 0.30435599\n",
            "Iteration 10920, loss = 0.30426504\n",
            "Iteration 10921, loss = 0.30432355\n",
            "Iteration 10922, loss = 0.30430097\n",
            "Iteration 10923, loss = 0.30425225\n",
            "Iteration 10924, loss = 0.30428569\n",
            "Iteration 10925, loss = 0.30425771\n",
            "Iteration 10926, loss = 0.30428074\n",
            "Iteration 10927, loss = 0.30420436\n",
            "Iteration 10928, loss = 0.30419702\n",
            "Iteration 10929, loss = 0.30422616\n",
            "Iteration 10930, loss = 0.30419213\n",
            "Iteration 10931, loss = 0.30416129\n",
            "Iteration 10932, loss = 0.30417060\n",
            "Iteration 10933, loss = 0.30415805\n",
            "Iteration 10934, loss = 0.30413392\n",
            "Iteration 10935, loss = 0.30410627\n",
            "Iteration 10936, loss = 0.30412478\n",
            "Iteration 10937, loss = 0.30409566\n",
            "Iteration 10938, loss = 0.30407953\n",
            "Iteration 10939, loss = 0.30407705\n",
            "Iteration 10940, loss = 0.30407293\n",
            "Iteration 10941, loss = 0.30404909\n",
            "Iteration 10942, loss = 0.30403292\n",
            "Iteration 10943, loss = 0.30401605\n",
            "Iteration 10944, loss = 0.30401757\n",
            "Iteration 10945, loss = 0.30400819\n",
            "Iteration 10946, loss = 0.30398724\n",
            "Iteration 10947, loss = 0.30398957\n",
            "Iteration 10948, loss = 0.30397495\n",
            "Iteration 10949, loss = 0.30401185\n",
            "Iteration 10950, loss = 0.30395611\n",
            "Iteration 10951, loss = 0.30395189\n",
            "Iteration 10952, loss = 0.30394199\n",
            "Iteration 10953, loss = 0.30393354\n",
            "Iteration 10954, loss = 0.30389970\n",
            "Iteration 10955, loss = 0.30394493\n",
            "Iteration 10956, loss = 0.30389500\n",
            "Iteration 10957, loss = 0.30389185\n",
            "Iteration 10958, loss = 0.30389283\n",
            "Iteration 10959, loss = 0.30387810\n",
            "Iteration 10960, loss = 0.30384691\n",
            "Iteration 10961, loss = 0.30384382\n",
            "Iteration 10962, loss = 0.30386538\n",
            "Iteration 10963, loss = 0.30380466\n",
            "Iteration 10964, loss = 0.30388168\n",
            "Iteration 10965, loss = 0.30383680\n",
            "Iteration 10966, loss = 0.30382221\n",
            "Iteration 10967, loss = 0.30381714\n",
            "Iteration 10968, loss = 0.30377571\n",
            "Iteration 10969, loss = 0.30383041\n",
            "Iteration 10970, loss = 0.30378890\n",
            "Iteration 10971, loss = 0.30373652\n",
            "Iteration 10972, loss = 0.30382815\n",
            "Iteration 10973, loss = 0.30369836\n",
            "Iteration 10974, loss = 0.30373432\n",
            "Iteration 10975, loss = 0.30371787\n",
            "Iteration 10976, loss = 0.30371635\n",
            "Iteration 10977, loss = 0.30366548\n",
            "Iteration 10978, loss = 0.30367651\n",
            "Iteration 10979, loss = 0.30368706\n",
            "Iteration 10980, loss = 0.30368328\n",
            "Iteration 10981, loss = 0.30363351\n",
            "Iteration 10982, loss = 0.30364645\n",
            "Iteration 10983, loss = 0.30367491\n",
            "Iteration 10984, loss = 0.30358647\n",
            "Iteration 10985, loss = 0.30364121\n",
            "Iteration 10986, loss = 0.30362172\n",
            "Iteration 10987, loss = 0.30360086\n",
            "Iteration 10988, loss = 0.30360677\n",
            "Iteration 10989, loss = 0.30354763\n",
            "Iteration 10990, loss = 0.30352983\n",
            "Iteration 10991, loss = 0.30353993\n",
            "Iteration 10992, loss = 0.30349759\n",
            "Iteration 10993, loss = 0.30350918\n",
            "Iteration 10994, loss = 0.30351090\n",
            "Iteration 10995, loss = 0.30348544\n",
            "Iteration 10996, loss = 0.30346957\n",
            "Iteration 10997, loss = 0.30344568\n",
            "Iteration 10998, loss = 0.30347144\n",
            "Iteration 10999, loss = 0.30342789\n",
            "Iteration 11000, loss = 0.30343764\n",
            "Iteration 11001, loss = 0.30342916\n",
            "Iteration 11002, loss = 0.30340121\n",
            "Iteration 11003, loss = 0.30339933\n",
            "Iteration 11004, loss = 0.30338486\n",
            "Iteration 11005, loss = 0.30336133\n",
            "Iteration 11006, loss = 0.30334947\n",
            "Iteration 11007, loss = 0.30335579\n",
            "Iteration 11008, loss = 0.30333071\n",
            "Iteration 11009, loss = 0.30330765\n",
            "Iteration 11010, loss = 0.30332527\n",
            "Iteration 11011, loss = 0.30330246\n",
            "Iteration 11012, loss = 0.30327975\n",
            "Iteration 11013, loss = 0.30326655\n",
            "Iteration 11014, loss = 0.30326486\n",
            "Iteration 11015, loss = 0.30326314\n",
            "Iteration 11016, loss = 0.30323498\n",
            "Iteration 11017, loss = 0.30323872\n",
            "Iteration 11018, loss = 0.30322079\n",
            "Iteration 11019, loss = 0.30322364\n",
            "Iteration 11020, loss = 0.30320842\n",
            "Iteration 11021, loss = 0.30318706\n",
            "Iteration 11022, loss = 0.30321695\n",
            "Iteration 11023, loss = 0.30318152\n",
            "Iteration 11024, loss = 0.30316720\n",
            "Iteration 11025, loss = 0.30315113\n",
            "Iteration 11026, loss = 0.30314493\n",
            "Iteration 11027, loss = 0.30311423\n",
            "Iteration 11028, loss = 0.30317402\n",
            "Iteration 11029, loss = 0.30315017\n",
            "Iteration 11030, loss = 0.30313713\n",
            "Iteration 11031, loss = 0.30314447\n",
            "Iteration 11032, loss = 0.30306419\n",
            "Iteration 11033, loss = 0.30309013\n",
            "Iteration 11034, loss = 0.30305059\n",
            "Iteration 11035, loss = 0.30308955\n",
            "Iteration 11036, loss = 0.30305611\n",
            "Iteration 11037, loss = 0.30306713\n",
            "Iteration 11038, loss = 0.30303296\n",
            "Iteration 11039, loss = 0.30301773\n",
            "Iteration 11040, loss = 0.30306483\n",
            "Iteration 11041, loss = 0.30299326\n",
            "Iteration 11042, loss = 0.30305400\n",
            "Iteration 11043, loss = 0.30299306\n",
            "Iteration 11044, loss = 0.30298358\n",
            "Iteration 11045, loss = 0.30300317\n",
            "Iteration 11046, loss = 0.30297272\n",
            "Iteration 11047, loss = 0.30296234\n",
            "Iteration 11048, loss = 0.30291487\n",
            "Iteration 11049, loss = 0.30292386\n",
            "Iteration 11050, loss = 0.30294876\n",
            "Iteration 11051, loss = 0.30292506\n",
            "Iteration 11052, loss = 0.30287329\n",
            "Iteration 11053, loss = 0.30287791\n",
            "Iteration 11054, loss = 0.30288230\n",
            "Iteration 11055, loss = 0.30285563\n",
            "Iteration 11056, loss = 0.30282669\n",
            "Iteration 11057, loss = 0.30281873\n",
            "Iteration 11058, loss = 0.30282241\n",
            "Iteration 11059, loss = 0.30279255\n",
            "Iteration 11060, loss = 0.30277478\n",
            "Iteration 11061, loss = 0.30278003\n",
            "Iteration 11062, loss = 0.30276067\n",
            "Iteration 11063, loss = 0.30274265\n",
            "Iteration 11064, loss = 0.30274132\n",
            "Iteration 11065, loss = 0.30272358\n",
            "Iteration 11066, loss = 0.30270829\n",
            "Iteration 11067, loss = 0.30271353\n",
            "Iteration 11068, loss = 0.30269836\n",
            "Iteration 11069, loss = 0.30268163\n",
            "Iteration 11070, loss = 0.30268651\n",
            "Iteration 11071, loss = 0.30270453\n",
            "Iteration 11072, loss = 0.30265180\n",
            "Iteration 11073, loss = 0.30267436\n",
            "Iteration 11074, loss = 0.30265166\n",
            "Iteration 11075, loss = 0.30263474\n",
            "Iteration 11076, loss = 0.30260743\n",
            "Iteration 11077, loss = 0.30262036\n",
            "Iteration 11078, loss = 0.30258668\n",
            "Iteration 11079, loss = 0.30261951\n",
            "Iteration 11080, loss = 0.30259911\n",
            "Iteration 11081, loss = 0.30256894\n",
            "Iteration 11082, loss = 0.30257583\n",
            "Iteration 11083, loss = 0.30255017\n",
            "Iteration 11084, loss = 0.30256240\n",
            "Iteration 11085, loss = 0.30252046\n",
            "Iteration 11086, loss = 0.30251704\n",
            "Iteration 11087, loss = 0.30251330\n",
            "Iteration 11088, loss = 0.30249493\n",
            "Iteration 11089, loss = 0.30248720\n",
            "Iteration 11090, loss = 0.30247860\n",
            "Iteration 11091, loss = 0.30245673\n",
            "Iteration 11092, loss = 0.30246285\n",
            "Iteration 11093, loss = 0.30243943\n",
            "Iteration 11094, loss = 0.30242906\n",
            "Iteration 11095, loss = 0.30243657\n",
            "Iteration 11096, loss = 0.30242597\n",
            "Iteration 11097, loss = 0.30239971\n",
            "Iteration 11098, loss = 0.30237890\n",
            "Iteration 11099, loss = 0.30238218\n",
            "Iteration 11100, loss = 0.30237398\n",
            "Iteration 11101, loss = 0.30233759\n",
            "Iteration 11102, loss = 0.30232626\n",
            "Iteration 11103, loss = 0.30232597\n",
            "Iteration 11104, loss = 0.30232952\n",
            "Iteration 11105, loss = 0.30230355\n",
            "Iteration 11106, loss = 0.30231082\n",
            "Iteration 11107, loss = 0.30230992\n",
            "Iteration 11108, loss = 0.30229760\n",
            "Iteration 11109, loss = 0.30228213\n",
            "Iteration 11110, loss = 0.30232057\n",
            "Iteration 11111, loss = 0.30227324\n",
            "Iteration 11112, loss = 0.30229147\n",
            "Iteration 11113, loss = 0.30230711\n",
            "Iteration 11114, loss = 0.30229821\n",
            "Iteration 11115, loss = 0.30224751\n",
            "Iteration 11116, loss = 0.30225050\n",
            "Iteration 11117, loss = 0.30219835\n",
            "Iteration 11118, loss = 0.30231048\n",
            "Iteration 11119, loss = 0.30230339\n",
            "Iteration 11120, loss = 0.30218967\n",
            "Iteration 11121, loss = 0.30225481\n",
            "Iteration 11122, loss = 0.30229689\n",
            "Iteration 11123, loss = 0.30216102\n",
            "Iteration 11124, loss = 0.30224343\n",
            "Iteration 11125, loss = 0.30220669\n",
            "Iteration 11126, loss = 0.30212790\n",
            "Iteration 11127, loss = 0.30225866\n",
            "Iteration 11128, loss = 0.30216913\n",
            "Iteration 11129, loss = 0.30209837\n",
            "Iteration 11130, loss = 0.30215749\n",
            "Iteration 11131, loss = 0.30209087\n",
            "Iteration 11132, loss = 0.30208178\n",
            "Iteration 11133, loss = 0.30210965\n",
            "Iteration 11134, loss = 0.30202112\n",
            "Iteration 11135, loss = 0.30204814\n",
            "Iteration 11136, loss = 0.30206745\n",
            "Iteration 11137, loss = 0.30201517\n",
            "Iteration 11138, loss = 0.30199178\n",
            "Iteration 11139, loss = 0.30200661\n",
            "Iteration 11140, loss = 0.30201965\n",
            "Iteration 11141, loss = 0.30204374\n",
            "Iteration 11142, loss = 0.30199142\n",
            "Iteration 11143, loss = 0.30191568\n",
            "Iteration 11144, loss = 0.30196703\n",
            "Iteration 11145, loss = 0.30191655\n",
            "Iteration 11146, loss = 0.30191545\n",
            "Iteration 11147, loss = 0.30190600\n",
            "Iteration 11148, loss = 0.30191897\n",
            "Iteration 11149, loss = 0.30191559\n",
            "Iteration 11150, loss = 0.30186296\n",
            "Iteration 11151, loss = 0.30183034\n",
            "Iteration 11152, loss = 0.30185068\n",
            "Iteration 11153, loss = 0.30184221\n",
            "Iteration 11154, loss = 0.30183024\n",
            "Iteration 11155, loss = 0.30179664\n",
            "Iteration 11156, loss = 0.30181185\n",
            "Iteration 11157, loss = 0.30180326\n",
            "Iteration 11158, loss = 0.30177941\n",
            "Iteration 11159, loss = 0.30176775\n",
            "Iteration 11160, loss = 0.30175600\n",
            "Iteration 11161, loss = 0.30176256\n",
            "Iteration 11162, loss = 0.30173524\n",
            "Iteration 11163, loss = 0.30173172\n",
            "Iteration 11164, loss = 0.30173093\n",
            "Iteration 11165, loss = 0.30169976\n",
            "Iteration 11166, loss = 0.30166757\n",
            "Iteration 11167, loss = 0.30168404\n",
            "Iteration 11168, loss = 0.30166625\n",
            "Iteration 11169, loss = 0.30164230\n",
            "Iteration 11170, loss = 0.30163596\n",
            "Iteration 11171, loss = 0.30165368\n",
            "Iteration 11172, loss = 0.30163782\n",
            "Iteration 11173, loss = 0.30159390\n",
            "Iteration 11174, loss = 0.30164025\n",
            "Iteration 11175, loss = 0.30158629\n",
            "Iteration 11176, loss = 0.30161450\n",
            "Iteration 11177, loss = 0.30158879\n",
            "Iteration 11178, loss = 0.30159172\n",
            "Iteration 11179, loss = 0.30161424\n",
            "Iteration 11180, loss = 0.30153124\n",
            "Iteration 11181, loss = 0.30161995\n",
            "Iteration 11182, loss = 0.30162294\n",
            "Iteration 11183, loss = 0.30154331\n",
            "Iteration 11184, loss = 0.30160362\n",
            "Iteration 11185, loss = 0.30158170\n",
            "Iteration 11186, loss = 0.30154311\n",
            "Iteration 11187, loss = 0.30164606\n",
            "Iteration 11188, loss = 0.30159159\n",
            "Iteration 11189, loss = 0.30147557\n",
            "Iteration 11190, loss = 0.30158968\n",
            "Iteration 11191, loss = 0.30151798\n",
            "Iteration 11192, loss = 0.30145008\n",
            "Iteration 11193, loss = 0.30153530\n",
            "Iteration 11194, loss = 0.30148757\n",
            "Iteration 11195, loss = 0.30139549\n",
            "Iteration 11196, loss = 0.30143509\n",
            "Iteration 11197, loss = 0.30139858\n",
            "Iteration 11198, loss = 0.30136312\n",
            "Iteration 11199, loss = 0.30137597\n",
            "Iteration 11200, loss = 0.30133848\n",
            "Iteration 11201, loss = 0.30133160\n",
            "Iteration 11202, loss = 0.30130854\n",
            "Iteration 11203, loss = 0.30131817\n",
            "Iteration 11204, loss = 0.30130777\n",
            "Iteration 11205, loss = 0.30128024\n",
            "Iteration 11206, loss = 0.30126834\n",
            "Iteration 11207, loss = 0.30125891\n",
            "Iteration 11208, loss = 0.30127152\n",
            "Iteration 11209, loss = 0.30124109\n",
            "Iteration 11210, loss = 0.30123052\n",
            "Iteration 11211, loss = 0.30122609\n",
            "Iteration 11212, loss = 0.30120681\n",
            "Iteration 11213, loss = 0.30121998\n",
            "Iteration 11214, loss = 0.30119514\n",
            "Iteration 11215, loss = 0.30117526\n",
            "Iteration 11216, loss = 0.30116798\n",
            "Iteration 11217, loss = 0.30115389\n",
            "Iteration 11218, loss = 0.30113771\n",
            "Iteration 11219, loss = 0.30114061\n",
            "Iteration 11220, loss = 0.30112313\n",
            "Iteration 11221, loss = 0.30112805\n",
            "Iteration 11222, loss = 0.30111834\n",
            "Iteration 11223, loss = 0.30108477\n",
            "Iteration 11224, loss = 0.30109896\n",
            "Iteration 11225, loss = 0.30108242\n",
            "Iteration 11226, loss = 0.30108223\n",
            "Iteration 11227, loss = 0.30104127\n",
            "Iteration 11228, loss = 0.30103214\n",
            "Iteration 11229, loss = 0.30102607\n",
            "Iteration 11230, loss = 0.30102283\n",
            "Iteration 11231, loss = 0.30101571\n",
            "Iteration 11232, loss = 0.30100113\n",
            "Iteration 11233, loss = 0.30101757\n",
            "Iteration 11234, loss = 0.30097282\n",
            "Iteration 11235, loss = 0.30101072\n",
            "Iteration 11236, loss = 0.30099373\n",
            "Iteration 11237, loss = 0.30094291\n",
            "Iteration 11238, loss = 0.30096766\n",
            "Iteration 11239, loss = 0.30094063\n",
            "Iteration 11240, loss = 0.30096403\n",
            "Iteration 11241, loss = 0.30093217\n",
            "Iteration 11242, loss = 0.30093608\n",
            "Iteration 11243, loss = 0.30093748\n",
            "Iteration 11244, loss = 0.30092894\n",
            "Iteration 11245, loss = 0.30085536\n",
            "Iteration 11246, loss = 0.30093016\n",
            "Iteration 11247, loss = 0.30088729\n",
            "Iteration 11248, loss = 0.30088231\n",
            "Iteration 11249, loss = 0.30088344\n",
            "Iteration 11250, loss = 0.30087032\n",
            "Iteration 11251, loss = 0.30086581\n",
            "Iteration 11252, loss = 0.30083009\n",
            "Iteration 11253, loss = 0.30080825\n",
            "Iteration 11254, loss = 0.30082873\n",
            "Iteration 11255, loss = 0.30083105\n",
            "Iteration 11256, loss = 0.30077878\n",
            "Iteration 11257, loss = 0.30076165\n",
            "Iteration 11258, loss = 0.30076779\n",
            "Iteration 11259, loss = 0.30075077\n",
            "Iteration 11260, loss = 0.30071767\n",
            "Iteration 11261, loss = 0.30071011\n",
            "Iteration 11262, loss = 0.30071039\n",
            "Iteration 11263, loss = 0.30069798\n",
            "Iteration 11264, loss = 0.30068017\n",
            "Iteration 11265, loss = 0.30068010\n",
            "Iteration 11266, loss = 0.30066441\n",
            "Iteration 11267, loss = 0.30064497\n",
            "Iteration 11268, loss = 0.30063972\n",
            "Iteration 11269, loss = 0.30062990\n",
            "Iteration 11270, loss = 0.30061562\n",
            "Iteration 11271, loss = 0.30060281\n",
            "Iteration 11272, loss = 0.30059431\n",
            "Iteration 11273, loss = 0.30058428\n",
            "Iteration 11274, loss = 0.30058215\n",
            "Iteration 11275, loss = 0.30056892\n",
            "Iteration 11276, loss = 0.30053916\n",
            "Iteration 11277, loss = 0.30054973\n",
            "Iteration 11278, loss = 0.30053097\n",
            "Iteration 11279, loss = 0.30051962\n",
            "Iteration 11280, loss = 0.30049604\n",
            "Iteration 11281, loss = 0.30051795\n",
            "Iteration 11282, loss = 0.30049823\n",
            "Iteration 11283, loss = 0.30046572\n",
            "Iteration 11284, loss = 0.30046937\n",
            "Iteration 11285, loss = 0.30046469\n",
            "Iteration 11286, loss = 0.30044217\n",
            "Iteration 11287, loss = 0.30048680\n",
            "Iteration 11288, loss = 0.30045389\n",
            "Iteration 11289, loss = 0.30045804\n",
            "Iteration 11290, loss = 0.30044439\n",
            "Iteration 11291, loss = 0.30043885\n",
            "Iteration 11292, loss = 0.30044667\n",
            "Iteration 11293, loss = 0.30040244\n",
            "Iteration 11294, loss = 0.30036535\n",
            "Iteration 11295, loss = 0.30037695\n",
            "Iteration 11296, loss = 0.30036134\n",
            "Iteration 11297, loss = 0.30031970\n",
            "Iteration 11298, loss = 0.30035962\n",
            "Iteration 11299, loss = 0.30039122\n",
            "Iteration 11300, loss = 0.30033896\n",
            "Iteration 11301, loss = 0.30028589\n",
            "Iteration 11302, loss = 0.30031862\n",
            "Iteration 11303, loss = 0.30027722\n",
            "Iteration 11304, loss = 0.30028696\n",
            "Iteration 11305, loss = 0.30026524\n",
            "Iteration 11306, loss = 0.30026770\n",
            "Iteration 11307, loss = 0.30023793\n",
            "Iteration 11308, loss = 0.30024862\n",
            "Iteration 11309, loss = 0.30024617\n",
            "Iteration 11310, loss = 0.30020184\n",
            "Iteration 11311, loss = 0.30021909\n",
            "Iteration 11312, loss = 0.30020717\n",
            "Iteration 11313, loss = 0.30017988\n",
            "Iteration 11314, loss = 0.30017630\n",
            "Iteration 11315, loss = 0.30016929\n",
            "Iteration 11316, loss = 0.30014018\n",
            "Iteration 11317, loss = 0.30015018\n",
            "Iteration 11318, loss = 0.30011761\n",
            "Iteration 11319, loss = 0.30014510\n",
            "Iteration 11320, loss = 0.30009725\n",
            "Iteration 11321, loss = 0.30010656\n",
            "Iteration 11322, loss = 0.30009156\n",
            "Iteration 11323, loss = 0.30007308\n",
            "Iteration 11324, loss = 0.30007846\n",
            "Iteration 11325, loss = 0.30005412\n",
            "Iteration 11326, loss = 0.30005380\n",
            "Iteration 11327, loss = 0.30004526\n",
            "Iteration 11328, loss = 0.30004594\n",
            "Iteration 11329, loss = 0.30001491\n",
            "Iteration 11330, loss = 0.30002271\n",
            "Iteration 11331, loss = 0.29998257\n",
            "Iteration 11332, loss = 0.30002586\n",
            "Iteration 11333, loss = 0.29998197\n",
            "Iteration 11334, loss = 0.29996219\n",
            "Iteration 11335, loss = 0.29994128\n",
            "Iteration 11336, loss = 0.29995846\n",
            "Iteration 11337, loss = 0.29991833\n",
            "Iteration 11338, loss = 0.29994925\n",
            "Iteration 11339, loss = 0.29995036\n",
            "Iteration 11340, loss = 0.29990879\n",
            "Iteration 11341, loss = 0.29993573\n",
            "Iteration 11342, loss = 0.29991400\n",
            "Iteration 11343, loss = 0.29987911\n",
            "Iteration 11344, loss = 0.29989961\n",
            "Iteration 11345, loss = 0.29985307\n",
            "Iteration 11346, loss = 0.29987913\n",
            "Iteration 11347, loss = 0.29983762\n",
            "Iteration 11348, loss = 0.29982230\n",
            "Iteration 11349, loss = 0.29984527\n",
            "Iteration 11350, loss = 0.29980549\n",
            "Iteration 11351, loss = 0.29983386\n",
            "Iteration 11352, loss = 0.29982671\n",
            "Iteration 11353, loss = 0.29977578\n",
            "Iteration 11354, loss = 0.29979672\n",
            "Iteration 11355, loss = 0.29975388\n",
            "Iteration 11356, loss = 0.29973831\n",
            "Iteration 11357, loss = 0.29974790\n",
            "Iteration 11358, loss = 0.29973055\n",
            "Iteration 11359, loss = 0.29972503\n",
            "Iteration 11360, loss = 0.29971249\n",
            "Iteration 11361, loss = 0.29969660\n",
            "Iteration 11362, loss = 0.29968756\n",
            "Iteration 11363, loss = 0.29967586\n",
            "Iteration 11364, loss = 0.29966320\n",
            "Iteration 11365, loss = 0.29965588\n",
            "Iteration 11366, loss = 0.29964241\n",
            "Iteration 11367, loss = 0.29965448\n",
            "Iteration 11368, loss = 0.29962046\n",
            "Iteration 11369, loss = 0.29964334\n",
            "Iteration 11370, loss = 0.29959332\n",
            "Iteration 11371, loss = 0.29960309\n",
            "Iteration 11372, loss = 0.29958886\n",
            "Iteration 11373, loss = 0.29959966\n",
            "Iteration 11374, loss = 0.29955800\n",
            "Iteration 11375, loss = 0.29956669\n",
            "Iteration 11376, loss = 0.29955567\n",
            "Iteration 11377, loss = 0.29952414\n",
            "Iteration 11378, loss = 0.29952518\n",
            "Iteration 11379, loss = 0.29953869\n",
            "Iteration 11380, loss = 0.29952175\n",
            "Iteration 11381, loss = 0.29949702\n",
            "Iteration 11382, loss = 0.29951930\n",
            "Iteration 11383, loss = 0.29947105\n",
            "Iteration 11384, loss = 0.29947975\n",
            "Iteration 11385, loss = 0.29946024\n",
            "Iteration 11386, loss = 0.29944492\n",
            "Iteration 11387, loss = 0.29944633\n",
            "Iteration 11388, loss = 0.29942474\n",
            "Iteration 11389, loss = 0.29939167\n",
            "Iteration 11390, loss = 0.29941232\n",
            "Iteration 11391, loss = 0.29943425\n",
            "Iteration 11392, loss = 0.29939258\n",
            "Iteration 11393, loss = 0.29935816\n",
            "Iteration 11394, loss = 0.29938381\n",
            "Iteration 11395, loss = 0.29935876\n",
            "Iteration 11396, loss = 0.29933952\n",
            "Iteration 11397, loss = 0.29932825\n",
            "Iteration 11398, loss = 0.29933702\n",
            "Iteration 11399, loss = 0.29932070\n",
            "Iteration 11400, loss = 0.29931718\n",
            "Iteration 11401, loss = 0.29931307\n",
            "Iteration 11402, loss = 0.29928640\n",
            "Iteration 11403, loss = 0.29929686\n",
            "Iteration 11404, loss = 0.29925172\n",
            "Iteration 11405, loss = 0.29925958\n",
            "Iteration 11406, loss = 0.29922881\n",
            "Iteration 11407, loss = 0.29923980\n",
            "Iteration 11408, loss = 0.29922825\n",
            "Iteration 11409, loss = 0.29920624\n",
            "Iteration 11410, loss = 0.29920656\n",
            "Iteration 11411, loss = 0.29918086\n",
            "Iteration 11412, loss = 0.29918285\n",
            "Iteration 11413, loss = 0.29917069\n",
            "Iteration 11414, loss = 0.29914950\n",
            "Iteration 11415, loss = 0.29918066\n",
            "Iteration 11416, loss = 0.29913481\n",
            "Iteration 11417, loss = 0.29913789\n",
            "Iteration 11418, loss = 0.29911414\n",
            "Iteration 11419, loss = 0.29912902\n",
            "Iteration 11420, loss = 0.29910118\n",
            "Iteration 11421, loss = 0.29912518\n",
            "Iteration 11422, loss = 0.29914067\n",
            "Iteration 11423, loss = 0.29906427\n",
            "Iteration 11424, loss = 0.29912939\n",
            "Iteration 11425, loss = 0.29909622\n",
            "Iteration 11426, loss = 0.29905519\n",
            "Iteration 11427, loss = 0.29902002\n",
            "Iteration 11428, loss = 0.29904327\n",
            "Iteration 11429, loss = 0.29906177\n",
            "Iteration 11430, loss = 0.29899347\n",
            "Iteration 11431, loss = 0.29905754\n",
            "Iteration 11432, loss = 0.29900794\n",
            "Iteration 11433, loss = 0.29899580\n",
            "Iteration 11434, loss = 0.29897417\n",
            "Iteration 11435, loss = 0.29899343\n",
            "Iteration 11436, loss = 0.29903201\n",
            "Iteration 11437, loss = 0.29895049\n",
            "Iteration 11438, loss = 0.29894592\n",
            "Iteration 11439, loss = 0.29892982\n",
            "Iteration 11440, loss = 0.29890621\n",
            "Iteration 11441, loss = 0.29892407\n",
            "Iteration 11442, loss = 0.29888963\n",
            "Iteration 11443, loss = 0.29887455\n",
            "Iteration 11444, loss = 0.29887883\n",
            "Iteration 11445, loss = 0.29884716\n",
            "Iteration 11446, loss = 0.29884693\n",
            "Iteration 11447, loss = 0.29883613\n",
            "Iteration 11448, loss = 0.29881701\n",
            "Iteration 11449, loss = 0.29880489\n",
            "Iteration 11450, loss = 0.29879532\n",
            "Iteration 11451, loss = 0.29879062\n",
            "Iteration 11452, loss = 0.29878285\n",
            "Iteration 11453, loss = 0.29876914\n",
            "Iteration 11454, loss = 0.29876149\n",
            "Iteration 11455, loss = 0.29874685\n",
            "Iteration 11456, loss = 0.29873627\n",
            "Iteration 11457, loss = 0.29872917\n",
            "Iteration 11458, loss = 0.29871777\n",
            "Iteration 11459, loss = 0.29870646\n",
            "Iteration 11460, loss = 0.29869105\n",
            "Iteration 11461, loss = 0.29867671\n",
            "Iteration 11462, loss = 0.29866088\n",
            "Iteration 11463, loss = 0.29866178\n",
            "Iteration 11464, loss = 0.29867610\n",
            "Iteration 11465, loss = 0.29866502\n",
            "Iteration 11466, loss = 0.29866413\n",
            "Iteration 11467, loss = 0.29866002\n",
            "Iteration 11468, loss = 0.29860522\n",
            "Iteration 11469, loss = 0.29862117\n",
            "Iteration 11470, loss = 0.29859666\n",
            "Iteration 11471, loss = 0.29858148\n",
            "Iteration 11472, loss = 0.29858050\n",
            "Iteration 11473, loss = 0.29857247\n",
            "Iteration 11474, loss = 0.29856053\n",
            "Iteration 11475, loss = 0.29854283\n",
            "Iteration 11476, loss = 0.29853696\n",
            "Iteration 11477, loss = 0.29853614\n",
            "Iteration 11478, loss = 0.29851223\n",
            "Iteration 11479, loss = 0.29850085\n",
            "Iteration 11480, loss = 0.29851746\n",
            "Iteration 11481, loss = 0.29847946\n",
            "Iteration 11482, loss = 0.29846767\n",
            "Iteration 11483, loss = 0.29847106\n",
            "Iteration 11484, loss = 0.29845974\n",
            "Iteration 11485, loss = 0.29844279\n",
            "Iteration 11486, loss = 0.29843880\n",
            "Iteration 11487, loss = 0.29842337\n",
            "Iteration 11488, loss = 0.29843619\n",
            "Iteration 11489, loss = 0.29841564\n",
            "Iteration 11490, loss = 0.29842301\n",
            "Iteration 11491, loss = 0.29839870\n",
            "Iteration 11492, loss = 0.29841287\n",
            "Iteration 11493, loss = 0.29839990\n",
            "Iteration 11494, loss = 0.29837122\n",
            "Iteration 11495, loss = 0.29835564\n",
            "Iteration 11496, loss = 0.29838526\n",
            "Iteration 11497, loss = 0.29837729\n",
            "Iteration 11498, loss = 0.29832747\n",
            "Iteration 11499, loss = 0.29831088\n",
            "Iteration 11500, loss = 0.29829723\n",
            "Iteration 11501, loss = 0.29831162\n",
            "Iteration 11502, loss = 0.29826986\n",
            "Iteration 11503, loss = 0.29828780\n",
            "Iteration 11504, loss = 0.29826064\n",
            "Iteration 11505, loss = 0.29825053\n",
            "Iteration 11506, loss = 0.29825912\n",
            "Iteration 11507, loss = 0.29824071\n",
            "Iteration 11508, loss = 0.29826453\n",
            "Iteration 11509, loss = 0.29823474\n",
            "Iteration 11510, loss = 0.29821327\n",
            "Iteration 11511, loss = 0.29823593\n",
            "Iteration 11512, loss = 0.29818427\n",
            "Iteration 11513, loss = 0.29818259\n",
            "Iteration 11514, loss = 0.29816547\n",
            "Iteration 11515, loss = 0.29816173\n",
            "Iteration 11516, loss = 0.29814886\n",
            "Iteration 11517, loss = 0.29813095\n",
            "Iteration 11518, loss = 0.29812857\n",
            "Iteration 11519, loss = 0.29811027\n",
            "Iteration 11520, loss = 0.29808926\n",
            "Iteration 11521, loss = 0.29808805\n",
            "Iteration 11522, loss = 0.29808615\n",
            "Iteration 11523, loss = 0.29806776\n",
            "Iteration 11524, loss = 0.29805599\n",
            "Iteration 11525, loss = 0.29804220\n",
            "Iteration 11526, loss = 0.29804738\n",
            "Iteration 11527, loss = 0.29803464\n",
            "Iteration 11528, loss = 0.29804037\n",
            "Iteration 11529, loss = 0.29801922\n",
            "Iteration 11530, loss = 0.29803536\n",
            "Iteration 11531, loss = 0.29802059\n",
            "Iteration 11532, loss = 0.29799818\n",
            "Iteration 11533, loss = 0.29796569\n",
            "Iteration 11534, loss = 0.29800086\n",
            "Iteration 11535, loss = 0.29796642\n",
            "Iteration 11536, loss = 0.29796407\n",
            "Iteration 11537, loss = 0.29796469\n",
            "Iteration 11538, loss = 0.29793039\n",
            "Iteration 11539, loss = 0.29794233\n",
            "Iteration 11540, loss = 0.29789043\n",
            "Iteration 11541, loss = 0.29796985\n",
            "Iteration 11542, loss = 0.29792179\n",
            "Iteration 11543, loss = 0.29786982\n",
            "Iteration 11544, loss = 0.29793334\n",
            "Iteration 11545, loss = 0.29786119\n",
            "Iteration 11546, loss = 0.29793179\n",
            "Iteration 11547, loss = 0.29787889\n",
            "Iteration 11548, loss = 0.29782556\n",
            "Iteration 11549, loss = 0.29787362\n",
            "Iteration 11550, loss = 0.29783930\n",
            "Iteration 11551, loss = 0.29781263\n",
            "Iteration 11552, loss = 0.29788000\n",
            "Iteration 11553, loss = 0.29778736\n",
            "Iteration 11554, loss = 0.29779814\n",
            "Iteration 11555, loss = 0.29778608\n",
            "Iteration 11556, loss = 0.29775760\n",
            "Iteration 11557, loss = 0.29778888\n",
            "Iteration 11558, loss = 0.29773728\n",
            "Iteration 11559, loss = 0.29771975\n",
            "Iteration 11560, loss = 0.29778805\n",
            "Iteration 11561, loss = 0.29772604\n",
            "Iteration 11562, loss = 0.29768888\n",
            "Iteration 11563, loss = 0.29771270\n",
            "Iteration 11564, loss = 0.29769166\n",
            "Iteration 11565, loss = 0.29765836\n",
            "Iteration 11566, loss = 0.29766240\n",
            "Iteration 11567, loss = 0.29764578\n",
            "Iteration 11568, loss = 0.29763709\n",
            "Iteration 11569, loss = 0.29762226\n",
            "Iteration 11570, loss = 0.29761100\n",
            "Iteration 11571, loss = 0.29761185\n",
            "Iteration 11572, loss = 0.29758955\n",
            "Iteration 11573, loss = 0.29758334\n",
            "Iteration 11574, loss = 0.29757880\n",
            "Iteration 11575, loss = 0.29757624\n",
            "Iteration 11576, loss = 0.29754841\n",
            "Iteration 11577, loss = 0.29752540\n",
            "Iteration 11578, loss = 0.29752136\n",
            "Iteration 11579, loss = 0.29752476\n",
            "Iteration 11580, loss = 0.29748852\n",
            "Iteration 11581, loss = 0.29749012\n",
            "Iteration 11582, loss = 0.29750306\n",
            "Iteration 11583, loss = 0.29748111\n",
            "Iteration 11584, loss = 0.29746652\n",
            "Iteration 11585, loss = 0.29748502\n",
            "Iteration 11586, loss = 0.29746104\n",
            "Iteration 11587, loss = 0.29747165\n",
            "Iteration 11588, loss = 0.29746791\n",
            "Iteration 11589, loss = 0.29742432\n",
            "Iteration 11590, loss = 0.29742830\n",
            "Iteration 11591, loss = 0.29740270\n",
            "Iteration 11592, loss = 0.29739782\n",
            "Iteration 11593, loss = 0.29736913\n",
            "Iteration 11594, loss = 0.29736398\n",
            "Iteration 11595, loss = 0.29735233\n",
            "Iteration 11596, loss = 0.29734142\n",
            "Iteration 11597, loss = 0.29732966\n",
            "Iteration 11598, loss = 0.29736104\n",
            "Iteration 11599, loss = 0.29731102\n",
            "Iteration 11600, loss = 0.29734008\n",
            "Iteration 11601, loss = 0.29732705\n",
            "Iteration 11602, loss = 0.29728363\n",
            "Iteration 11603, loss = 0.29732528\n",
            "Iteration 11604, loss = 0.29728750\n",
            "Iteration 11605, loss = 0.29730897\n",
            "Iteration 11606, loss = 0.29725724\n",
            "Iteration 11607, loss = 0.29728895\n",
            "Iteration 11608, loss = 0.29723201\n",
            "Iteration 11609, loss = 0.29723268\n",
            "Iteration 11610, loss = 0.29726383\n",
            "Iteration 11611, loss = 0.29720708\n",
            "Iteration 11612, loss = 0.29722899\n",
            "Iteration 11613, loss = 0.29720545\n",
            "Iteration 11614, loss = 0.29723426\n",
            "Iteration 11615, loss = 0.29721130\n",
            "Iteration 11616, loss = 0.29715112\n",
            "Iteration 11617, loss = 0.29722188\n",
            "Iteration 11618, loss = 0.29716467\n",
            "Iteration 11619, loss = 0.29713619\n",
            "Iteration 11620, loss = 0.29720822\n",
            "Iteration 11621, loss = 0.29712161\n",
            "Iteration 11622, loss = 0.29714024\n",
            "Iteration 11623, loss = 0.29711547\n",
            "Iteration 11624, loss = 0.29708509\n",
            "Iteration 11625, loss = 0.29709665\n",
            "Iteration 11626, loss = 0.29706509\n",
            "Iteration 11627, loss = 0.29705476\n",
            "Iteration 11628, loss = 0.29706286\n",
            "Iteration 11629, loss = 0.29703247\n",
            "Iteration 11630, loss = 0.29704792\n",
            "Iteration 11631, loss = 0.29703493\n",
            "Iteration 11632, loss = 0.29700850\n",
            "Iteration 11633, loss = 0.29700463\n",
            "Iteration 11634, loss = 0.29698933\n",
            "Iteration 11635, loss = 0.29701003\n",
            "Iteration 11636, loss = 0.29699161\n",
            "Iteration 11637, loss = 0.29693869\n",
            "Iteration 11638, loss = 0.29697150\n",
            "Iteration 11639, loss = 0.29693252\n",
            "Iteration 11640, loss = 0.29693729\n",
            "Iteration 11641, loss = 0.29697929\n",
            "Iteration 11642, loss = 0.29690701\n",
            "Iteration 11643, loss = 0.29700394\n",
            "Iteration 11644, loss = 0.29689430\n",
            "Iteration 11645, loss = 0.29691899\n",
            "Iteration 11646, loss = 0.29690925\n",
            "Iteration 11647, loss = 0.29686356\n",
            "Iteration 11648, loss = 0.29687214\n",
            "Iteration 11649, loss = 0.29685506\n",
            "Iteration 11650, loss = 0.29683533\n",
            "Iteration 11651, loss = 0.29684740\n",
            "Iteration 11652, loss = 0.29682894\n",
            "Iteration 11653, loss = 0.29678714\n",
            "Iteration 11654, loss = 0.29679455\n",
            "Iteration 11655, loss = 0.29679001\n",
            "Iteration 11656, loss = 0.29676048\n",
            "Iteration 11657, loss = 0.29680682\n",
            "Iteration 11658, loss = 0.29678501\n",
            "Iteration 11659, loss = 0.29673566\n",
            "Iteration 11660, loss = 0.29677788\n",
            "Iteration 11661, loss = 0.29672037\n",
            "Iteration 11662, loss = 0.29673821\n",
            "Iteration 11663, loss = 0.29673649\n",
            "Iteration 11664, loss = 0.29668747\n",
            "Iteration 11665, loss = 0.29673821\n",
            "Iteration 11666, loss = 0.29670299\n",
            "Iteration 11667, loss = 0.29668121\n",
            "Iteration 11668, loss = 0.29670284\n",
            "Iteration 11669, loss = 0.29664672\n",
            "Iteration 11670, loss = 0.29666457\n",
            "Iteration 11671, loss = 0.29663833\n",
            "Iteration 11672, loss = 0.29661892\n",
            "Iteration 11673, loss = 0.29666509\n",
            "Iteration 11674, loss = 0.29658248\n",
            "Iteration 11675, loss = 0.29665571\n",
            "Iteration 11676, loss = 0.29659677\n",
            "Iteration 11677, loss = 0.29655750\n",
            "Iteration 11678, loss = 0.29656335\n",
            "Iteration 11679, loss = 0.29656362\n",
            "Iteration 11680, loss = 0.29652987\n",
            "Iteration 11681, loss = 0.29656697\n",
            "Iteration 11682, loss = 0.29657875\n",
            "Iteration 11683, loss = 0.29650183\n",
            "Iteration 11684, loss = 0.29651396\n",
            "Iteration 11685, loss = 0.29651500\n",
            "Iteration 11686, loss = 0.29648050\n",
            "Iteration 11687, loss = 0.29648780\n",
            "Iteration 11688, loss = 0.29650133\n",
            "Iteration 11689, loss = 0.29650679\n",
            "Iteration 11690, loss = 0.29648970\n",
            "Iteration 11691, loss = 0.29642920\n",
            "Iteration 11692, loss = 0.29647122\n",
            "Iteration 11693, loss = 0.29643003\n",
            "Iteration 11694, loss = 0.29644485\n",
            "Iteration 11695, loss = 0.29640151\n",
            "Iteration 11696, loss = 0.29640141\n",
            "Iteration 11697, loss = 0.29638498\n",
            "Iteration 11698, loss = 0.29636916\n",
            "Iteration 11699, loss = 0.29638329\n",
            "Iteration 11700, loss = 0.29634707\n",
            "Iteration 11701, loss = 0.29634581\n",
            "Iteration 11702, loss = 0.29634394\n",
            "Iteration 11703, loss = 0.29632999\n",
            "Iteration 11704, loss = 0.29631210\n",
            "Iteration 11705, loss = 0.29630833\n",
            "Iteration 11706, loss = 0.29628158\n",
            "Iteration 11707, loss = 0.29628000\n",
            "Iteration 11708, loss = 0.29626910\n",
            "Iteration 11709, loss = 0.29624542\n",
            "Iteration 11710, loss = 0.29624346\n",
            "Iteration 11711, loss = 0.29623225\n",
            "Iteration 11712, loss = 0.29621276\n",
            "Iteration 11713, loss = 0.29621521\n",
            "Iteration 11714, loss = 0.29620046\n",
            "Iteration 11715, loss = 0.29620011\n",
            "Iteration 11716, loss = 0.29619280\n",
            "Iteration 11717, loss = 0.29618615\n",
            "Iteration 11718, loss = 0.29615379\n",
            "Iteration 11719, loss = 0.29617555\n",
            "Iteration 11720, loss = 0.29616873\n",
            "Iteration 11721, loss = 0.29613133\n",
            "Iteration 11722, loss = 0.29616865\n",
            "Iteration 11723, loss = 0.29616215\n",
            "Iteration 11724, loss = 0.29612528\n",
            "Iteration 11725, loss = 0.29610352\n",
            "Iteration 11726, loss = 0.29612682\n",
            "Iteration 11727, loss = 0.29610222\n",
            "Iteration 11728, loss = 0.29606362\n",
            "Iteration 11729, loss = 0.29608721\n",
            "Iteration 11730, loss = 0.29609714\n",
            "Iteration 11731, loss = 0.29604035\n",
            "Iteration 11732, loss = 0.29605165\n",
            "Iteration 11733, loss = 0.29608744\n",
            "Iteration 11734, loss = 0.29605924\n",
            "Iteration 11735, loss = 0.29601046\n",
            "Iteration 11736, loss = 0.29600571\n",
            "Iteration 11737, loss = 0.29604550\n",
            "Iteration 11738, loss = 0.29599931\n",
            "Iteration 11739, loss = 0.29598356\n",
            "Iteration 11740, loss = 0.29600944\n",
            "Iteration 11741, loss = 0.29599923\n",
            "Iteration 11742, loss = 0.29595307\n",
            "Iteration 11743, loss = 0.29593688\n",
            "Iteration 11744, loss = 0.29597157\n",
            "Iteration 11745, loss = 0.29590848\n",
            "Iteration 11746, loss = 0.29593547\n",
            "Iteration 11747, loss = 0.29592032\n",
            "Iteration 11748, loss = 0.29589574\n",
            "Iteration 11749, loss = 0.29586223\n",
            "Iteration 11750, loss = 0.29587147\n",
            "Iteration 11751, loss = 0.29585260\n",
            "Iteration 11752, loss = 0.29585102\n",
            "Iteration 11753, loss = 0.29584060\n",
            "Iteration 11754, loss = 0.29582157\n",
            "Iteration 11755, loss = 0.29580966\n",
            "Iteration 11756, loss = 0.29580674\n",
            "Iteration 11757, loss = 0.29580976\n",
            "Iteration 11758, loss = 0.29579322\n",
            "Iteration 11759, loss = 0.29577994\n",
            "Iteration 11760, loss = 0.29578379\n",
            "Iteration 11761, loss = 0.29575918\n",
            "Iteration 11762, loss = 0.29575368\n",
            "Iteration 11763, loss = 0.29573866\n",
            "Iteration 11764, loss = 0.29574576\n",
            "Iteration 11765, loss = 0.29572216\n",
            "Iteration 11766, loss = 0.29572440\n",
            "Iteration 11767, loss = 0.29571776\n",
            "Iteration 11768, loss = 0.29568497\n",
            "Iteration 11769, loss = 0.29569138\n",
            "Iteration 11770, loss = 0.29571569\n",
            "Iteration 11771, loss = 0.29564523\n",
            "Iteration 11772, loss = 0.29571042\n",
            "Iteration 11773, loss = 0.29571564\n",
            "Iteration 11774, loss = 0.29565761\n",
            "Iteration 11775, loss = 0.29567288\n",
            "Iteration 11776, loss = 0.29565008\n",
            "Iteration 11777, loss = 0.29567840\n",
            "Iteration 11778, loss = 0.29564107\n",
            "Iteration 11779, loss = 0.29563644\n",
            "Iteration 11780, loss = 0.29569249\n",
            "Iteration 11781, loss = 0.29562429\n",
            "Iteration 11782, loss = 0.29559870\n",
            "Iteration 11783, loss = 0.29560357\n",
            "Iteration 11784, loss = 0.29557775\n",
            "Iteration 11785, loss = 0.29555589\n",
            "Iteration 11786, loss = 0.29551544\n",
            "Iteration 11787, loss = 0.29553863\n",
            "Iteration 11788, loss = 0.29552465\n",
            "Iteration 11789, loss = 0.29547987\n",
            "Iteration 11790, loss = 0.29549181\n",
            "Iteration 11791, loss = 0.29549657\n",
            "Iteration 11792, loss = 0.29548411\n",
            "Iteration 11793, loss = 0.29546037\n",
            "Iteration 11794, loss = 0.29545380\n",
            "Iteration 11795, loss = 0.29545020\n",
            "Iteration 11796, loss = 0.29542000\n",
            "Iteration 11797, loss = 0.29540699\n",
            "Iteration 11798, loss = 0.29540205\n",
            "Iteration 11799, loss = 0.29539121\n",
            "Iteration 11800, loss = 0.29538255\n",
            "Iteration 11801, loss = 0.29536903\n",
            "Iteration 11802, loss = 0.29535991\n",
            "Iteration 11803, loss = 0.29536252\n",
            "Iteration 11804, loss = 0.29535119\n",
            "Iteration 11805, loss = 0.29534389\n",
            "Iteration 11806, loss = 0.29532830\n",
            "Iteration 11807, loss = 0.29531028\n",
            "Iteration 11808, loss = 0.29530634\n",
            "Iteration 11809, loss = 0.29530957\n",
            "Iteration 11810, loss = 0.29528369\n",
            "Iteration 11811, loss = 0.29528511\n",
            "Iteration 11812, loss = 0.29528532\n",
            "Iteration 11813, loss = 0.29525835\n",
            "Iteration 11814, loss = 0.29526511\n",
            "Iteration 11815, loss = 0.29527870\n",
            "Iteration 11816, loss = 0.29525498\n",
            "Iteration 11817, loss = 0.29522157\n",
            "Iteration 11818, loss = 0.29527604\n",
            "Iteration 11819, loss = 0.29524172\n",
            "Iteration 11820, loss = 0.29519825\n",
            "Iteration 11821, loss = 0.29521822\n",
            "Iteration 11822, loss = 0.29521494\n",
            "Iteration 11823, loss = 0.29518102\n",
            "Iteration 11824, loss = 0.29516742\n",
            "Iteration 11825, loss = 0.29518742\n",
            "Iteration 11826, loss = 0.29514512\n",
            "Iteration 11827, loss = 0.29511423\n",
            "Iteration 11828, loss = 0.29512929\n",
            "Iteration 11829, loss = 0.29511350\n",
            "Iteration 11830, loss = 0.29508729\n",
            "Iteration 11831, loss = 0.29510206\n",
            "Iteration 11832, loss = 0.29507658\n",
            "Iteration 11833, loss = 0.29507185\n",
            "Iteration 11834, loss = 0.29507909\n",
            "Iteration 11835, loss = 0.29506015\n",
            "Iteration 11836, loss = 0.29504283\n",
            "Iteration 11837, loss = 0.29504732\n",
            "Iteration 11838, loss = 0.29504119\n",
            "Iteration 11839, loss = 0.29501453\n",
            "Iteration 11840, loss = 0.29505777\n",
            "Iteration 11841, loss = 0.29501500\n",
            "Iteration 11842, loss = 0.29505684\n",
            "Iteration 11843, loss = 0.29502706\n",
            "Iteration 11844, loss = 0.29499044\n",
            "Iteration 11845, loss = 0.29495651\n",
            "Iteration 11846, loss = 0.29495449\n",
            "Iteration 11847, loss = 0.29494017\n",
            "Iteration 11848, loss = 0.29492771\n",
            "Iteration 11849, loss = 0.29491115\n",
            "Iteration 11850, loss = 0.29490448\n",
            "Iteration 11851, loss = 0.29490818\n",
            "Iteration 11852, loss = 0.29488069\n",
            "Iteration 11853, loss = 0.29486944\n",
            "Iteration 11854, loss = 0.29488152\n",
            "Iteration 11855, loss = 0.29488645\n",
            "Iteration 11856, loss = 0.29487690\n",
            "Iteration 11857, loss = 0.29484095\n",
            "Iteration 11858, loss = 0.29489476\n",
            "Iteration 11859, loss = 0.29486183\n",
            "Iteration 11860, loss = 0.29489861\n",
            "Iteration 11861, loss = 0.29488132\n",
            "Iteration 11862, loss = 0.29481052\n",
            "Iteration 11863, loss = 0.29488062\n",
            "Iteration 11864, loss = 0.29480763\n",
            "Iteration 11865, loss = 0.29485974\n",
            "Iteration 11866, loss = 0.29482255\n",
            "Iteration 11867, loss = 0.29482472\n",
            "Iteration 11868, loss = 0.29481874\n",
            "Iteration 11869, loss = 0.29477090\n",
            "Iteration 11870, loss = 0.29474308\n",
            "Iteration 11871, loss = 0.29487645\n",
            "Iteration 11872, loss = 0.29484452\n",
            "Iteration 11873, loss = 0.29469091\n",
            "Iteration 11874, loss = 0.29474741\n",
            "Iteration 11875, loss = 0.29474703\n",
            "Iteration 11876, loss = 0.29473980\n",
            "Iteration 11877, loss = 0.29470229\n",
            "Iteration 11878, loss = 0.29465828\n",
            "Iteration 11879, loss = 0.29468746\n",
            "Iteration 11880, loss = 0.29466519\n",
            "Iteration 11881, loss = 0.29463225\n",
            "Iteration 11882, loss = 0.29463984\n",
            "Iteration 11883, loss = 0.29464247\n",
            "Iteration 11884, loss = 0.29460022\n",
            "Iteration 11885, loss = 0.29459956\n",
            "Iteration 11886, loss = 0.29462529\n",
            "Iteration 11887, loss = 0.29456776\n",
            "Iteration 11888, loss = 0.29459120\n",
            "Iteration 11889, loss = 0.29458848\n",
            "Iteration 11890, loss = 0.29455103\n",
            "Iteration 11891, loss = 0.29454733\n",
            "Iteration 11892, loss = 0.29453364\n",
            "Iteration 11893, loss = 0.29452485\n",
            "Iteration 11894, loss = 0.29451085\n",
            "Iteration 11895, loss = 0.29451199\n",
            "Iteration 11896, loss = 0.29451045\n",
            "Iteration 11897, loss = 0.29448896\n",
            "Iteration 11898, loss = 0.29446903\n",
            "Iteration 11899, loss = 0.29446947\n",
            "Iteration 11900, loss = 0.29446194\n",
            "Iteration 11901, loss = 0.29445162\n",
            "Iteration 11902, loss = 0.29442790\n",
            "Iteration 11903, loss = 0.29442548\n",
            "Iteration 11904, loss = 0.29442999\n",
            "Iteration 11905, loss = 0.29440711\n",
            "Iteration 11906, loss = 0.29438768\n",
            "Iteration 11907, loss = 0.29439467\n",
            "Iteration 11908, loss = 0.29436574\n",
            "Iteration 11909, loss = 0.29435726\n",
            "Iteration 11910, loss = 0.29435884\n",
            "Iteration 11911, loss = 0.29435934\n",
            "Iteration 11912, loss = 0.29433742\n",
            "Iteration 11913, loss = 0.29433553\n",
            "Iteration 11914, loss = 0.29436337\n",
            "Iteration 11915, loss = 0.29438213\n",
            "Iteration 11916, loss = 0.29430906\n",
            "Iteration 11917, loss = 0.29439280\n",
            "Iteration 11918, loss = 0.29429985\n",
            "Iteration 11919, loss = 0.29440612\n",
            "Iteration 11920, loss = 0.29438664\n",
            "Iteration 11921, loss = 0.29434921\n",
            "Iteration 11922, loss = 0.29436159\n",
            "Iteration 11923, loss = 0.29436381\n",
            "Iteration 11924, loss = 0.29427020\n",
            "Iteration 11925, loss = 0.29424826\n",
            "Iteration 11926, loss = 0.29432696\n",
            "Iteration 11927, loss = 0.29421877\n",
            "Iteration 11928, loss = 0.29423020\n",
            "Iteration 11929, loss = 0.29420563\n",
            "Iteration 11930, loss = 0.29421942\n",
            "Iteration 11931, loss = 0.29421315\n",
            "Iteration 11932, loss = 0.29415119\n",
            "Iteration 11933, loss = 0.29419918\n",
            "Iteration 11934, loss = 0.29418756\n",
            "Iteration 11935, loss = 0.29419006\n",
            "Iteration 11936, loss = 0.29412417\n",
            "Iteration 11937, loss = 0.29416585\n",
            "Iteration 11938, loss = 0.29417691\n",
            "Iteration 11939, loss = 0.29415513\n",
            "Iteration 11940, loss = 0.29411919\n",
            "Iteration 11941, loss = 0.29409930\n",
            "Iteration 11942, loss = 0.29415283\n",
            "Iteration 11943, loss = 0.29413389\n",
            "Iteration 11944, loss = 0.29407182\n",
            "Iteration 11945, loss = 0.29405537\n",
            "Iteration 11946, loss = 0.29407172\n",
            "Iteration 11947, loss = 0.29405292\n",
            "Iteration 11948, loss = 0.29401270\n",
            "Iteration 11949, loss = 0.29401733\n",
            "Iteration 11950, loss = 0.29403342\n",
            "Iteration 11951, loss = 0.29397772\n",
            "Iteration 11952, loss = 0.29402983\n",
            "Iteration 11953, loss = 0.29400169\n",
            "Iteration 11954, loss = 0.29398839\n",
            "Iteration 11955, loss = 0.29395454\n",
            "Iteration 11956, loss = 0.29393837\n",
            "Iteration 11957, loss = 0.29395544\n",
            "Iteration 11958, loss = 0.29392778\n",
            "Iteration 11959, loss = 0.29390386\n",
            "Iteration 11960, loss = 0.29391395\n",
            "Iteration 11961, loss = 0.29391716\n",
            "Iteration 11962, loss = 0.29389509\n",
            "Iteration 11963, loss = 0.29386080\n",
            "Iteration 11964, loss = 0.29386724\n",
            "Iteration 11965, loss = 0.29385630\n",
            "Iteration 11966, loss = 0.29383426\n",
            "Iteration 11967, loss = 0.29383611\n",
            "Iteration 11968, loss = 0.29383591\n",
            "Iteration 11969, loss = 0.29380206\n",
            "Iteration 11970, loss = 0.29380341\n",
            "Iteration 11971, loss = 0.29380245\n",
            "Iteration 11972, loss = 0.29380924\n",
            "Iteration 11973, loss = 0.29378511\n",
            "Iteration 11974, loss = 0.29376074\n",
            "Iteration 11975, loss = 0.29377920\n",
            "Iteration 11976, loss = 0.29375449\n",
            "Iteration 11977, loss = 0.29373951\n",
            "Iteration 11978, loss = 0.29373144\n",
            "Iteration 11979, loss = 0.29372784\n",
            "Iteration 11980, loss = 0.29370362\n",
            "Iteration 11981, loss = 0.29374460\n",
            "Iteration 11982, loss = 0.29370993\n",
            "Iteration 11983, loss = 0.29370412\n",
            "Iteration 11984, loss = 0.29367649\n",
            "Iteration 11985, loss = 0.29368932\n",
            "Iteration 11986, loss = 0.29368407\n",
            "Iteration 11987, loss = 0.29367917\n",
            "Iteration 11988, loss = 0.29362726\n",
            "Iteration 11989, loss = 0.29371975\n",
            "Iteration 11990, loss = 0.29363161\n",
            "Iteration 11991, loss = 0.29371584\n",
            "Iteration 11992, loss = 0.29368670\n",
            "Iteration 11993, loss = 0.29366700\n",
            "Iteration 11994, loss = 0.29367499\n",
            "Iteration 11995, loss = 0.29370206\n",
            "Iteration 11996, loss = 0.29360580\n",
            "Iteration 11997, loss = 0.29357827\n",
            "Iteration 11998, loss = 0.29365379\n",
            "Iteration 11999, loss = 0.29355896\n",
            "Iteration 12000, loss = 0.29359266\n",
            "Iteration 12001, loss = 0.29357407\n",
            "Iteration 12002, loss = 0.29358257\n",
            "Iteration 12003, loss = 0.29355824\n",
            "Iteration 12004, loss = 0.29350425\n",
            "Iteration 12005, loss = 0.29352816\n",
            "Iteration 12006, loss = 0.29357319\n",
            "Iteration 12007, loss = 0.29349979\n",
            "Iteration 12008, loss = 0.29347104\n",
            "Iteration 12009, loss = 0.29349313\n",
            "Iteration 12010, loss = 0.29349016\n",
            "Iteration 12011, loss = 0.29348558\n",
            "Iteration 12012, loss = 0.29343882\n",
            "Iteration 12013, loss = 0.29343926\n",
            "Iteration 12014, loss = 0.29344601\n",
            "Iteration 12015, loss = 0.29339890\n",
            "Iteration 12016, loss = 0.29339403\n",
            "Iteration 12017, loss = 0.29338186\n",
            "Iteration 12018, loss = 0.29336816\n",
            "Iteration 12019, loss = 0.29336280\n",
            "Iteration 12020, loss = 0.29335765\n",
            "Iteration 12021, loss = 0.29334648\n",
            "Iteration 12022, loss = 0.29333558\n",
            "Iteration 12023, loss = 0.29334110\n",
            "Iteration 12024, loss = 0.29333886\n",
            "Iteration 12025, loss = 0.29332785\n",
            "Iteration 12026, loss = 0.29329001\n",
            "Iteration 12027, loss = 0.29329215\n",
            "Iteration 12028, loss = 0.29329411\n",
            "Iteration 12029, loss = 0.29328801\n",
            "Iteration 12030, loss = 0.29325051\n",
            "Iteration 12031, loss = 0.29327367\n",
            "Iteration 12032, loss = 0.29326314\n",
            "Iteration 12033, loss = 0.29328189\n",
            "Iteration 12034, loss = 0.29321359\n",
            "Iteration 12035, loss = 0.29320814\n",
            "Iteration 12036, loss = 0.29319904\n",
            "Iteration 12037, loss = 0.29318962\n",
            "Iteration 12038, loss = 0.29318435\n",
            "Iteration 12039, loss = 0.29318334\n",
            "Iteration 12040, loss = 0.29316143\n",
            "Iteration 12041, loss = 0.29317199\n",
            "Iteration 12042, loss = 0.29314520\n",
            "Iteration 12043, loss = 0.29313649\n",
            "Iteration 12044, loss = 0.29312714\n",
            "Iteration 12045, loss = 0.29311558\n",
            "Iteration 12046, loss = 0.29310410\n",
            "Iteration 12047, loss = 0.29309486\n",
            "Iteration 12048, loss = 0.29309964\n",
            "Iteration 12049, loss = 0.29307696\n",
            "Iteration 12050, loss = 0.29311946\n",
            "Iteration 12051, loss = 0.29305297\n",
            "Iteration 12052, loss = 0.29305043\n",
            "Iteration 12053, loss = 0.29307631\n",
            "Iteration 12054, loss = 0.29306001\n",
            "Iteration 12055, loss = 0.29301300\n",
            "Iteration 12056, loss = 0.29300662\n",
            "Iteration 12057, loss = 0.29301834\n",
            "Iteration 12058, loss = 0.29300674\n",
            "Iteration 12059, loss = 0.29299326\n",
            "Iteration 12060, loss = 0.29302948\n",
            "Iteration 12061, loss = 0.29298333\n",
            "Iteration 12062, loss = 0.29297980\n",
            "Iteration 12063, loss = 0.29297863\n",
            "Iteration 12064, loss = 0.29297647\n",
            "Iteration 12065, loss = 0.29296161\n",
            "Iteration 12066, loss = 0.29293975\n",
            "Iteration 12067, loss = 0.29292873\n",
            "Iteration 12068, loss = 0.29294912\n",
            "Iteration 12069, loss = 0.29290355\n",
            "Iteration 12070, loss = 0.29293161\n",
            "Iteration 12071, loss = 0.29292914\n",
            "Iteration 12072, loss = 0.29293008\n",
            "Iteration 12073, loss = 0.29288589\n",
            "Iteration 12074, loss = 0.29286519\n",
            "Iteration 12075, loss = 0.29289567\n",
            "Iteration 12076, loss = 0.29284502\n",
            "Iteration 12077, loss = 0.29288156\n",
            "Iteration 12078, loss = 0.29286410\n",
            "Iteration 12079, loss = 0.29286146\n",
            "Iteration 12080, loss = 0.29286881\n",
            "Iteration 12081, loss = 0.29280036\n",
            "Iteration 12082, loss = 0.29282700\n",
            "Iteration 12083, loss = 0.29281586\n",
            "Iteration 12084, loss = 0.29278950\n",
            "Iteration 12085, loss = 0.29282031\n",
            "Iteration 12086, loss = 0.29280606\n",
            "Iteration 12087, loss = 0.29282688\n",
            "Iteration 12088, loss = 0.29280231\n",
            "Iteration 12089, loss = 0.29273574\n",
            "Iteration 12090, loss = 0.29277117\n",
            "Iteration 12091, loss = 0.29278311\n",
            "Iteration 12092, loss = 0.29270904\n",
            "Iteration 12093, loss = 0.29277380\n",
            "Iteration 12094, loss = 0.29273570\n",
            "Iteration 12095, loss = 0.29274406\n",
            "Iteration 12096, loss = 0.29275713\n",
            "Iteration 12097, loss = 0.29275626\n",
            "Iteration 12098, loss = 0.29265624\n",
            "Iteration 12099, loss = 0.29273060\n",
            "Iteration 12100, loss = 0.29269970\n",
            "Iteration 12101, loss = 0.29264130\n",
            "Iteration 12102, loss = 0.29268223\n",
            "Iteration 12103, loss = 0.29265814\n",
            "Iteration 12104, loss = 0.29266640\n",
            "Iteration 12105, loss = 0.29266880\n",
            "Iteration 12106, loss = 0.29261893\n",
            "Iteration 12107, loss = 0.29258201\n",
            "Iteration 12108, loss = 0.29259500\n",
            "Iteration 12109, loss = 0.29258440\n",
            "Iteration 12110, loss = 0.29254965\n",
            "Iteration 12111, loss = 0.29254844\n",
            "Iteration 12112, loss = 0.29255564\n",
            "Iteration 12113, loss = 0.29252940\n",
            "Iteration 12114, loss = 0.29249792\n",
            "Iteration 12115, loss = 0.29249908\n",
            "Iteration 12116, loss = 0.29250886\n",
            "Iteration 12117, loss = 0.29247806\n",
            "Iteration 12118, loss = 0.29248031\n",
            "Iteration 12119, loss = 0.29246161\n",
            "Iteration 12120, loss = 0.29245510\n",
            "Iteration 12121, loss = 0.29244941\n",
            "Iteration 12122, loss = 0.29244902\n",
            "Iteration 12123, loss = 0.29242678\n",
            "Iteration 12124, loss = 0.29245240\n",
            "Iteration 12125, loss = 0.29242749\n",
            "Iteration 12126, loss = 0.29242438\n",
            "Iteration 12127, loss = 0.29243616\n",
            "Iteration 12128, loss = 0.29237291\n",
            "Iteration 12129, loss = 0.29248061\n",
            "Iteration 12130, loss = 0.29242689\n",
            "Iteration 12131, loss = 0.29239987\n",
            "Iteration 12132, loss = 0.29241406\n",
            "Iteration 12133, loss = 0.29234227\n",
            "Iteration 12134, loss = 0.29235033\n",
            "Iteration 12135, loss = 0.29233296\n",
            "Iteration 12136, loss = 0.29230452\n",
            "Iteration 12137, loss = 0.29232035\n",
            "Iteration 12138, loss = 0.29230395\n",
            "Iteration 12139, loss = 0.29228079\n",
            "Iteration 12140, loss = 0.29229909\n",
            "Iteration 12141, loss = 0.29231884\n",
            "Iteration 12142, loss = 0.29228473\n",
            "Iteration 12143, loss = 0.29227463\n",
            "Iteration 12144, loss = 0.29225368\n",
            "Iteration 12145, loss = 0.29224120\n",
            "Iteration 12146, loss = 0.29227536\n",
            "Iteration 12147, loss = 0.29220876\n",
            "Iteration 12148, loss = 0.29229273\n",
            "Iteration 12149, loss = 0.29224098\n",
            "Iteration 12150, loss = 0.29219793\n",
            "Iteration 12151, loss = 0.29221767\n",
            "Iteration 12152, loss = 0.29220857\n",
            "Iteration 12153, loss = 0.29220188\n",
            "Iteration 12154, loss = 0.29219718\n",
            "Iteration 12155, loss = 0.29214849\n",
            "Iteration 12156, loss = 0.29217886\n",
            "Iteration 12157, loss = 0.29215564\n",
            "Iteration 12158, loss = 0.29211892\n",
            "Iteration 12159, loss = 0.29211339\n",
            "Iteration 12160, loss = 0.29212043\n",
            "Iteration 12161, loss = 0.29211218\n",
            "Iteration 12162, loss = 0.29210426\n",
            "Iteration 12163, loss = 0.29208266\n",
            "Iteration 12164, loss = 0.29208268\n",
            "Iteration 12165, loss = 0.29208284\n",
            "Iteration 12166, loss = 0.29204823\n",
            "Iteration 12167, loss = 0.29208398\n",
            "Iteration 12168, loss = 0.29205601\n",
            "Iteration 12169, loss = 0.29202390\n",
            "Iteration 12170, loss = 0.29203749\n",
            "Iteration 12171, loss = 0.29200013\n",
            "Iteration 12172, loss = 0.29200576\n",
            "Iteration 12173, loss = 0.29199257\n",
            "Iteration 12174, loss = 0.29197368\n",
            "Iteration 12175, loss = 0.29197175\n",
            "Iteration 12176, loss = 0.29194376\n",
            "Iteration 12177, loss = 0.29194454\n",
            "Iteration 12178, loss = 0.29193896\n",
            "Iteration 12179, loss = 0.29192373\n",
            "Iteration 12180, loss = 0.29192233\n",
            "Iteration 12181, loss = 0.29190996\n",
            "Iteration 12182, loss = 0.29189296\n",
            "Iteration 12183, loss = 0.29192538\n",
            "Iteration 12184, loss = 0.29188340\n",
            "Iteration 12185, loss = 0.29186927\n",
            "Iteration 12186, loss = 0.29186398\n",
            "Iteration 12187, loss = 0.29186654\n",
            "Iteration 12188, loss = 0.29185107\n",
            "Iteration 12189, loss = 0.29185855\n",
            "Iteration 12190, loss = 0.29185119\n",
            "Iteration 12191, loss = 0.29185221\n",
            "Iteration 12192, loss = 0.29183855\n",
            "Iteration 12193, loss = 0.29183001\n",
            "Iteration 12194, loss = 0.29183543\n",
            "Iteration 12195, loss = 0.29177263\n",
            "Iteration 12196, loss = 0.29184087\n",
            "Iteration 12197, loss = 0.29177361\n",
            "Iteration 12198, loss = 0.29181871\n",
            "Iteration 12199, loss = 0.29180211\n",
            "Iteration 12200, loss = 0.29176991\n",
            "Iteration 12201, loss = 0.29180038\n",
            "Iteration 12202, loss = 0.29172643\n",
            "Iteration 12203, loss = 0.29178446\n",
            "Iteration 12204, loss = 0.29174014\n",
            "Iteration 12205, loss = 0.29172731\n",
            "Iteration 12206, loss = 0.29174762\n",
            "Iteration 12207, loss = 0.29171144\n",
            "Iteration 12208, loss = 0.29168235\n",
            "Iteration 12209, loss = 0.29169974\n",
            "Iteration 12210, loss = 0.29167420\n",
            "Iteration 12211, loss = 0.29166650\n",
            "Iteration 12212, loss = 0.29165397\n",
            "Iteration 12213, loss = 0.29162791\n",
            "Iteration 12214, loss = 0.29162286\n",
            "Iteration 12215, loss = 0.29160595\n",
            "Iteration 12216, loss = 0.29159089\n",
            "Iteration 12217, loss = 0.29158063\n",
            "Iteration 12218, loss = 0.29157918\n",
            "Iteration 12219, loss = 0.29156222\n",
            "Iteration 12220, loss = 0.29157190\n",
            "Iteration 12221, loss = 0.29155497\n",
            "Iteration 12222, loss = 0.29154824\n",
            "Iteration 12223, loss = 0.29153759\n",
            "Iteration 12224, loss = 0.29152835\n",
            "Iteration 12225, loss = 0.29152592\n",
            "Iteration 12226, loss = 0.29151670\n",
            "Iteration 12227, loss = 0.29149883\n",
            "Iteration 12228, loss = 0.29149963\n",
            "Iteration 12229, loss = 0.29148017\n",
            "Iteration 12230, loss = 0.29148892\n",
            "Iteration 12231, loss = 0.29146192\n",
            "Iteration 12232, loss = 0.29148153\n",
            "Iteration 12233, loss = 0.29144222\n",
            "Iteration 12234, loss = 0.29145281\n",
            "Iteration 12235, loss = 0.29142987\n",
            "Iteration 12236, loss = 0.29142927\n",
            "Iteration 12237, loss = 0.29141736\n",
            "Iteration 12238, loss = 0.29141301\n",
            "Iteration 12239, loss = 0.29138865\n",
            "Iteration 12240, loss = 0.29136905\n",
            "Iteration 12241, loss = 0.29138933\n",
            "Iteration 12242, loss = 0.29135335\n",
            "Iteration 12243, loss = 0.29136413\n",
            "Iteration 12244, loss = 0.29136055\n",
            "Iteration 12245, loss = 0.29135657\n",
            "Iteration 12246, loss = 0.29133626\n",
            "Iteration 12247, loss = 0.29134514\n",
            "Iteration 12248, loss = 0.29133246\n",
            "Iteration 12249, loss = 0.29133802\n",
            "Iteration 12250, loss = 0.29132142\n",
            "Iteration 12251, loss = 0.29133536\n",
            "Iteration 12252, loss = 0.29131237\n",
            "Iteration 12253, loss = 0.29133430\n",
            "Iteration 12254, loss = 0.29129329\n",
            "Iteration 12255, loss = 0.29126803\n",
            "Iteration 12256, loss = 0.29129708\n",
            "Iteration 12257, loss = 0.29124773\n",
            "Iteration 12258, loss = 0.29123780\n",
            "Iteration 12259, loss = 0.29124376\n",
            "Iteration 12260, loss = 0.29123952\n",
            "Iteration 12261, loss = 0.29121521\n",
            "Iteration 12262, loss = 0.29119072\n",
            "Iteration 12263, loss = 0.29120365\n",
            "Iteration 12264, loss = 0.29120566\n",
            "Iteration 12265, loss = 0.29118072\n",
            "Iteration 12266, loss = 0.29115928\n",
            "Iteration 12267, loss = 0.29117511\n",
            "Iteration 12268, loss = 0.29117050\n",
            "Iteration 12269, loss = 0.29113885\n",
            "Iteration 12270, loss = 0.29114163\n",
            "Iteration 12271, loss = 0.29115054\n",
            "Iteration 12272, loss = 0.29113199\n",
            "Iteration 12273, loss = 0.29109934\n",
            "Iteration 12274, loss = 0.29109942\n",
            "Iteration 12275, loss = 0.29108589\n",
            "Iteration 12276, loss = 0.29106925\n",
            "Iteration 12277, loss = 0.29105809\n",
            "Iteration 12278, loss = 0.29105517\n",
            "Iteration 12279, loss = 0.29103839\n",
            "Iteration 12280, loss = 0.29102643\n",
            "Iteration 12281, loss = 0.29101845\n",
            "Iteration 12282, loss = 0.29100424\n",
            "Iteration 12283, loss = 0.29099837\n",
            "Iteration 12284, loss = 0.29098804\n",
            "Iteration 12285, loss = 0.29098119\n",
            "Iteration 12286, loss = 0.29097986\n",
            "Iteration 12287, loss = 0.29096907\n",
            "Iteration 12288, loss = 0.29097296\n",
            "Iteration 12289, loss = 0.29095358\n",
            "Iteration 12290, loss = 0.29097346\n",
            "Iteration 12291, loss = 0.29093955\n",
            "Iteration 12292, loss = 0.29094218\n",
            "Iteration 12293, loss = 0.29091938\n",
            "Iteration 12294, loss = 0.29092392\n",
            "Iteration 12295, loss = 0.29092457\n",
            "Iteration 12296, loss = 0.29089241\n",
            "Iteration 12297, loss = 0.29088373\n",
            "Iteration 12298, loss = 0.29089620\n",
            "Iteration 12299, loss = 0.29087633\n",
            "Iteration 12300, loss = 0.29087088\n",
            "Iteration 12301, loss = 0.29084538\n",
            "Iteration 12302, loss = 0.29083103\n",
            "Iteration 12303, loss = 0.29082676\n",
            "Iteration 12304, loss = 0.29081844\n",
            "Iteration 12305, loss = 0.29084317\n",
            "Iteration 12306, loss = 0.29079674\n",
            "Iteration 12307, loss = 0.29079866\n",
            "Iteration 12308, loss = 0.29078906\n",
            "Iteration 12309, loss = 0.29078048\n",
            "Iteration 12310, loss = 0.29077638\n",
            "Iteration 12311, loss = 0.29075949\n",
            "Iteration 12312, loss = 0.29076719\n",
            "Iteration 12313, loss = 0.29075139\n",
            "Iteration 12314, loss = 0.29074984\n",
            "Iteration 12315, loss = 0.29073336\n",
            "Iteration 12316, loss = 0.29071891\n",
            "Iteration 12317, loss = 0.29072727\n",
            "Iteration 12318, loss = 0.29071601\n",
            "Iteration 12319, loss = 0.29070039\n",
            "Iteration 12320, loss = 0.29068519\n",
            "Iteration 12321, loss = 0.29068124\n",
            "Iteration 12322, loss = 0.29065891\n",
            "Iteration 12323, loss = 0.29065357\n",
            "Iteration 12324, loss = 0.29064694\n",
            "Iteration 12325, loss = 0.29062620\n",
            "Iteration 12326, loss = 0.29065389\n",
            "Iteration 12327, loss = 0.29061507\n",
            "Iteration 12328, loss = 0.29061582\n",
            "Iteration 12329, loss = 0.29060634\n",
            "Iteration 12330, loss = 0.29060167\n",
            "Iteration 12331, loss = 0.29060117\n",
            "Iteration 12332, loss = 0.29061450\n",
            "Iteration 12333, loss = 0.29058992\n",
            "Iteration 12334, loss = 0.29057709\n",
            "Iteration 12335, loss = 0.29056926\n",
            "Iteration 12336, loss = 0.29057654\n",
            "Iteration 12337, loss = 0.29055095\n",
            "Iteration 12338, loss = 0.29052364\n",
            "Iteration 12339, loss = 0.29056053\n",
            "Iteration 12340, loss = 0.29054632\n",
            "Iteration 12341, loss = 0.29050093\n",
            "Iteration 12342, loss = 0.29049178\n",
            "Iteration 12343, loss = 0.29049579\n",
            "Iteration 12344, loss = 0.29047756\n",
            "Iteration 12345, loss = 0.29045834\n",
            "Iteration 12346, loss = 0.29046105\n",
            "Iteration 12347, loss = 0.29044818\n",
            "Iteration 12348, loss = 0.29042937\n",
            "Iteration 12349, loss = 0.29044076\n",
            "Iteration 12350, loss = 0.29042787\n",
            "Iteration 12351, loss = 0.29042201\n",
            "Iteration 12352, loss = 0.29043600\n",
            "Iteration 12353, loss = 0.29043965\n",
            "Iteration 12354, loss = 0.29040157\n",
            "Iteration 12355, loss = 0.29040381\n",
            "Iteration 12356, loss = 0.29039832\n",
            "Iteration 12357, loss = 0.29041774\n",
            "Iteration 12358, loss = 0.29035365\n",
            "Iteration 12359, loss = 0.29037829\n",
            "Iteration 12360, loss = 0.29039071\n",
            "Iteration 12361, loss = 0.29036145\n",
            "Iteration 12362, loss = 0.29031644\n",
            "Iteration 12363, loss = 0.29034423\n",
            "Iteration 12364, loss = 0.29032225\n",
            "Iteration 12365, loss = 0.29030064\n",
            "Iteration 12366, loss = 0.29029457\n",
            "Iteration 12367, loss = 0.29027815\n",
            "Iteration 12368, loss = 0.29025291\n",
            "Iteration 12369, loss = 0.29027209\n",
            "Iteration 12370, loss = 0.29024557\n",
            "Iteration 12371, loss = 0.29023059\n",
            "Iteration 12372, loss = 0.29024025\n",
            "Iteration 12373, loss = 0.29024727\n",
            "Iteration 12374, loss = 0.29022460\n",
            "Iteration 12375, loss = 0.29020850\n",
            "Iteration 12376, loss = 0.29021638\n",
            "Iteration 12377, loss = 0.29019802\n",
            "Iteration 12378, loss = 0.29018169\n",
            "Iteration 12379, loss = 0.29018567\n",
            "Iteration 12380, loss = 0.29016786\n",
            "Iteration 12381, loss = 0.29016022\n",
            "Iteration 12382, loss = 0.29015680\n",
            "Iteration 12383, loss = 0.29014569\n",
            "Iteration 12384, loss = 0.29012942\n",
            "Iteration 12385, loss = 0.29016313\n",
            "Iteration 12386, loss = 0.29016758\n",
            "Iteration 12387, loss = 0.29012054\n",
            "Iteration 12388, loss = 0.29010554\n",
            "Iteration 12389, loss = 0.29010482\n",
            "Iteration 12390, loss = 0.29010302\n",
            "Iteration 12391, loss = 0.29007597\n",
            "Iteration 12392, loss = 0.29006008\n",
            "Iteration 12393, loss = 0.29007944\n",
            "Iteration 12394, loss = 0.29007527\n",
            "Iteration 12395, loss = 0.29005831\n",
            "Iteration 12396, loss = 0.29005086\n",
            "Iteration 12397, loss = 0.29001735\n",
            "Iteration 12398, loss = 0.29004200\n",
            "Iteration 12399, loss = 0.29000320\n",
            "Iteration 12400, loss = 0.28999980\n",
            "Iteration 12401, loss = 0.28999033\n",
            "Iteration 12402, loss = 0.28997465\n",
            "Iteration 12403, loss = 0.28998464\n",
            "Iteration 12404, loss = 0.28995957\n",
            "Iteration 12405, loss = 0.28994457\n",
            "Iteration 12406, loss = 0.28993619\n",
            "Iteration 12407, loss = 0.28992186\n",
            "Iteration 12408, loss = 0.28991283\n",
            "Iteration 12409, loss = 0.28990204\n",
            "Iteration 12410, loss = 0.28989394\n",
            "Iteration 12411, loss = 0.28989110\n",
            "Iteration 12412, loss = 0.28990193\n",
            "Iteration 12413, loss = 0.28986686\n",
            "Iteration 12414, loss = 0.28986001\n",
            "Iteration 12415, loss = 0.28987024\n",
            "Iteration 12416, loss = 0.28986545\n",
            "Iteration 12417, loss = 0.28985410\n",
            "Iteration 12418, loss = 0.28985507\n",
            "Iteration 12419, loss = 0.28986292\n",
            "Iteration 12420, loss = 0.28986090\n",
            "Iteration 12421, loss = 0.28981868\n",
            "Iteration 12422, loss = 0.28986674\n",
            "Iteration 12423, loss = 0.28979049\n",
            "Iteration 12424, loss = 0.28981251\n",
            "Iteration 12425, loss = 0.28978643\n",
            "Iteration 12426, loss = 0.28978449\n",
            "Iteration 12427, loss = 0.28976991\n",
            "Iteration 12428, loss = 0.28975364\n",
            "Iteration 12429, loss = 0.28975190\n",
            "Iteration 12430, loss = 0.28973855\n",
            "Iteration 12431, loss = 0.28972023\n",
            "Iteration 12432, loss = 0.28972297\n",
            "Iteration 12433, loss = 0.28971590\n",
            "Iteration 12434, loss = 0.28969712\n",
            "Iteration 12435, loss = 0.28970759\n",
            "Iteration 12436, loss = 0.28967619\n",
            "Iteration 12437, loss = 0.28968275\n",
            "Iteration 12438, loss = 0.28967397\n",
            "Iteration 12439, loss = 0.28965822\n",
            "Iteration 12440, loss = 0.28964650\n",
            "Iteration 12441, loss = 0.28963824\n",
            "Iteration 12442, loss = 0.28962907\n",
            "Iteration 12443, loss = 0.28961893\n",
            "Iteration 12444, loss = 0.28961135\n",
            "Iteration 12445, loss = 0.28960240\n",
            "Iteration 12446, loss = 0.28959355\n",
            "Iteration 12447, loss = 0.28959478\n",
            "Iteration 12448, loss = 0.28959197\n",
            "Iteration 12449, loss = 0.28955913\n",
            "Iteration 12450, loss = 0.28956108\n",
            "Iteration 12451, loss = 0.28954436\n",
            "Iteration 12452, loss = 0.28952806\n",
            "Iteration 12453, loss = 0.28953310\n",
            "Iteration 12454, loss = 0.28951561\n",
            "Iteration 12455, loss = 0.28953227\n",
            "Iteration 12456, loss = 0.28948774\n",
            "Iteration 12457, loss = 0.28952789\n",
            "Iteration 12458, loss = 0.28948064\n",
            "Iteration 12459, loss = 0.28951844\n",
            "Iteration 12460, loss = 0.28949205\n",
            "Iteration 12461, loss = 0.28948414\n",
            "Iteration 12462, loss = 0.28946507\n",
            "Iteration 12463, loss = 0.28945045\n",
            "Iteration 12464, loss = 0.28949950\n",
            "Iteration 12465, loss = 0.28943757\n",
            "Iteration 12466, loss = 0.28945264\n",
            "Iteration 12467, loss = 0.28942255\n",
            "Iteration 12468, loss = 0.28940021\n",
            "Iteration 12469, loss = 0.28939507\n",
            "Iteration 12470, loss = 0.28938221\n",
            "Iteration 12471, loss = 0.28938546\n",
            "Iteration 12472, loss = 0.28938190\n",
            "Iteration 12473, loss = 0.28936103\n",
            "Iteration 12474, loss = 0.28933869\n",
            "Iteration 12475, loss = 0.28935196\n",
            "Iteration 12476, loss = 0.28935497\n",
            "Iteration 12477, loss = 0.28936268\n",
            "Iteration 12478, loss = 0.28930230\n",
            "Iteration 12479, loss = 0.28931900\n",
            "Iteration 12480, loss = 0.28932473\n",
            "Iteration 12481, loss = 0.28934361\n",
            "Iteration 12482, loss = 0.28928303\n",
            "Iteration 12483, loss = 0.28929547\n",
            "Iteration 12484, loss = 0.28930890\n",
            "Iteration 12485, loss = 0.28928362\n",
            "Iteration 12486, loss = 0.28931328\n",
            "Iteration 12487, loss = 0.28934176\n",
            "Iteration 12488, loss = 0.28923610\n",
            "Iteration 12489, loss = 0.28928394\n",
            "Iteration 12490, loss = 0.28927072\n",
            "Iteration 12491, loss = 0.28920547\n",
            "Iteration 12492, loss = 0.28924491\n",
            "Iteration 12493, loss = 0.28922431\n",
            "Iteration 12494, loss = 0.28918735\n",
            "Iteration 12495, loss = 0.28919671\n",
            "Iteration 12496, loss = 0.28917004\n",
            "Iteration 12497, loss = 0.28914737\n",
            "Iteration 12498, loss = 0.28917335\n",
            "Iteration 12499, loss = 0.28914187\n",
            "Iteration 12500, loss = 0.28912023\n",
            "Iteration 12501, loss = 0.28915858\n",
            "Iteration 12502, loss = 0.28910556\n",
            "Iteration 12503, loss = 0.28911572\n",
            "Iteration 12504, loss = 0.28912738\n",
            "Iteration 12505, loss = 0.28910412\n",
            "Iteration 12506, loss = 0.28907623\n",
            "Iteration 12507, loss = 0.28910751\n",
            "Iteration 12508, loss = 0.28905953\n",
            "Iteration 12509, loss = 0.28905693\n",
            "Iteration 12510, loss = 0.28904859\n",
            "Iteration 12511, loss = 0.28902946\n",
            "Iteration 12512, loss = 0.28903077\n",
            "Iteration 12513, loss = 0.28901009\n",
            "Iteration 12514, loss = 0.28901344\n",
            "Iteration 12515, loss = 0.28899102\n",
            "Iteration 12516, loss = 0.28898009\n",
            "Iteration 12517, loss = 0.28897139\n",
            "Iteration 12518, loss = 0.28895987\n",
            "Iteration 12519, loss = 0.28894573\n",
            "Iteration 12520, loss = 0.28894297\n",
            "Iteration 12521, loss = 0.28893105\n",
            "Iteration 12522, loss = 0.28892062\n",
            "Iteration 12523, loss = 0.28891641\n",
            "Iteration 12524, loss = 0.28891052\n",
            "Iteration 12525, loss = 0.28890690\n",
            "Iteration 12526, loss = 0.28889586\n",
            "Iteration 12527, loss = 0.28890313\n",
            "Iteration 12528, loss = 0.28888104\n",
            "Iteration 12529, loss = 0.28888888\n",
            "Iteration 12530, loss = 0.28887447\n",
            "Iteration 12531, loss = 0.28888448\n",
            "Iteration 12532, loss = 0.28884179\n",
            "Iteration 12533, loss = 0.28887644\n",
            "Iteration 12534, loss = 0.28883488\n",
            "Iteration 12535, loss = 0.28884903\n",
            "Iteration 12536, loss = 0.28885607\n",
            "Iteration 12537, loss = 0.28879786\n",
            "Iteration 12538, loss = 0.28880615\n",
            "Iteration 12539, loss = 0.28876916\n",
            "Iteration 12540, loss = 0.28877583\n",
            "Iteration 12541, loss = 0.28875305\n",
            "Iteration 12542, loss = 0.28878055\n",
            "Iteration 12543, loss = 0.28874193\n",
            "Iteration 12544, loss = 0.28874647\n",
            "Iteration 12545, loss = 0.28873407\n",
            "Iteration 12546, loss = 0.28871723\n",
            "Iteration 12547, loss = 0.28871523\n",
            "Iteration 12548, loss = 0.28870521\n",
            "Iteration 12549, loss = 0.28869380\n",
            "Iteration 12550, loss = 0.28870508\n",
            "Iteration 12551, loss = 0.28867829\n",
            "Iteration 12552, loss = 0.28867527\n",
            "Iteration 12553, loss = 0.28866522\n",
            "Iteration 12554, loss = 0.28866568\n",
            "Iteration 12555, loss = 0.28865497\n",
            "Iteration 12556, loss = 0.28864252\n",
            "Iteration 12557, loss = 0.28863259\n",
            "Iteration 12558, loss = 0.28862725\n",
            "Iteration 12559, loss = 0.28860717\n",
            "Iteration 12560, loss = 0.28859851\n",
            "Iteration 12561, loss = 0.28860022\n",
            "Iteration 12562, loss = 0.28858130\n",
            "Iteration 12563, loss = 0.28856548\n",
            "Iteration 12564, loss = 0.28859292\n",
            "Iteration 12565, loss = 0.28855534\n",
            "Iteration 12566, loss = 0.28855268\n",
            "Iteration 12567, loss = 0.28855276\n",
            "Iteration 12568, loss = 0.28852681\n",
            "Iteration 12569, loss = 0.28854390\n",
            "Iteration 12570, loss = 0.28853028\n",
            "Iteration 12571, loss = 0.28851485\n",
            "Iteration 12572, loss = 0.28852120\n",
            "Iteration 12573, loss = 0.28853272\n",
            "Iteration 12574, loss = 0.28852646\n",
            "Iteration 12575, loss = 0.28847858\n",
            "Iteration 12576, loss = 0.28848201\n",
            "Iteration 12577, loss = 0.28848901\n",
            "Iteration 12578, loss = 0.28848337\n",
            "Iteration 12579, loss = 0.28844880\n",
            "Iteration 12580, loss = 0.28842492\n",
            "Iteration 12581, loss = 0.28844382\n",
            "Iteration 12582, loss = 0.28843850\n",
            "Iteration 12583, loss = 0.28840511\n",
            "Iteration 12584, loss = 0.28838312\n",
            "Iteration 12585, loss = 0.28840026\n",
            "Iteration 12586, loss = 0.28842547\n",
            "Iteration 12587, loss = 0.28837825\n",
            "Iteration 12588, loss = 0.28836708\n",
            "Iteration 12589, loss = 0.28835407\n",
            "Iteration 12590, loss = 0.28836845\n",
            "Iteration 12591, loss = 0.28835606\n",
            "Iteration 12592, loss = 0.28839111\n",
            "Iteration 12593, loss = 0.28833491\n",
            "Iteration 12594, loss = 0.28837619\n",
            "Iteration 12595, loss = 0.28841266\n",
            "Iteration 12596, loss = 0.28831259\n",
            "Iteration 12597, loss = 0.28835767\n",
            "Iteration 12598, loss = 0.28836218\n",
            "Iteration 12599, loss = 0.28829963\n",
            "Iteration 12600, loss = 0.28831206\n",
            "Iteration 12601, loss = 0.28827175\n",
            "Iteration 12602, loss = 0.28825754\n",
            "Iteration 12603, loss = 0.28827108\n",
            "Iteration 12604, loss = 0.28824562\n",
            "Iteration 12605, loss = 0.28820946\n",
            "Iteration 12606, loss = 0.28822016\n",
            "Iteration 12607, loss = 0.28822213\n",
            "Iteration 12608, loss = 0.28823104\n",
            "Iteration 12609, loss = 0.28817119\n",
            "Iteration 12610, loss = 0.28817915\n",
            "Iteration 12611, loss = 0.28817313\n",
            "Iteration 12612, loss = 0.28816889\n",
            "Iteration 12613, loss = 0.28819477\n",
            "Iteration 12614, loss = 0.28816321\n",
            "Iteration 12615, loss = 0.28813566\n",
            "Iteration 12616, loss = 0.28816490\n",
            "Iteration 12617, loss = 0.28812292\n",
            "Iteration 12618, loss = 0.28811371\n",
            "Iteration 12619, loss = 0.28813254\n",
            "Iteration 12620, loss = 0.28813891\n",
            "Iteration 12621, loss = 0.28808986\n",
            "Iteration 12622, loss = 0.28812902\n",
            "Iteration 12623, loss = 0.28811081\n",
            "Iteration 12624, loss = 0.28807957\n",
            "Iteration 12625, loss = 0.28808850\n",
            "Iteration 12626, loss = 0.28806021\n",
            "Iteration 12627, loss = 0.28802122\n",
            "Iteration 12628, loss = 0.28802196\n",
            "Iteration 12629, loss = 0.28801163\n",
            "Iteration 12630, loss = 0.28801658\n",
            "Iteration 12631, loss = 0.28798458\n",
            "Iteration 12632, loss = 0.28798329\n",
            "Iteration 12633, loss = 0.28797257\n",
            "Iteration 12634, loss = 0.28796565\n",
            "Iteration 12635, loss = 0.28796550\n",
            "Iteration 12636, loss = 0.28795061\n",
            "Iteration 12637, loss = 0.28794586\n",
            "Iteration 12638, loss = 0.28795100\n",
            "Iteration 12639, loss = 0.28794153\n",
            "Iteration 12640, loss = 0.28792922\n",
            "Iteration 12641, loss = 0.28791172\n",
            "Iteration 12642, loss = 0.28790327\n",
            "Iteration 12643, loss = 0.28788969\n",
            "Iteration 12644, loss = 0.28788147\n",
            "Iteration 12645, loss = 0.28788197\n",
            "Iteration 12646, loss = 0.28786531\n",
            "Iteration 12647, loss = 0.28788505\n",
            "Iteration 12648, loss = 0.28786171\n",
            "Iteration 12649, loss = 0.28784197\n",
            "Iteration 12650, loss = 0.28784935\n",
            "Iteration 12651, loss = 0.28781559\n",
            "Iteration 12652, loss = 0.28783719\n",
            "Iteration 12653, loss = 0.28780334\n",
            "Iteration 12654, loss = 0.28778949\n",
            "Iteration 12655, loss = 0.28781721\n",
            "Iteration 12656, loss = 0.28780015\n",
            "Iteration 12657, loss = 0.28776718\n",
            "Iteration 12658, loss = 0.28779382\n",
            "Iteration 12659, loss = 0.28779298\n",
            "Iteration 12660, loss = 0.28775963\n",
            "Iteration 12661, loss = 0.28776383\n",
            "Iteration 12662, loss = 0.28773534\n",
            "Iteration 12663, loss = 0.28773184\n",
            "Iteration 12664, loss = 0.28771850\n",
            "Iteration 12665, loss = 0.28770907\n",
            "Iteration 12666, loss = 0.28769949\n",
            "Iteration 12667, loss = 0.28768356\n",
            "Iteration 12668, loss = 0.28767166\n",
            "Iteration 12669, loss = 0.28767357\n",
            "Iteration 12670, loss = 0.28766002\n",
            "Iteration 12671, loss = 0.28763937\n",
            "Iteration 12672, loss = 0.28763124\n",
            "Iteration 12673, loss = 0.28762159\n",
            "Iteration 12674, loss = 0.28761726\n",
            "Iteration 12675, loss = 0.28760431\n",
            "Iteration 12676, loss = 0.28760290\n",
            "Iteration 12677, loss = 0.28760939\n",
            "Iteration 12678, loss = 0.28759581\n",
            "Iteration 12679, loss = 0.28757765\n",
            "Iteration 12680, loss = 0.28759307\n",
            "Iteration 12681, loss = 0.28759322\n",
            "Iteration 12682, loss = 0.28757759\n",
            "Iteration 12683, loss = 0.28754946\n",
            "Iteration 12684, loss = 0.28757021\n",
            "Iteration 12685, loss = 0.28756323\n",
            "Iteration 12686, loss = 0.28754302\n",
            "Iteration 12687, loss = 0.28752632\n",
            "Iteration 12688, loss = 0.28750432\n",
            "Iteration 12689, loss = 0.28753085\n",
            "Iteration 12690, loss = 0.28751469\n",
            "Iteration 12691, loss = 0.28749812\n",
            "Iteration 12692, loss = 0.28747115\n",
            "Iteration 12693, loss = 0.28747695\n",
            "Iteration 12694, loss = 0.28747770\n",
            "Iteration 12695, loss = 0.28744544\n",
            "Iteration 12696, loss = 0.28743472\n",
            "Iteration 12697, loss = 0.28748702\n",
            "Iteration 12698, loss = 0.28743199\n",
            "Iteration 12699, loss = 0.28740283\n",
            "Iteration 12700, loss = 0.28741765\n",
            "Iteration 12701, loss = 0.28740965\n",
            "Iteration 12702, loss = 0.28741071\n",
            "Iteration 12703, loss = 0.28742181\n",
            "Iteration 12704, loss = 0.28737683\n",
            "Iteration 12705, loss = 0.28737284\n",
            "Iteration 12706, loss = 0.28736335\n",
            "Iteration 12707, loss = 0.28734317\n",
            "Iteration 12708, loss = 0.28735545\n",
            "Iteration 12709, loss = 0.28735694\n",
            "Iteration 12710, loss = 0.28733838\n",
            "Iteration 12711, loss = 0.28731513\n",
            "Iteration 12712, loss = 0.28729654\n",
            "Iteration 12713, loss = 0.28730625\n",
            "Iteration 12714, loss = 0.28730264\n",
            "Iteration 12715, loss = 0.28728389\n",
            "Iteration 12716, loss = 0.28725328\n",
            "Iteration 12717, loss = 0.28724486\n",
            "Iteration 12718, loss = 0.28724745\n",
            "Iteration 12719, loss = 0.28725955\n",
            "Iteration 12720, loss = 0.28721648\n",
            "Iteration 12721, loss = 0.28722243\n",
            "Iteration 12722, loss = 0.28721915\n",
            "Iteration 12723, loss = 0.28720243\n",
            "Iteration 12724, loss = 0.28721055\n",
            "Iteration 12725, loss = 0.28720499\n",
            "Iteration 12726, loss = 0.28722501\n",
            "Iteration 12727, loss = 0.28719554\n",
            "Iteration 12728, loss = 0.28716507\n",
            "Iteration 12729, loss = 0.28718807\n",
            "Iteration 12730, loss = 0.28715140\n",
            "Iteration 12731, loss = 0.28714609\n",
            "Iteration 12732, loss = 0.28713576\n",
            "Iteration 12733, loss = 0.28712216\n",
            "Iteration 12734, loss = 0.28711178\n",
            "Iteration 12735, loss = 0.28710658\n",
            "Iteration 12736, loss = 0.28708871\n",
            "Iteration 12737, loss = 0.28710211\n",
            "Iteration 12738, loss = 0.28706715\n",
            "Iteration 12739, loss = 0.28707210\n",
            "Iteration 12740, loss = 0.28705603\n",
            "Iteration 12741, loss = 0.28704691\n",
            "Iteration 12742, loss = 0.28703468\n",
            "Iteration 12743, loss = 0.28703966\n",
            "Iteration 12744, loss = 0.28703114\n",
            "Iteration 12745, loss = 0.28702821\n",
            "Iteration 12746, loss = 0.28700786\n",
            "Iteration 12747, loss = 0.28700818\n",
            "Iteration 12748, loss = 0.28701380\n",
            "Iteration 12749, loss = 0.28699911\n",
            "Iteration 12750, loss = 0.28698347\n",
            "Iteration 12751, loss = 0.28697725\n",
            "Iteration 12752, loss = 0.28697799\n",
            "Iteration 12753, loss = 0.28696412\n",
            "Iteration 12754, loss = 0.28694171\n",
            "Iteration 12755, loss = 0.28693268\n",
            "Iteration 12756, loss = 0.28692614\n",
            "Iteration 12757, loss = 0.28691131\n",
            "Iteration 12758, loss = 0.28690142\n",
            "Iteration 12759, loss = 0.28690392\n",
            "Iteration 12760, loss = 0.28689058\n",
            "Iteration 12761, loss = 0.28686790\n",
            "Iteration 12762, loss = 0.28686654\n",
            "Iteration 12763, loss = 0.28686689\n",
            "Iteration 12764, loss = 0.28686160\n",
            "Iteration 12765, loss = 0.28684141\n",
            "Iteration 12766, loss = 0.28684765\n",
            "Iteration 12767, loss = 0.28684144\n",
            "Iteration 12768, loss = 0.28681796\n",
            "Iteration 12769, loss = 0.28681311\n",
            "Iteration 12770, loss = 0.28681847\n",
            "Iteration 12771, loss = 0.28679933\n",
            "Iteration 12772, loss = 0.28678700\n",
            "Iteration 12773, loss = 0.28678053\n",
            "Iteration 12774, loss = 0.28676566\n",
            "Iteration 12775, loss = 0.28675597\n",
            "Iteration 12776, loss = 0.28674532\n",
            "Iteration 12777, loss = 0.28674704\n",
            "Iteration 12778, loss = 0.28673004\n",
            "Iteration 12779, loss = 0.28672430\n",
            "Iteration 12780, loss = 0.28671826\n",
            "Iteration 12781, loss = 0.28670671\n",
            "Iteration 12782, loss = 0.28668861\n",
            "Iteration 12783, loss = 0.28667900\n",
            "Iteration 12784, loss = 0.28667813\n",
            "Iteration 12785, loss = 0.28666260\n",
            "Iteration 12786, loss = 0.28665189\n",
            "Iteration 12787, loss = 0.28667408\n",
            "Iteration 12788, loss = 0.28665281\n",
            "Iteration 12789, loss = 0.28663954\n",
            "Iteration 12790, loss = 0.28663697\n",
            "Iteration 12791, loss = 0.28663837\n",
            "Iteration 12792, loss = 0.28660868\n",
            "Iteration 12793, loss = 0.28663097\n",
            "Iteration 12794, loss = 0.28661655\n",
            "Iteration 12795, loss = 0.28659080\n",
            "Iteration 12796, loss = 0.28660523\n",
            "Iteration 12797, loss = 0.28657985\n",
            "Iteration 12798, loss = 0.28658054\n",
            "Iteration 12799, loss = 0.28658394\n",
            "Iteration 12800, loss = 0.28654324\n",
            "Iteration 12801, loss = 0.28655607\n",
            "Iteration 12802, loss = 0.28654263\n",
            "Iteration 12803, loss = 0.28652054\n",
            "Iteration 12804, loss = 0.28651269\n",
            "Iteration 12805, loss = 0.28651715\n",
            "Iteration 12806, loss = 0.28650398\n",
            "Iteration 12807, loss = 0.28647985\n",
            "Iteration 12808, loss = 0.28646775\n",
            "Iteration 12809, loss = 0.28648443\n",
            "Iteration 12810, loss = 0.28644930\n",
            "Iteration 12811, loss = 0.28645340\n",
            "Iteration 12812, loss = 0.28645535\n",
            "Iteration 12813, loss = 0.28643617\n",
            "Iteration 12814, loss = 0.28642943\n",
            "Iteration 12815, loss = 0.28643210\n",
            "Iteration 12816, loss = 0.28641355\n",
            "Iteration 12817, loss = 0.28640496\n",
            "Iteration 12818, loss = 0.28641054\n",
            "Iteration 12819, loss = 0.28640242\n",
            "Iteration 12820, loss = 0.28637402\n",
            "Iteration 12821, loss = 0.28638264\n",
            "Iteration 12822, loss = 0.28638559\n",
            "Iteration 12823, loss = 0.28637002\n",
            "Iteration 12824, loss = 0.28634447\n",
            "Iteration 12825, loss = 0.28632643\n",
            "Iteration 12826, loss = 0.28632506\n",
            "Iteration 12827, loss = 0.28630520\n",
            "Iteration 12828, loss = 0.28629436\n",
            "Iteration 12829, loss = 0.28628322\n",
            "Iteration 12830, loss = 0.28627086\n",
            "Iteration 12831, loss = 0.28627107\n",
            "Iteration 12832, loss = 0.28629379\n",
            "Iteration 12833, loss = 0.28626623\n",
            "Iteration 12834, loss = 0.28624365\n",
            "Iteration 12835, loss = 0.28623518\n",
            "Iteration 12836, loss = 0.28623465\n",
            "Iteration 12837, loss = 0.28622102\n",
            "Iteration 12838, loss = 0.28621405\n",
            "Iteration 12839, loss = 0.28620978\n",
            "Iteration 12840, loss = 0.28620178\n",
            "Iteration 12841, loss = 0.28619850\n",
            "Iteration 12842, loss = 0.28618562\n",
            "Iteration 12843, loss = 0.28617878\n",
            "Iteration 12844, loss = 0.28617386\n",
            "Iteration 12845, loss = 0.28615852\n",
            "Iteration 12846, loss = 0.28615353\n",
            "Iteration 12847, loss = 0.28614138\n",
            "Iteration 12848, loss = 0.28613759\n",
            "Iteration 12849, loss = 0.28611896\n",
            "Iteration 12850, loss = 0.28612867\n",
            "Iteration 12851, loss = 0.28610673\n",
            "Iteration 12852, loss = 0.28609257\n",
            "Iteration 12853, loss = 0.28609407\n",
            "Iteration 12854, loss = 0.28607513\n",
            "Iteration 12855, loss = 0.28608423\n",
            "Iteration 12856, loss = 0.28607867\n",
            "Iteration 12857, loss = 0.28606421\n",
            "Iteration 12858, loss = 0.28606658\n",
            "Iteration 12859, loss = 0.28604758\n",
            "Iteration 12860, loss = 0.28605446\n",
            "Iteration 12861, loss = 0.28604283\n",
            "Iteration 12862, loss = 0.28601803\n",
            "Iteration 12863, loss = 0.28601669\n",
            "Iteration 12864, loss = 0.28600764\n",
            "Iteration 12865, loss = 0.28600181\n",
            "Iteration 12866, loss = 0.28598955\n",
            "Iteration 12867, loss = 0.28597307\n",
            "Iteration 12868, loss = 0.28596505\n",
            "Iteration 12869, loss = 0.28595186\n",
            "Iteration 12870, loss = 0.28595162\n",
            "Iteration 12871, loss = 0.28593773\n",
            "Iteration 12872, loss = 0.28592468\n",
            "Iteration 12873, loss = 0.28591585\n",
            "Iteration 12874, loss = 0.28590347\n",
            "Iteration 12875, loss = 0.28589395\n",
            "Iteration 12876, loss = 0.28588282\n",
            "Iteration 12877, loss = 0.28587561\n",
            "Iteration 12878, loss = 0.28587449\n",
            "Iteration 12879, loss = 0.28586039\n",
            "Iteration 12880, loss = 0.28586630\n",
            "Iteration 12881, loss = 0.28586649\n",
            "Iteration 12882, loss = 0.28585764\n",
            "Iteration 12883, loss = 0.28583877\n",
            "Iteration 12884, loss = 0.28584146\n",
            "Iteration 12885, loss = 0.28583135\n",
            "Iteration 12886, loss = 0.28580979\n",
            "Iteration 12887, loss = 0.28580629\n",
            "Iteration 12888, loss = 0.28579710\n",
            "Iteration 12889, loss = 0.28579412\n",
            "Iteration 12890, loss = 0.28577970\n",
            "Iteration 12891, loss = 0.28577039\n",
            "Iteration 12892, loss = 0.28576053\n",
            "Iteration 12893, loss = 0.28574833\n",
            "Iteration 12894, loss = 0.28574527\n",
            "Iteration 12895, loss = 0.28573557\n",
            "Iteration 12896, loss = 0.28571663\n",
            "Iteration 12897, loss = 0.28570236\n",
            "Iteration 12898, loss = 0.28569885\n",
            "Iteration 12899, loss = 0.28568831\n",
            "Iteration 12900, loss = 0.28568529\n",
            "Iteration 12901, loss = 0.28566506\n",
            "Iteration 12902, loss = 0.28566667\n",
            "Iteration 12903, loss = 0.28566847\n",
            "Iteration 12904, loss = 0.28565884\n",
            "Iteration 12905, loss = 0.28566242\n",
            "Iteration 12906, loss = 0.28564033\n",
            "Iteration 12907, loss = 0.28562905\n",
            "Iteration 12908, loss = 0.28563738\n",
            "Iteration 12909, loss = 0.28564326\n",
            "Iteration 12910, loss = 0.28560854\n",
            "Iteration 12911, loss = 0.28561047\n",
            "Iteration 12912, loss = 0.28560188\n",
            "Iteration 12913, loss = 0.28558163\n",
            "Iteration 12914, loss = 0.28558040\n",
            "Iteration 12915, loss = 0.28558098\n",
            "Iteration 12916, loss = 0.28556888\n",
            "Iteration 12917, loss = 0.28554456\n",
            "Iteration 12918, loss = 0.28553978\n",
            "Iteration 12919, loss = 0.28554169\n",
            "Iteration 12920, loss = 0.28552130\n",
            "Iteration 12921, loss = 0.28550499\n",
            "Iteration 12922, loss = 0.28553598\n",
            "Iteration 12923, loss = 0.28551612\n",
            "Iteration 12924, loss = 0.28547694\n",
            "Iteration 12925, loss = 0.28549990\n",
            "Iteration 12926, loss = 0.28546079\n",
            "Iteration 12927, loss = 0.28549192\n",
            "Iteration 12928, loss = 0.28547767\n",
            "Iteration 12929, loss = 0.28544652\n",
            "Iteration 12930, loss = 0.28546152\n",
            "Iteration 12931, loss = 0.28545256\n",
            "Iteration 12932, loss = 0.28541771\n",
            "Iteration 12933, loss = 0.28544418\n",
            "Iteration 12934, loss = 0.28542310\n",
            "Iteration 12935, loss = 0.28539863\n",
            "Iteration 12936, loss = 0.28543056\n",
            "Iteration 12937, loss = 0.28537986\n",
            "Iteration 12938, loss = 0.28536657\n",
            "Iteration 12939, loss = 0.28537784\n",
            "Iteration 12940, loss = 0.28537317\n",
            "Iteration 12941, loss = 0.28534739\n",
            "Iteration 12942, loss = 0.28532732\n",
            "Iteration 12943, loss = 0.28532394\n",
            "Iteration 12944, loss = 0.28532418\n",
            "Iteration 12945, loss = 0.28531380\n",
            "Iteration 12946, loss = 0.28531781\n",
            "Iteration 12947, loss = 0.28528127\n",
            "Iteration 12948, loss = 0.28528286\n",
            "Iteration 12949, loss = 0.28528076\n",
            "Iteration 12950, loss = 0.28526830\n",
            "Iteration 12951, loss = 0.28526705\n",
            "Iteration 12952, loss = 0.28525953\n",
            "Iteration 12953, loss = 0.28525016\n",
            "Iteration 12954, loss = 0.28524608\n",
            "Iteration 12955, loss = 0.28523846\n",
            "Iteration 12956, loss = 0.28523683\n",
            "Iteration 12957, loss = 0.28520939\n",
            "Iteration 12958, loss = 0.28521117\n",
            "Iteration 12959, loss = 0.28521205\n",
            "Iteration 12960, loss = 0.28519524\n",
            "Iteration 12961, loss = 0.28516970\n",
            "Iteration 12962, loss = 0.28516382\n",
            "Iteration 12963, loss = 0.28516305\n",
            "Iteration 12964, loss = 0.28515206\n",
            "Iteration 12965, loss = 0.28511650\n",
            "Iteration 12966, loss = 0.28512750\n",
            "Iteration 12967, loss = 0.28511527\n",
            "Iteration 12968, loss = 0.28508876\n",
            "Iteration 12969, loss = 0.28510515\n",
            "Iteration 12970, loss = 0.28509192\n",
            "Iteration 12971, loss = 0.28507221\n",
            "Iteration 12972, loss = 0.28508613\n",
            "Iteration 12973, loss = 0.28506243\n",
            "Iteration 12974, loss = 0.28504964\n",
            "Iteration 12975, loss = 0.28504105\n",
            "Iteration 12976, loss = 0.28503349\n",
            "Iteration 12977, loss = 0.28501981\n",
            "Iteration 12978, loss = 0.28501081\n",
            "Iteration 12979, loss = 0.28500400\n",
            "Iteration 12980, loss = 0.28498946\n",
            "Iteration 12981, loss = 0.28498170\n",
            "Iteration 12982, loss = 0.28496899\n",
            "Iteration 12983, loss = 0.28495753\n",
            "Iteration 12984, loss = 0.28494901\n",
            "Iteration 12985, loss = 0.28494040\n",
            "Iteration 12986, loss = 0.28492480\n",
            "Iteration 12987, loss = 0.28492904\n",
            "Iteration 12988, loss = 0.28490870\n",
            "Iteration 12989, loss = 0.28490516\n",
            "Iteration 12990, loss = 0.28490394\n",
            "Iteration 12991, loss = 0.28489041\n",
            "Iteration 12992, loss = 0.28487433\n",
            "Iteration 12993, loss = 0.28486249\n",
            "Iteration 12994, loss = 0.28486101\n",
            "Iteration 12995, loss = 0.28484806\n",
            "Iteration 12996, loss = 0.28485077\n",
            "Iteration 12997, loss = 0.28484142\n",
            "Iteration 12998, loss = 0.28483361\n",
            "Iteration 12999, loss = 0.28482332\n",
            "Iteration 13000, loss = 0.28481408\n",
            "Iteration 13001, loss = 0.28480395\n",
            "Iteration 13002, loss = 0.28479709\n",
            "Iteration 13003, loss = 0.28477904\n",
            "Iteration 13004, loss = 0.28478002\n",
            "Iteration 13005, loss = 0.28476613\n",
            "Iteration 13006, loss = 0.28475317\n",
            "Iteration 13007, loss = 0.28475048\n",
            "Iteration 13008, loss = 0.28473610\n",
            "Iteration 13009, loss = 0.28471425\n",
            "Iteration 13010, loss = 0.28471445\n",
            "Iteration 13011, loss = 0.28469619\n",
            "Iteration 13012, loss = 0.28469324\n",
            "Iteration 13013, loss = 0.28468785\n",
            "Iteration 13014, loss = 0.28467254\n",
            "Iteration 13015, loss = 0.28464831\n",
            "Iteration 13016, loss = 0.28470560\n",
            "Iteration 13017, loss = 0.28465237\n",
            "Iteration 13018, loss = 0.28463113\n",
            "Iteration 13019, loss = 0.28463472\n",
            "Iteration 13020, loss = 0.28464093\n",
            "Iteration 13021, loss = 0.28461494\n",
            "Iteration 13022, loss = 0.28460648\n",
            "Iteration 13023, loss = 0.28460489\n",
            "Iteration 13024, loss = 0.28458137\n",
            "Iteration 13025, loss = 0.28459206\n",
            "Iteration 13026, loss = 0.28459586\n",
            "Iteration 13027, loss = 0.28457881\n",
            "Iteration 13028, loss = 0.28455501\n",
            "Iteration 13029, loss = 0.28455657\n",
            "Iteration 13030, loss = 0.28455985\n",
            "Iteration 13031, loss = 0.28453267\n",
            "Iteration 13032, loss = 0.28450788\n",
            "Iteration 13033, loss = 0.28450884\n",
            "Iteration 13034, loss = 0.28450041\n",
            "Iteration 13035, loss = 0.28447665\n",
            "Iteration 13036, loss = 0.28449236\n",
            "Iteration 13037, loss = 0.28448415\n",
            "Iteration 13038, loss = 0.28445850\n",
            "Iteration 13039, loss = 0.28448958\n",
            "Iteration 13040, loss = 0.28443244\n",
            "Iteration 13041, loss = 0.28442946\n",
            "Iteration 13042, loss = 0.28441543\n",
            "Iteration 13043, loss = 0.28442103\n",
            "Iteration 13044, loss = 0.28441550\n",
            "Iteration 13045, loss = 0.28439284\n",
            "Iteration 13046, loss = 0.28439400\n",
            "Iteration 13047, loss = 0.28438879\n",
            "Iteration 13048, loss = 0.28437261\n",
            "Iteration 13049, loss = 0.28436597\n",
            "Iteration 13050, loss = 0.28435699\n",
            "Iteration 13051, loss = 0.28433723\n",
            "Iteration 13052, loss = 0.28436299\n",
            "Iteration 13053, loss = 0.28434361\n",
            "Iteration 13054, loss = 0.28438741\n",
            "Iteration 13055, loss = 0.28437559\n",
            "Iteration 13056, loss = 0.28432731\n",
            "Iteration 13057, loss = 0.28429409\n",
            "Iteration 13058, loss = 0.28432330\n",
            "Iteration 13059, loss = 0.28426744\n",
            "Iteration 13060, loss = 0.28428131\n",
            "Iteration 13061, loss = 0.28426776\n",
            "Iteration 13062, loss = 0.28425416\n",
            "Iteration 13063, loss = 0.28424403\n",
            "Iteration 13064, loss = 0.28422412\n",
            "Iteration 13065, loss = 0.28422390\n",
            "Iteration 13066, loss = 0.28421331\n",
            "Iteration 13067, loss = 0.28420738\n",
            "Iteration 13068, loss = 0.28421145\n",
            "Iteration 13069, loss = 0.28421125\n",
            "Iteration 13070, loss = 0.28417808\n",
            "Iteration 13071, loss = 0.28418897\n",
            "Iteration 13072, loss = 0.28419272\n",
            "Iteration 13073, loss = 0.28418170\n",
            "Iteration 13074, loss = 0.28415652\n",
            "Iteration 13075, loss = 0.28413621\n",
            "Iteration 13076, loss = 0.28412853\n",
            "Iteration 13077, loss = 0.28412318\n",
            "Iteration 13078, loss = 0.28411426\n",
            "Iteration 13079, loss = 0.28411716\n",
            "Iteration 13080, loss = 0.28410772\n",
            "Iteration 13081, loss = 0.28408663\n",
            "Iteration 13082, loss = 0.28405569\n",
            "Iteration 13083, loss = 0.28407951\n",
            "Iteration 13084, loss = 0.28405673\n",
            "Iteration 13085, loss = 0.28406035\n",
            "Iteration 13086, loss = 0.28404118\n",
            "Iteration 13087, loss = 0.28404003\n",
            "Iteration 13088, loss = 0.28402937\n",
            "Iteration 13089, loss = 0.28403082\n",
            "Iteration 13090, loss = 0.28399158\n",
            "Iteration 13091, loss = 0.28402494\n",
            "Iteration 13092, loss = 0.28404028\n",
            "Iteration 13093, loss = 0.28400262\n",
            "Iteration 13094, loss = 0.28397722\n",
            "Iteration 13095, loss = 0.28396154\n",
            "Iteration 13096, loss = 0.28396540\n",
            "Iteration 13097, loss = 0.28396001\n",
            "Iteration 13098, loss = 0.28393178\n",
            "Iteration 13099, loss = 0.28392754\n",
            "Iteration 13100, loss = 0.28392988\n",
            "Iteration 13101, loss = 0.28390645\n",
            "Iteration 13102, loss = 0.28387918\n",
            "Iteration 13103, loss = 0.28387655\n",
            "Iteration 13104, loss = 0.28386713\n",
            "Iteration 13105, loss = 0.28385198\n",
            "Iteration 13106, loss = 0.28385009\n",
            "Iteration 13107, loss = 0.28386532\n",
            "Iteration 13108, loss = 0.28385412\n",
            "Iteration 13109, loss = 0.28382380\n",
            "Iteration 13110, loss = 0.28380680\n",
            "Iteration 13111, loss = 0.28387462\n",
            "Iteration 13112, loss = 0.28381262\n",
            "Iteration 13113, loss = 0.28385895\n",
            "Iteration 13114, loss = 0.28383597\n",
            "Iteration 13115, loss = 0.28381030\n",
            "Iteration 13116, loss = 0.28382915\n",
            "Iteration 13117, loss = 0.28382456\n",
            "Iteration 13118, loss = 0.28374590\n",
            "Iteration 13119, loss = 0.28376802\n",
            "Iteration 13120, loss = 0.28378783\n",
            "Iteration 13121, loss = 0.28377723\n",
            "Iteration 13122, loss = 0.28373950\n",
            "Iteration 13123, loss = 0.28369834\n",
            "Iteration 13124, loss = 0.28370120\n",
            "Iteration 13125, loss = 0.28371953\n",
            "Iteration 13126, loss = 0.28367931\n",
            "Iteration 13127, loss = 0.28365545\n",
            "Iteration 13128, loss = 0.28366549\n",
            "Iteration 13129, loss = 0.28366478\n",
            "Iteration 13130, loss = 0.28364354\n",
            "Iteration 13131, loss = 0.28360853\n",
            "Iteration 13132, loss = 0.28365672\n",
            "Iteration 13133, loss = 0.28360639\n",
            "Iteration 13134, loss = 0.28359575\n",
            "Iteration 13135, loss = 0.28358437\n",
            "Iteration 13136, loss = 0.28358639\n",
            "Iteration 13137, loss = 0.28357898\n",
            "Iteration 13138, loss = 0.28356607\n",
            "Iteration 13139, loss = 0.28356128\n",
            "Iteration 13140, loss = 0.28355293\n",
            "Iteration 13141, loss = 0.28354156\n",
            "Iteration 13142, loss = 0.28353050\n",
            "Iteration 13143, loss = 0.28351883\n",
            "Iteration 13144, loss = 0.28351582\n",
            "Iteration 13145, loss = 0.28350535\n",
            "Iteration 13146, loss = 0.28349555\n",
            "Iteration 13147, loss = 0.28348297\n",
            "Iteration 13148, loss = 0.28347509\n",
            "Iteration 13149, loss = 0.28345990\n",
            "Iteration 13150, loss = 0.28345006\n",
            "Iteration 13151, loss = 0.28344152\n",
            "Iteration 13152, loss = 0.28342999\n",
            "Iteration 13153, loss = 0.28342054\n",
            "Iteration 13154, loss = 0.28341254\n",
            "Iteration 13155, loss = 0.28343816\n",
            "Iteration 13156, loss = 0.28339696\n",
            "Iteration 13157, loss = 0.28339101\n",
            "Iteration 13158, loss = 0.28338632\n",
            "Iteration 13159, loss = 0.28337876\n",
            "Iteration 13160, loss = 0.28337369\n",
            "Iteration 13161, loss = 0.28336468\n",
            "Iteration 13162, loss = 0.28336364\n",
            "Iteration 13163, loss = 0.28335037\n",
            "Iteration 13164, loss = 0.28334542\n",
            "Iteration 13165, loss = 0.28334099\n",
            "Iteration 13166, loss = 0.28332736\n",
            "Iteration 13167, loss = 0.28331391\n",
            "Iteration 13168, loss = 0.28330308\n",
            "Iteration 13169, loss = 0.28330280\n",
            "Iteration 13170, loss = 0.28329439\n",
            "Iteration 13171, loss = 0.28327025\n",
            "Iteration 13172, loss = 0.28326053\n",
            "Iteration 13173, loss = 0.28324930\n",
            "Iteration 13174, loss = 0.28323996\n",
            "Iteration 13175, loss = 0.28322959\n",
            "Iteration 13176, loss = 0.28322243\n",
            "Iteration 13177, loss = 0.28322007\n",
            "Iteration 13178, loss = 0.28320416\n",
            "Iteration 13179, loss = 0.28320298\n",
            "Iteration 13180, loss = 0.28319276\n",
            "Iteration 13181, loss = 0.28317899\n",
            "Iteration 13182, loss = 0.28317590\n",
            "Iteration 13183, loss = 0.28316799\n",
            "Iteration 13184, loss = 0.28316897\n",
            "Iteration 13185, loss = 0.28316362\n",
            "Iteration 13186, loss = 0.28314705\n",
            "Iteration 13187, loss = 0.28314337\n",
            "Iteration 13188, loss = 0.28314569\n",
            "Iteration 13189, loss = 0.28312823\n",
            "Iteration 13190, loss = 0.28314123\n",
            "Iteration 13191, loss = 0.28314085\n",
            "Iteration 13192, loss = 0.28311537\n",
            "Iteration 13193, loss = 0.28308523\n",
            "Iteration 13194, loss = 0.28311176\n",
            "Iteration 13195, loss = 0.28308060\n",
            "Iteration 13196, loss = 0.28307760\n",
            "Iteration 13197, loss = 0.28308488\n",
            "Iteration 13198, loss = 0.28307521\n",
            "Iteration 13199, loss = 0.28305792\n",
            "Iteration 13200, loss = 0.28303271\n",
            "Iteration 13201, loss = 0.28301608\n",
            "Iteration 13202, loss = 0.28301714\n",
            "Iteration 13203, loss = 0.28300320\n",
            "Iteration 13204, loss = 0.28300385\n",
            "Iteration 13205, loss = 0.28299019\n",
            "Iteration 13206, loss = 0.28296077\n",
            "Iteration 13207, loss = 0.28296079\n",
            "Iteration 13208, loss = 0.28296480\n",
            "Iteration 13209, loss = 0.28295646\n",
            "Iteration 13210, loss = 0.28293782\n",
            "Iteration 13211, loss = 0.28292254\n",
            "Iteration 13212, loss = 0.28292897\n",
            "Iteration 13213, loss = 0.28292002\n",
            "Iteration 13214, loss = 0.28289877\n",
            "Iteration 13215, loss = 0.28289654\n",
            "Iteration 13216, loss = 0.28290113\n",
            "Iteration 13217, loss = 0.28288626\n",
            "Iteration 13218, loss = 0.28286590\n",
            "Iteration 13219, loss = 0.28285431\n",
            "Iteration 13220, loss = 0.28284014\n",
            "Iteration 13221, loss = 0.28284401\n",
            "Iteration 13222, loss = 0.28282249\n",
            "Iteration 13223, loss = 0.28282638\n",
            "Iteration 13224, loss = 0.28282162\n",
            "Iteration 13225, loss = 0.28280444\n",
            "Iteration 13226, loss = 0.28284383\n",
            "Iteration 13227, loss = 0.28278769\n",
            "Iteration 13228, loss = 0.28279585\n",
            "Iteration 13229, loss = 0.28276781\n",
            "Iteration 13230, loss = 0.28276823\n",
            "Iteration 13231, loss = 0.28277074\n",
            "Iteration 13232, loss = 0.28276091\n",
            "Iteration 13233, loss = 0.28273886\n",
            "Iteration 13234, loss = 0.28273881\n",
            "Iteration 13235, loss = 0.28273723\n",
            "Iteration 13236, loss = 0.28271326\n",
            "Iteration 13237, loss = 0.28270428\n",
            "Iteration 13238, loss = 0.28270999\n",
            "Iteration 13239, loss = 0.28268636\n",
            "Iteration 13240, loss = 0.28266242\n",
            "Iteration 13241, loss = 0.28266082\n",
            "Iteration 13242, loss = 0.28264218\n",
            "Iteration 13243, loss = 0.28264235\n",
            "Iteration 13244, loss = 0.28263521\n",
            "Iteration 13245, loss = 0.28264219\n",
            "Iteration 13246, loss = 0.28260216\n",
            "Iteration 13247, loss = 0.28262463\n",
            "Iteration 13248, loss = 0.28263379\n",
            "Iteration 13249, loss = 0.28262313\n",
            "Iteration 13250, loss = 0.28256916\n",
            "Iteration 13251, loss = 0.28258451\n",
            "Iteration 13252, loss = 0.28262033\n",
            "Iteration 13253, loss = 0.28260768\n",
            "Iteration 13254, loss = 0.28256573\n",
            "Iteration 13255, loss = 0.28253736\n",
            "Iteration 13256, loss = 0.28255777\n",
            "Iteration 13257, loss = 0.28255475\n",
            "Iteration 13258, loss = 0.28251095\n",
            "Iteration 13259, loss = 0.28252689\n",
            "Iteration 13260, loss = 0.28253255\n",
            "Iteration 13261, loss = 0.28252327\n",
            "Iteration 13262, loss = 0.28250102\n",
            "Iteration 13263, loss = 0.28246882\n",
            "Iteration 13264, loss = 0.28248989\n",
            "Iteration 13265, loss = 0.28245763\n",
            "Iteration 13266, loss = 0.28243966\n",
            "Iteration 13267, loss = 0.28243944\n",
            "Iteration 13268, loss = 0.28242517\n",
            "Iteration 13269, loss = 0.28239949\n",
            "Iteration 13270, loss = 0.28239556\n",
            "Iteration 13271, loss = 0.28238176\n",
            "Iteration 13272, loss = 0.28237236\n",
            "Iteration 13273, loss = 0.28238055\n",
            "Iteration 13274, loss = 0.28235669\n",
            "Iteration 13275, loss = 0.28236030\n",
            "Iteration 13276, loss = 0.28234443\n",
            "Iteration 13277, loss = 0.28234886\n",
            "Iteration 13278, loss = 0.28234997\n",
            "Iteration 13279, loss = 0.28233863\n",
            "Iteration 13280, loss = 0.28233038\n",
            "Iteration 13281, loss = 0.28230821\n",
            "Iteration 13282, loss = 0.28231492\n",
            "Iteration 13283, loss = 0.28230601\n",
            "Iteration 13284, loss = 0.28228403\n",
            "Iteration 13285, loss = 0.28227971\n",
            "Iteration 13286, loss = 0.28226847\n",
            "Iteration 13287, loss = 0.28226532\n",
            "Iteration 13288, loss = 0.28225287\n",
            "Iteration 13289, loss = 0.28222954\n",
            "Iteration 13290, loss = 0.28222410\n",
            "Iteration 13291, loss = 0.28221035\n",
            "Iteration 13292, loss = 0.28220790\n",
            "Iteration 13293, loss = 0.28219417\n",
            "Iteration 13294, loss = 0.28217649\n",
            "Iteration 13295, loss = 0.28217670\n",
            "Iteration 13296, loss = 0.28216416\n",
            "Iteration 13297, loss = 0.28218315\n",
            "Iteration 13298, loss = 0.28214902\n",
            "Iteration 13299, loss = 0.28214573\n",
            "Iteration 13300, loss = 0.28214396\n",
            "Iteration 13301, loss = 0.28213328\n",
            "Iteration 13302, loss = 0.28212345\n",
            "Iteration 13303, loss = 0.28212839\n",
            "Iteration 13304, loss = 0.28211466\n",
            "Iteration 13305, loss = 0.28210478\n",
            "Iteration 13306, loss = 0.28210147\n",
            "Iteration 13307, loss = 0.28208642\n",
            "Iteration 13308, loss = 0.28208349\n",
            "Iteration 13309, loss = 0.28207368\n",
            "Iteration 13310, loss = 0.28207052\n",
            "Iteration 13311, loss = 0.28204893\n",
            "Iteration 13312, loss = 0.28204336\n",
            "Iteration 13313, loss = 0.28204264\n",
            "Iteration 13314, loss = 0.28202936\n",
            "Iteration 13315, loss = 0.28200619\n",
            "Iteration 13316, loss = 0.28200627\n",
            "Iteration 13317, loss = 0.28199854\n",
            "Iteration 13318, loss = 0.28198717\n",
            "Iteration 13319, loss = 0.28196391\n",
            "Iteration 13320, loss = 0.28196018\n",
            "Iteration 13321, loss = 0.28198679\n",
            "Iteration 13322, loss = 0.28194484\n",
            "Iteration 13323, loss = 0.28194048\n",
            "Iteration 13324, loss = 0.28193423\n",
            "Iteration 13325, loss = 0.28192768\n",
            "Iteration 13326, loss = 0.28193944\n",
            "Iteration 13327, loss = 0.28193114\n",
            "Iteration 13328, loss = 0.28191046\n",
            "Iteration 13329, loss = 0.28189610\n",
            "Iteration 13330, loss = 0.28189211\n",
            "Iteration 13331, loss = 0.28189389\n",
            "Iteration 13332, loss = 0.28186771\n",
            "Iteration 13333, loss = 0.28185143\n",
            "Iteration 13334, loss = 0.28185378\n",
            "Iteration 13335, loss = 0.28183891\n",
            "Iteration 13336, loss = 0.28181839\n",
            "Iteration 13337, loss = 0.28182172\n",
            "Iteration 13338, loss = 0.28181431\n",
            "Iteration 13339, loss = 0.28179489\n",
            "Iteration 13340, loss = 0.28178864\n",
            "Iteration 13341, loss = 0.28177738\n",
            "Iteration 13342, loss = 0.28176194\n",
            "Iteration 13343, loss = 0.28174354\n",
            "Iteration 13344, loss = 0.28173853\n",
            "Iteration 13345, loss = 0.28174372\n",
            "Iteration 13346, loss = 0.28171354\n",
            "Iteration 13347, loss = 0.28172584\n",
            "Iteration 13348, loss = 0.28172551\n",
            "Iteration 13349, loss = 0.28170554\n",
            "Iteration 13350, loss = 0.28169531\n",
            "Iteration 13351, loss = 0.28169375\n",
            "Iteration 13352, loss = 0.28169071\n",
            "Iteration 13353, loss = 0.28168428\n",
            "Iteration 13354, loss = 0.28168203\n",
            "Iteration 13355, loss = 0.28166742\n",
            "Iteration 13356, loss = 0.28165545\n",
            "Iteration 13357, loss = 0.28164990\n",
            "Iteration 13358, loss = 0.28163296\n",
            "Iteration 13359, loss = 0.28162176\n",
            "Iteration 13360, loss = 0.28160659\n",
            "Iteration 13361, loss = 0.28160327\n",
            "Iteration 13362, loss = 0.28159359\n",
            "Iteration 13363, loss = 0.28157957\n",
            "Iteration 13364, loss = 0.28156835\n",
            "Iteration 13365, loss = 0.28155860\n",
            "Iteration 13366, loss = 0.28154803\n",
            "Iteration 13367, loss = 0.28153093\n",
            "Iteration 13368, loss = 0.28152988\n",
            "Iteration 13369, loss = 0.28152336\n",
            "Iteration 13370, loss = 0.28150214\n",
            "Iteration 13371, loss = 0.28151095\n",
            "Iteration 13372, loss = 0.28149510\n",
            "Iteration 13373, loss = 0.28150160\n",
            "Iteration 13374, loss = 0.28150062\n",
            "Iteration 13375, loss = 0.28148754\n",
            "Iteration 13376, loss = 0.28147508\n",
            "Iteration 13377, loss = 0.28147449\n",
            "Iteration 13378, loss = 0.28146174\n",
            "Iteration 13379, loss = 0.28144774\n",
            "Iteration 13380, loss = 0.28143399\n",
            "Iteration 13381, loss = 0.28144335\n",
            "Iteration 13382, loss = 0.28143061\n",
            "Iteration 13383, loss = 0.28140158\n",
            "Iteration 13384, loss = 0.28139743\n",
            "Iteration 13385, loss = 0.28139123\n",
            "Iteration 13386, loss = 0.28137574\n",
            "Iteration 13387, loss = 0.28136626\n",
            "Iteration 13388, loss = 0.28135401\n",
            "Iteration 13389, loss = 0.28133756\n",
            "Iteration 13390, loss = 0.28132926\n",
            "Iteration 13391, loss = 0.28131995\n",
            "Iteration 13392, loss = 0.28130444\n",
            "Iteration 13393, loss = 0.28129993\n",
            "Iteration 13394, loss = 0.28133527\n",
            "Iteration 13395, loss = 0.28127622\n",
            "Iteration 13396, loss = 0.28128366\n",
            "Iteration 13397, loss = 0.28127045\n",
            "Iteration 13398, loss = 0.28127259\n",
            "Iteration 13399, loss = 0.28126432\n",
            "Iteration 13400, loss = 0.28125550\n",
            "Iteration 13401, loss = 0.28125156\n",
            "Iteration 13402, loss = 0.28124635\n",
            "Iteration 13403, loss = 0.28122830\n",
            "Iteration 13404, loss = 0.28122752\n",
            "Iteration 13405, loss = 0.28121805\n",
            "Iteration 13406, loss = 0.28120332\n",
            "Iteration 13407, loss = 0.28119975\n",
            "Iteration 13408, loss = 0.28120391\n",
            "Iteration 13409, loss = 0.28116902\n",
            "Iteration 13410, loss = 0.28117944\n",
            "Iteration 13411, loss = 0.28119250\n",
            "Iteration 13412, loss = 0.28117509\n",
            "Iteration 13413, loss = 0.28115242\n",
            "Iteration 13414, loss = 0.28112175\n",
            "Iteration 13415, loss = 0.28114354\n",
            "Iteration 13416, loss = 0.28113620\n",
            "Iteration 13417, loss = 0.28109106\n",
            "Iteration 13418, loss = 0.28113761\n",
            "Iteration 13419, loss = 0.28111303\n",
            "Iteration 13420, loss = 0.28110168\n",
            "Iteration 13421, loss = 0.28107713\n",
            "Iteration 13422, loss = 0.28105901\n",
            "Iteration 13423, loss = 0.28107418\n",
            "Iteration 13424, loss = 0.28104997\n",
            "Iteration 13425, loss = 0.28103982\n",
            "Iteration 13426, loss = 0.28103859\n",
            "Iteration 13427, loss = 0.28103047\n",
            "Iteration 13428, loss = 0.28101049\n",
            "Iteration 13429, loss = 0.28100522\n",
            "Iteration 13430, loss = 0.28100393\n",
            "Iteration 13431, loss = 0.28099045\n",
            "Iteration 13432, loss = 0.28096543\n",
            "Iteration 13433, loss = 0.28097245\n",
            "Iteration 13434, loss = 0.28097161\n",
            "Iteration 13435, loss = 0.28095706\n",
            "Iteration 13436, loss = 0.28093468\n",
            "Iteration 13437, loss = 0.28091376\n",
            "Iteration 13438, loss = 0.28090889\n",
            "Iteration 13439, loss = 0.28089171\n",
            "Iteration 13440, loss = 0.28088113\n",
            "Iteration 13441, loss = 0.28087627\n",
            "Iteration 13442, loss = 0.28087264\n",
            "Iteration 13443, loss = 0.28085550\n",
            "Iteration 13444, loss = 0.28086113\n",
            "Iteration 13445, loss = 0.28085765\n",
            "Iteration 13446, loss = 0.28084347\n",
            "Iteration 13447, loss = 0.28083686\n",
            "Iteration 13448, loss = 0.28082570\n",
            "Iteration 13449, loss = 0.28082370\n",
            "Iteration 13450, loss = 0.28081157\n",
            "Iteration 13451, loss = 0.28079804\n",
            "Iteration 13452, loss = 0.28078985\n",
            "Iteration 13453, loss = 0.28079096\n",
            "Iteration 13454, loss = 0.28078346\n",
            "Iteration 13455, loss = 0.28076441\n",
            "Iteration 13456, loss = 0.28076929\n",
            "Iteration 13457, loss = 0.28076231\n",
            "Iteration 13458, loss = 0.28073042\n",
            "Iteration 13459, loss = 0.28072711\n",
            "Iteration 13460, loss = 0.28071405\n",
            "Iteration 13461, loss = 0.28070875\n",
            "Iteration 13462, loss = 0.28070135\n",
            "Iteration 13463, loss = 0.28068420\n",
            "Iteration 13464, loss = 0.28067762\n",
            "Iteration 13465, loss = 0.28067590\n",
            "Iteration 13466, loss = 0.28064838\n",
            "Iteration 13467, loss = 0.28067782\n",
            "Iteration 13468, loss = 0.28065987\n",
            "Iteration 13469, loss = 0.28063325\n",
            "Iteration 13470, loss = 0.28064244\n",
            "Iteration 13471, loss = 0.28065386\n",
            "Iteration 13472, loss = 0.28064053\n",
            "Iteration 13473, loss = 0.28061962\n",
            "Iteration 13474, loss = 0.28060724\n",
            "Iteration 13475, loss = 0.28061072\n",
            "Iteration 13476, loss = 0.28059417\n",
            "Iteration 13477, loss = 0.28056795\n",
            "Iteration 13478, loss = 0.28057522\n",
            "Iteration 13479, loss = 0.28057026\n",
            "Iteration 13480, loss = 0.28054111\n",
            "Iteration 13481, loss = 0.28054700\n",
            "Iteration 13482, loss = 0.28054965\n",
            "Iteration 13483, loss = 0.28053856\n",
            "Iteration 13484, loss = 0.28052141\n",
            "Iteration 13485, loss = 0.28049586\n",
            "Iteration 13486, loss = 0.28049464\n",
            "Iteration 13487, loss = 0.28049806\n",
            "Iteration 13488, loss = 0.28047525\n",
            "Iteration 13489, loss = 0.28045081\n",
            "Iteration 13490, loss = 0.28046101\n",
            "Iteration 13491, loss = 0.28045816\n",
            "Iteration 13492, loss = 0.28048800\n",
            "Iteration 13493, loss = 0.28042099\n",
            "Iteration 13494, loss = 0.28042249\n",
            "Iteration 13495, loss = 0.28044004\n",
            "Iteration 13496, loss = 0.28040913\n",
            "Iteration 13497, loss = 0.28040821\n",
            "Iteration 13498, loss = 0.28042431\n",
            "Iteration 13499, loss = 0.28043018\n",
            "Iteration 13500, loss = 0.28042326\n",
            "Iteration 13501, loss = 0.28041783\n",
            "Iteration 13502, loss = 0.28038157\n",
            "Iteration 13503, loss = 0.28033777\n",
            "Iteration 13504, loss = 0.28037306\n",
            "Iteration 13505, loss = 0.28036778\n",
            "Iteration 13506, loss = 0.28031886\n",
            "Iteration 13507, loss = 0.28033090\n",
            "Iteration 13508, loss = 0.28032668\n",
            "Iteration 13509, loss = 0.28025936\n",
            "Iteration 13510, loss = 0.28025450\n",
            "Iteration 13511, loss = 0.28024470\n",
            "Iteration 13512, loss = 0.28023145\n",
            "Iteration 13513, loss = 0.28021505\n",
            "Iteration 13514, loss = 0.28020816\n",
            "Iteration 13515, loss = 0.28017911\n",
            "Iteration 13516, loss = 0.28016946\n",
            "Iteration 13517, loss = 0.28016170\n",
            "Iteration 13518, loss = 0.28014794\n",
            "Iteration 13519, loss = 0.28014429\n",
            "Iteration 13520, loss = 0.28014240\n",
            "Iteration 13521, loss = 0.28013756\n",
            "Iteration 13522, loss = 0.28013164\n",
            "Iteration 13523, loss = 0.28012175\n",
            "Iteration 13524, loss = 0.28010652\n",
            "Iteration 13525, loss = 0.28008626\n",
            "Iteration 13526, loss = 0.28008416\n",
            "Iteration 13527, loss = 0.28006356\n",
            "Iteration 13528, loss = 0.28009366\n",
            "Iteration 13529, loss = 0.28003482\n",
            "Iteration 13530, loss = 0.28003156\n",
            "Iteration 13531, loss = 0.28003011\n",
            "Iteration 13532, loss = 0.28003311\n",
            "Iteration 13533, loss = 0.28000454\n",
            "Iteration 13534, loss = 0.27999124\n",
            "Iteration 13535, loss = 0.27998949\n",
            "Iteration 13536, loss = 0.27996232\n",
            "Iteration 13537, loss = 0.27993823\n",
            "Iteration 13538, loss = 0.27993784\n",
            "Iteration 13539, loss = 0.27992293\n",
            "Iteration 13540, loss = 0.27990236\n",
            "Iteration 13541, loss = 0.27989412\n",
            "Iteration 13542, loss = 0.27989689\n",
            "Iteration 13543, loss = 0.27987857\n",
            "Iteration 13544, loss = 0.27989534\n",
            "Iteration 13545, loss = 0.27990355\n",
            "Iteration 13546, loss = 0.27986183\n",
            "Iteration 13547, loss = 0.27985241\n",
            "Iteration 13548, loss = 0.27986669\n",
            "Iteration 13549, loss = 0.27982897\n",
            "Iteration 13550, loss = 0.27980673\n",
            "Iteration 13551, loss = 0.27980655\n",
            "Iteration 13552, loss = 0.27979206\n",
            "Iteration 13553, loss = 0.27977600\n",
            "Iteration 13554, loss = 0.27976863\n",
            "Iteration 13555, loss = 0.27975549\n",
            "Iteration 13556, loss = 0.27973686\n",
            "Iteration 13557, loss = 0.27972445\n",
            "Iteration 13558, loss = 0.27971128\n",
            "Iteration 13559, loss = 0.27969175\n",
            "Iteration 13560, loss = 0.27967896\n",
            "Iteration 13561, loss = 0.27966642\n",
            "Iteration 13562, loss = 0.27968887\n",
            "Iteration 13563, loss = 0.27965095\n",
            "Iteration 13564, loss = 0.27964467\n",
            "Iteration 13565, loss = 0.27963072\n",
            "Iteration 13566, loss = 0.27965300\n",
            "Iteration 13567, loss = 0.27962398\n",
            "Iteration 13568, loss = 0.27963829\n",
            "Iteration 13569, loss = 0.27961994\n",
            "Iteration 13570, loss = 0.27961329\n",
            "Iteration 13571, loss = 0.27960830\n",
            "Iteration 13572, loss = 0.27960731\n",
            "Iteration 13573, loss = 0.27958182\n",
            "Iteration 13574, loss = 0.27955886\n",
            "Iteration 13575, loss = 0.27955437\n",
            "Iteration 13576, loss = 0.27954077\n",
            "Iteration 13577, loss = 0.27954767\n",
            "Iteration 13578, loss = 0.27954740\n",
            "Iteration 13579, loss = 0.27950465\n",
            "Iteration 13580, loss = 0.27949132\n",
            "Iteration 13581, loss = 0.27947862\n",
            "Iteration 13582, loss = 0.27945924\n",
            "Iteration 13583, loss = 0.27945404\n",
            "Iteration 13584, loss = 0.27948553\n",
            "Iteration 13585, loss = 0.27946143\n",
            "Iteration 13586, loss = 0.27945264\n",
            "Iteration 13587, loss = 0.27943525\n",
            "Iteration 13588, loss = 0.27943768\n",
            "Iteration 13589, loss = 0.27942153\n",
            "Iteration 13590, loss = 0.27939075\n",
            "Iteration 13591, loss = 0.27937801\n",
            "Iteration 13592, loss = 0.27938183\n",
            "Iteration 13593, loss = 0.27936774\n",
            "Iteration 13594, loss = 0.27936618\n",
            "Iteration 13595, loss = 0.27935561\n",
            "Iteration 13596, loss = 0.27933380\n",
            "Iteration 13597, loss = 0.27934807\n",
            "Iteration 13598, loss = 0.27933490\n",
            "Iteration 13599, loss = 0.27931307\n",
            "Iteration 13600, loss = 0.27930482\n",
            "Iteration 13601, loss = 0.27928981\n",
            "Iteration 13602, loss = 0.27926644\n",
            "Iteration 13603, loss = 0.27925039\n",
            "Iteration 13604, loss = 0.27923338\n",
            "Iteration 13605, loss = 0.27921257\n",
            "Iteration 13606, loss = 0.27920688\n",
            "Iteration 13607, loss = 0.27922426\n",
            "Iteration 13608, loss = 0.27918993\n",
            "Iteration 13609, loss = 0.27918844\n",
            "Iteration 13610, loss = 0.27919165\n",
            "Iteration 13611, loss = 0.27918854\n",
            "Iteration 13612, loss = 0.27918265\n",
            "Iteration 13613, loss = 0.27916348\n",
            "Iteration 13614, loss = 0.27914716\n",
            "Iteration 13615, loss = 0.27912850\n",
            "Iteration 13616, loss = 0.27912240\n",
            "Iteration 13617, loss = 0.27911125\n",
            "Iteration 13618, loss = 0.27910402\n",
            "Iteration 13619, loss = 0.27909040\n",
            "Iteration 13620, loss = 0.27908379\n",
            "Iteration 13621, loss = 0.27905975\n",
            "Iteration 13622, loss = 0.27904655\n",
            "Iteration 13623, loss = 0.27904654\n",
            "Iteration 13624, loss = 0.27903498\n",
            "Iteration 13625, loss = 0.27902397\n",
            "Iteration 13626, loss = 0.27900642\n",
            "Iteration 13627, loss = 0.27898502\n",
            "Iteration 13628, loss = 0.27896900\n",
            "Iteration 13629, loss = 0.27901645\n",
            "Iteration 13630, loss = 0.27896616\n",
            "Iteration 13631, loss = 0.27894849\n",
            "Iteration 13632, loss = 0.27894177\n",
            "Iteration 13633, loss = 0.27893596\n",
            "Iteration 13634, loss = 0.27893642\n",
            "Iteration 13635, loss = 0.27892645\n",
            "Iteration 13636, loss = 0.27891528\n",
            "Iteration 13637, loss = 0.27889836\n",
            "Iteration 13638, loss = 0.27890216\n",
            "Iteration 13639, loss = 0.27889658\n",
            "Iteration 13640, loss = 0.27888113\n",
            "Iteration 13641, loss = 0.27886641\n",
            "Iteration 13642, loss = 0.27885929\n",
            "Iteration 13643, loss = 0.27883315\n",
            "Iteration 13644, loss = 0.27885765\n",
            "Iteration 13645, loss = 0.27883225\n",
            "Iteration 13646, loss = 0.27880158\n",
            "Iteration 13647, loss = 0.27879072\n",
            "Iteration 13648, loss = 0.27877400\n",
            "Iteration 13649, loss = 0.27876650\n",
            "Iteration 13650, loss = 0.27875982\n",
            "Iteration 13651, loss = 0.27880126\n",
            "Iteration 13652, loss = 0.27872861\n",
            "Iteration 13653, loss = 0.27873506\n",
            "Iteration 13654, loss = 0.27873118\n",
            "Iteration 13655, loss = 0.27871949\n",
            "Iteration 13656, loss = 0.27871499\n",
            "Iteration 13657, loss = 0.27870454\n",
            "Iteration 13658, loss = 0.27868794\n",
            "Iteration 13659, loss = 0.27868834\n",
            "Iteration 13660, loss = 0.27867032\n",
            "Iteration 13661, loss = 0.27866500\n",
            "Iteration 13662, loss = 0.27865006\n",
            "Iteration 13663, loss = 0.27863527\n",
            "Iteration 13664, loss = 0.27862519\n",
            "Iteration 13665, loss = 0.27860810\n",
            "Iteration 13666, loss = 0.27860798\n",
            "Iteration 13667, loss = 0.27859671\n",
            "Iteration 13668, loss = 0.27857764\n",
            "Iteration 13669, loss = 0.27858631\n",
            "Iteration 13670, loss = 0.27854892\n",
            "Iteration 13671, loss = 0.27858027\n",
            "Iteration 13672, loss = 0.27855354\n",
            "Iteration 13673, loss = 0.27855747\n",
            "Iteration 13674, loss = 0.27854908\n",
            "Iteration 13675, loss = 0.27853733\n",
            "Iteration 13676, loss = 0.27849634\n",
            "Iteration 13677, loss = 0.27850153\n",
            "Iteration 13678, loss = 0.27849381\n",
            "Iteration 13679, loss = 0.27847359\n",
            "Iteration 13680, loss = 0.27846821\n",
            "Iteration 13681, loss = 0.27845669\n",
            "Iteration 13682, loss = 0.27844396\n",
            "Iteration 13683, loss = 0.27843774\n",
            "Iteration 13684, loss = 0.27843068\n",
            "Iteration 13685, loss = 0.27842238\n",
            "Iteration 13686, loss = 0.27840676\n",
            "Iteration 13687, loss = 0.27839156\n",
            "Iteration 13688, loss = 0.27838015\n",
            "Iteration 13689, loss = 0.27837092\n",
            "Iteration 13690, loss = 0.27836032\n",
            "Iteration 13691, loss = 0.27834348\n",
            "Iteration 13692, loss = 0.27832828\n",
            "Iteration 13693, loss = 0.27831502\n",
            "Iteration 13694, loss = 0.27830139\n",
            "Iteration 13695, loss = 0.27829545\n",
            "Iteration 13696, loss = 0.27828906\n",
            "Iteration 13697, loss = 0.27828071\n",
            "Iteration 13698, loss = 0.27827659\n",
            "Iteration 13699, loss = 0.27826449\n",
            "Iteration 13700, loss = 0.27826689\n",
            "Iteration 13701, loss = 0.27826073\n",
            "Iteration 13702, loss = 0.27824011\n",
            "Iteration 13703, loss = 0.27825634\n",
            "Iteration 13704, loss = 0.27823864\n",
            "Iteration 13705, loss = 0.27821957\n",
            "Iteration 13706, loss = 0.27822584\n",
            "Iteration 13707, loss = 0.27818820\n",
            "Iteration 13708, loss = 0.27818344\n",
            "Iteration 13709, loss = 0.27817214\n",
            "Iteration 13710, loss = 0.27816381\n",
            "Iteration 13711, loss = 0.27814611\n",
            "Iteration 13712, loss = 0.27813075\n",
            "Iteration 13713, loss = 0.27812033\n",
            "Iteration 13714, loss = 0.27810327\n",
            "Iteration 13715, loss = 0.27808757\n",
            "Iteration 13716, loss = 0.27807586\n",
            "Iteration 13717, loss = 0.27806288\n",
            "Iteration 13718, loss = 0.27810254\n",
            "Iteration 13719, loss = 0.27804502\n",
            "Iteration 13720, loss = 0.27803816\n",
            "Iteration 13721, loss = 0.27804009\n",
            "Iteration 13722, loss = 0.27803537\n",
            "Iteration 13723, loss = 0.27803305\n",
            "Iteration 13724, loss = 0.27801892\n",
            "Iteration 13725, loss = 0.27800053\n",
            "Iteration 13726, loss = 0.27799853\n",
            "Iteration 13727, loss = 0.27799016\n",
            "Iteration 13728, loss = 0.27797677\n",
            "Iteration 13729, loss = 0.27796122\n",
            "Iteration 13730, loss = 0.27794966\n",
            "Iteration 13731, loss = 0.27794372\n",
            "Iteration 13732, loss = 0.27793302\n",
            "Iteration 13733, loss = 0.27791518\n",
            "Iteration 13734, loss = 0.27792026\n",
            "Iteration 13735, loss = 0.27788959\n",
            "Iteration 13736, loss = 0.27792786\n",
            "Iteration 13737, loss = 0.27786458\n",
            "Iteration 13738, loss = 0.27790587\n",
            "Iteration 13739, loss = 0.27787790\n",
            "Iteration 13740, loss = 0.27787887\n",
            "Iteration 13741, loss = 0.27788372\n",
            "Iteration 13742, loss = 0.27787132\n",
            "Iteration 13743, loss = 0.27782182\n",
            "Iteration 13744, loss = 0.27784481\n",
            "Iteration 13745, loss = 0.27783139\n",
            "Iteration 13746, loss = 0.27779188\n",
            "Iteration 13747, loss = 0.27783008\n",
            "Iteration 13748, loss = 0.27777580\n",
            "Iteration 13749, loss = 0.27777380\n",
            "Iteration 13750, loss = 0.27775918\n",
            "Iteration 13751, loss = 0.27775875\n",
            "Iteration 13752, loss = 0.27775507\n",
            "Iteration 13753, loss = 0.27774441\n",
            "Iteration 13754, loss = 0.27772673\n",
            "Iteration 13755, loss = 0.27770317\n",
            "Iteration 13756, loss = 0.27769183\n",
            "Iteration 13757, loss = 0.27769688\n",
            "Iteration 13758, loss = 0.27766487\n",
            "Iteration 13759, loss = 0.27765423\n",
            "Iteration 13760, loss = 0.27764761\n",
            "Iteration 13761, loss = 0.27763158\n",
            "Iteration 13762, loss = 0.27760972\n",
            "Iteration 13763, loss = 0.27766618\n",
            "Iteration 13764, loss = 0.27760095\n",
            "Iteration 13765, loss = 0.27759717\n",
            "Iteration 13766, loss = 0.27758260\n",
            "Iteration 13767, loss = 0.27758907\n",
            "Iteration 13768, loss = 0.27760923\n",
            "Iteration 13769, loss = 0.27756122\n",
            "Iteration 13770, loss = 0.27759616\n",
            "Iteration 13771, loss = 0.27756568\n",
            "Iteration 13772, loss = 0.27755255\n",
            "Iteration 13773, loss = 0.27754690\n",
            "Iteration 13774, loss = 0.27752796\n",
            "Iteration 13775, loss = 0.27750371\n",
            "Iteration 13776, loss = 0.27749124\n",
            "Iteration 13777, loss = 0.27749905\n",
            "Iteration 13778, loss = 0.27749523\n",
            "Iteration 13779, loss = 0.27746007\n",
            "Iteration 13780, loss = 0.27744899\n",
            "Iteration 13781, loss = 0.27745454\n",
            "Iteration 13782, loss = 0.27745231\n",
            "Iteration 13783, loss = 0.27744115\n",
            "Iteration 13784, loss = 0.27742271\n",
            "Iteration 13785, loss = 0.27739902\n",
            "Iteration 13786, loss = 0.27743872\n",
            "Iteration 13787, loss = 0.27736845\n",
            "Iteration 13788, loss = 0.27736671\n",
            "Iteration 13789, loss = 0.27735240\n",
            "Iteration 13790, loss = 0.27735550\n",
            "Iteration 13791, loss = 0.27735849\n",
            "Iteration 13792, loss = 0.27734221\n",
            "Iteration 13793, loss = 0.27733485\n",
            "Iteration 13794, loss = 0.27732678\n",
            "Iteration 13795, loss = 0.27731259\n",
            "Iteration 13796, loss = 0.27730228\n",
            "Iteration 13797, loss = 0.27728770\n",
            "Iteration 13798, loss = 0.27726738\n",
            "Iteration 13799, loss = 0.27726809\n",
            "Iteration 13800, loss = 0.27726124\n",
            "Iteration 13801, loss = 0.27725395\n",
            "Iteration 13802, loss = 0.27721924\n",
            "Iteration 13803, loss = 0.27720974\n",
            "Iteration 13804, loss = 0.27719923\n",
            "Iteration 13805, loss = 0.27718200\n",
            "Iteration 13806, loss = 0.27716864\n",
            "Iteration 13807, loss = 0.27715283\n",
            "Iteration 13808, loss = 0.27714505\n",
            "Iteration 13809, loss = 0.27720226\n",
            "Iteration 13810, loss = 0.27712433\n",
            "Iteration 13811, loss = 0.27711977\n",
            "Iteration 13812, loss = 0.27711709\n",
            "Iteration 13813, loss = 0.27712017\n",
            "Iteration 13814, loss = 0.27710183\n",
            "Iteration 13815, loss = 0.27712385\n",
            "Iteration 13816, loss = 0.27710571\n",
            "Iteration 13817, loss = 0.27709016\n",
            "Iteration 13818, loss = 0.27708448\n",
            "Iteration 13819, loss = 0.27705412\n",
            "Iteration 13820, loss = 0.27704083\n",
            "Iteration 13821, loss = 0.27703073\n",
            "Iteration 13822, loss = 0.27702167\n",
            "Iteration 13823, loss = 0.27700872\n",
            "Iteration 13824, loss = 0.27700478\n",
            "Iteration 13825, loss = 0.27698780\n",
            "Iteration 13826, loss = 0.27697865\n",
            "Iteration 13827, loss = 0.27696311\n",
            "Iteration 13828, loss = 0.27694892\n",
            "Iteration 13829, loss = 0.27694476\n",
            "Iteration 13830, loss = 0.27692235\n",
            "Iteration 13831, loss = 0.27695105\n",
            "Iteration 13832, loss = 0.27690905\n",
            "Iteration 13833, loss = 0.27691340\n",
            "Iteration 13834, loss = 0.27692364\n",
            "Iteration 13835, loss = 0.27690189\n",
            "Iteration 13836, loss = 0.27688211\n",
            "Iteration 13837, loss = 0.27691769\n",
            "Iteration 13838, loss = 0.27688504\n",
            "Iteration 13839, loss = 0.27687062\n",
            "Iteration 13840, loss = 0.27688053\n",
            "Iteration 13841, loss = 0.27684002\n",
            "Iteration 13842, loss = 0.27685109\n",
            "Iteration 13843, loss = 0.27684956\n",
            "Iteration 13844, loss = 0.27682406\n",
            "Iteration 13845, loss = 0.27680360\n",
            "Iteration 13846, loss = 0.27680799\n",
            "Iteration 13847, loss = 0.27678338\n",
            "Iteration 13848, loss = 0.27676075\n",
            "Iteration 13849, loss = 0.27676353\n",
            "Iteration 13850, loss = 0.27673627\n",
            "Iteration 13851, loss = 0.27672790\n",
            "Iteration 13852, loss = 0.27672678\n",
            "Iteration 13853, loss = 0.27670061\n",
            "Iteration 13854, loss = 0.27668978\n",
            "Iteration 13855, loss = 0.27668891\n",
            "Iteration 13856, loss = 0.27667479\n",
            "Iteration 13857, loss = 0.27666975\n",
            "Iteration 13858, loss = 0.27665974\n",
            "Iteration 13859, loss = 0.27665579\n",
            "Iteration 13860, loss = 0.27665036\n",
            "Iteration 13861, loss = 0.27664011\n",
            "Iteration 13862, loss = 0.27663366\n",
            "Iteration 13863, loss = 0.27662097\n",
            "Iteration 13864, loss = 0.27661171\n",
            "Iteration 13865, loss = 0.27659796\n",
            "Iteration 13866, loss = 0.27659351\n",
            "Iteration 13867, loss = 0.27658603\n",
            "Iteration 13868, loss = 0.27657173\n",
            "Iteration 13869, loss = 0.27655118\n",
            "Iteration 13870, loss = 0.27654395\n",
            "Iteration 13871, loss = 0.27653459\n",
            "Iteration 13872, loss = 0.27651364\n",
            "Iteration 13873, loss = 0.27652093\n",
            "Iteration 13874, loss = 0.27649191\n",
            "Iteration 13875, loss = 0.27650252\n",
            "Iteration 13876, loss = 0.27647097\n",
            "Iteration 13877, loss = 0.27646624\n",
            "Iteration 13878, loss = 0.27651713\n",
            "Iteration 13879, loss = 0.27643183\n",
            "Iteration 13880, loss = 0.27645259\n",
            "Iteration 13881, loss = 0.27642662\n",
            "Iteration 13882, loss = 0.27643599\n",
            "Iteration 13883, loss = 0.27642008\n",
            "Iteration 13884, loss = 0.27641196\n",
            "Iteration 13885, loss = 0.27643630\n",
            "Iteration 13886, loss = 0.27638966\n",
            "Iteration 13887, loss = 0.27640776\n",
            "Iteration 13888, loss = 0.27642077\n",
            "Iteration 13889, loss = 0.27638204\n",
            "Iteration 13890, loss = 0.27636891\n",
            "Iteration 13891, loss = 0.27636902\n",
            "Iteration 13892, loss = 0.27633709\n",
            "Iteration 13893, loss = 0.27630960\n",
            "Iteration 13894, loss = 0.27632422\n",
            "Iteration 13895, loss = 0.27631802\n",
            "Iteration 13896, loss = 0.27629857\n",
            "Iteration 13897, loss = 0.27627503\n",
            "Iteration 13898, loss = 0.27625634\n",
            "Iteration 13899, loss = 0.27623627\n",
            "Iteration 13900, loss = 0.27623110\n",
            "Iteration 13901, loss = 0.27627779\n",
            "Iteration 13902, loss = 0.27621503\n",
            "Iteration 13903, loss = 0.27621027\n",
            "Iteration 13904, loss = 0.27619408\n",
            "Iteration 13905, loss = 0.27619108\n",
            "Iteration 13906, loss = 0.27618744\n",
            "Iteration 13907, loss = 0.27617446\n",
            "Iteration 13908, loss = 0.27618161\n",
            "Iteration 13909, loss = 0.27617833\n",
            "Iteration 13910, loss = 0.27616251\n",
            "Iteration 13911, loss = 0.27614697\n",
            "Iteration 13912, loss = 0.27612655\n",
            "Iteration 13913, loss = 0.27611721\n",
            "Iteration 13914, loss = 0.27610756\n",
            "Iteration 13915, loss = 0.27609454\n",
            "Iteration 13916, loss = 0.27609392\n",
            "Iteration 13917, loss = 0.27606871\n",
            "Iteration 13918, loss = 0.27606176\n",
            "Iteration 13919, loss = 0.27606919\n",
            "Iteration 13920, loss = 0.27604450\n",
            "Iteration 13921, loss = 0.27602252\n",
            "Iteration 13922, loss = 0.27600345\n",
            "Iteration 13923, loss = 0.27599350\n",
            "Iteration 13924, loss = 0.27599921\n",
            "Iteration 13925, loss = 0.27597880\n",
            "Iteration 13926, loss = 0.27597094\n",
            "Iteration 13927, loss = 0.27595843\n",
            "Iteration 13928, loss = 0.27595125\n",
            "Iteration 13929, loss = 0.27594929\n",
            "Iteration 13930, loss = 0.27593584\n",
            "Iteration 13931, loss = 0.27593930\n",
            "Iteration 13932, loss = 0.27593638\n",
            "Iteration 13933, loss = 0.27592764\n",
            "Iteration 13934, loss = 0.27591390\n",
            "Iteration 13935, loss = 0.27589674\n",
            "Iteration 13936, loss = 0.27588649\n",
            "Iteration 13937, loss = 0.27588809\n",
            "Iteration 13938, loss = 0.27589475\n",
            "Iteration 13939, loss = 0.27585717\n",
            "Iteration 13940, loss = 0.27584056\n",
            "Iteration 13941, loss = 0.27582299\n",
            "Iteration 13942, loss = 0.27581587\n",
            "Iteration 13943, loss = 0.27580256\n",
            "Iteration 13944, loss = 0.27578309\n",
            "Iteration 13945, loss = 0.27578423\n",
            "Iteration 13946, loss = 0.27575511\n",
            "Iteration 13947, loss = 0.27575143\n",
            "Iteration 13948, loss = 0.27579275\n",
            "Iteration 13949, loss = 0.27574293\n",
            "Iteration 13950, loss = 0.27572422\n",
            "Iteration 13951, loss = 0.27574154\n",
            "Iteration 13952, loss = 0.27572726\n",
            "Iteration 13953, loss = 0.27571701\n",
            "Iteration 13954, loss = 0.27572499\n",
            "Iteration 13955, loss = 0.27570194\n",
            "Iteration 13956, loss = 0.27567726\n",
            "Iteration 13957, loss = 0.27568132\n",
            "Iteration 13958, loss = 0.27568621\n",
            "Iteration 13959, loss = 0.27568564\n",
            "Iteration 13960, loss = 0.27566128\n",
            "Iteration 13961, loss = 0.27566152\n",
            "Iteration 13962, loss = 0.27568160\n",
            "Iteration 13963, loss = 0.27566397\n",
            "Iteration 13964, loss = 0.27564566\n",
            "Iteration 13965, loss = 0.27563518\n",
            "Iteration 13966, loss = 0.27559665\n",
            "Iteration 13967, loss = 0.27556211\n",
            "Iteration 13968, loss = 0.27555272\n",
            "Iteration 13969, loss = 0.27555490\n",
            "Iteration 13970, loss = 0.27558322\n",
            "Iteration 13971, loss = 0.27552335\n",
            "Iteration 13972, loss = 0.27558334\n",
            "Iteration 13973, loss = 0.27551399\n",
            "Iteration 13974, loss = 0.27552564\n",
            "Iteration 13975, loss = 0.27553087\n",
            "Iteration 13976, loss = 0.27552849\n",
            "Iteration 13977, loss = 0.27551630\n",
            "Iteration 13978, loss = 0.27549592\n",
            "Iteration 13979, loss = 0.27547086\n",
            "Iteration 13980, loss = 0.27544667\n",
            "Iteration 13981, loss = 0.27544466\n",
            "Iteration 13982, loss = 0.27542479\n",
            "Iteration 13983, loss = 0.27542066\n",
            "Iteration 13984, loss = 0.27541825\n",
            "Iteration 13985, loss = 0.27539759\n",
            "Iteration 13986, loss = 0.27537570\n",
            "Iteration 13987, loss = 0.27538987\n",
            "Iteration 13988, loss = 0.27538164\n",
            "Iteration 13989, loss = 0.27534837\n",
            "Iteration 13990, loss = 0.27534362\n",
            "Iteration 13991, loss = 0.27533547\n",
            "Iteration 13992, loss = 0.27531405\n",
            "Iteration 13993, loss = 0.27531154\n",
            "Iteration 13994, loss = 0.27528366\n",
            "Iteration 13995, loss = 0.27526968\n",
            "Iteration 13996, loss = 0.27531534\n",
            "Iteration 13997, loss = 0.27525145\n",
            "Iteration 13998, loss = 0.27525447\n",
            "Iteration 13999, loss = 0.27523986\n",
            "Iteration 14000, loss = 0.27523344\n",
            "Iteration 14001, loss = 0.27522194\n",
            "Iteration 14002, loss = 0.27523584\n",
            "Iteration 14003, loss = 0.27520714\n",
            "Iteration 14004, loss = 0.27520752\n",
            "Iteration 14005, loss = 0.27519441\n",
            "Iteration 14006, loss = 0.27518170\n",
            "Iteration 14007, loss = 0.27518028\n",
            "Iteration 14008, loss = 0.27515879\n",
            "Iteration 14009, loss = 0.27515256\n",
            "Iteration 14010, loss = 0.27513814\n",
            "Iteration 14011, loss = 0.27513084\n",
            "Iteration 14012, loss = 0.27511784\n",
            "Iteration 14013, loss = 0.27510358\n",
            "Iteration 14014, loss = 0.27510750\n",
            "Iteration 14015, loss = 0.27507157\n",
            "Iteration 14016, loss = 0.27506737\n",
            "Iteration 14017, loss = 0.27505235\n",
            "Iteration 14018, loss = 0.27503935\n",
            "Iteration 14019, loss = 0.27502606\n",
            "Iteration 14020, loss = 0.27507188\n",
            "Iteration 14021, loss = 0.27500385\n",
            "Iteration 14022, loss = 0.27499626\n",
            "Iteration 14023, loss = 0.27499559\n",
            "Iteration 14024, loss = 0.27498914\n",
            "Iteration 14025, loss = 0.27497889\n",
            "Iteration 14026, loss = 0.27496874\n",
            "Iteration 14027, loss = 0.27497312\n",
            "Iteration 14028, loss = 0.27496891\n",
            "Iteration 14029, loss = 0.27494883\n",
            "Iteration 14030, loss = 0.27493929\n",
            "Iteration 14031, loss = 0.27492888\n",
            "Iteration 14032, loss = 0.27491928\n",
            "Iteration 14033, loss = 0.27490194\n",
            "Iteration 14034, loss = 0.27488806\n",
            "Iteration 14035, loss = 0.27487690\n",
            "Iteration 14036, loss = 0.27486333\n",
            "Iteration 14037, loss = 0.27485952\n",
            "Iteration 14038, loss = 0.27484358\n",
            "Iteration 14039, loss = 0.27483153\n",
            "Iteration 14040, loss = 0.27482062\n",
            "Iteration 14041, loss = 0.27480305\n",
            "Iteration 14042, loss = 0.27480371\n",
            "Iteration 14043, loss = 0.27477594\n",
            "Iteration 14044, loss = 0.27482801\n",
            "Iteration 14045, loss = 0.27476296\n",
            "Iteration 14046, loss = 0.27476845\n",
            "Iteration 14047, loss = 0.27477431\n",
            "Iteration 14048, loss = 0.27477817\n",
            "Iteration 14049, loss = 0.27475248\n",
            "Iteration 14050, loss = 0.27475379\n",
            "Iteration 14051, loss = 0.27472939\n",
            "Iteration 14052, loss = 0.27474327\n",
            "Iteration 14053, loss = 0.27474637\n",
            "Iteration 14054, loss = 0.27470343\n",
            "Iteration 14055, loss = 0.27470574\n",
            "Iteration 14056, loss = 0.27467909\n",
            "Iteration 14057, loss = 0.27466865\n",
            "Iteration 14058, loss = 0.27466981\n",
            "Iteration 14059, loss = 0.27465734\n",
            "Iteration 14060, loss = 0.27464306\n",
            "Iteration 14061, loss = 0.27463944\n",
            "Iteration 14062, loss = 0.27460234\n",
            "Iteration 14063, loss = 0.27459168\n",
            "Iteration 14064, loss = 0.27459657\n",
            "Iteration 14065, loss = 0.27457510\n",
            "Iteration 14066, loss = 0.27456710\n",
            "Iteration 14067, loss = 0.27454331\n",
            "Iteration 14068, loss = 0.27455537\n",
            "Iteration 14069, loss = 0.27454263\n",
            "Iteration 14070, loss = 0.27453898\n",
            "Iteration 14071, loss = 0.27452796\n",
            "Iteration 14072, loss = 0.27451251\n",
            "Iteration 14073, loss = 0.27452039\n",
            "Iteration 14074, loss = 0.27448780\n",
            "Iteration 14075, loss = 0.27449183\n",
            "Iteration 14076, loss = 0.27447542\n",
            "Iteration 14077, loss = 0.27446303\n",
            "Iteration 14078, loss = 0.27445656\n",
            "Iteration 14079, loss = 0.27444431\n",
            "Iteration 14080, loss = 0.27444097\n",
            "Iteration 14081, loss = 0.27442625\n",
            "Iteration 14082, loss = 0.27442305\n",
            "Iteration 14083, loss = 0.27441759\n",
            "Iteration 14084, loss = 0.27440661\n",
            "Iteration 14085, loss = 0.27438902\n",
            "Iteration 14086, loss = 0.27436556\n",
            "Iteration 14087, loss = 0.27436125\n",
            "Iteration 14088, loss = 0.27433075\n",
            "Iteration 14089, loss = 0.27431902\n",
            "Iteration 14090, loss = 0.27431536\n",
            "Iteration 14091, loss = 0.27430697\n",
            "Iteration 14092, loss = 0.27428242\n",
            "Iteration 14093, loss = 0.27434394\n",
            "Iteration 14094, loss = 0.27426847\n",
            "Iteration 14095, loss = 0.27428266\n",
            "Iteration 14096, loss = 0.27427126\n",
            "Iteration 14097, loss = 0.27425173\n",
            "Iteration 14098, loss = 0.27426679\n",
            "Iteration 14099, loss = 0.27423213\n",
            "Iteration 14100, loss = 0.27429635\n",
            "Iteration 14101, loss = 0.27423917\n",
            "Iteration 14102, loss = 0.27427163\n",
            "Iteration 14103, loss = 0.27423191\n",
            "Iteration 14104, loss = 0.27424532\n",
            "Iteration 14105, loss = 0.27425078\n",
            "Iteration 14106, loss = 0.27420106\n",
            "Iteration 14107, loss = 0.27427836\n",
            "Iteration 14108, loss = 0.27417486\n",
            "Iteration 14109, loss = 0.27419219\n",
            "Iteration 14110, loss = 0.27415504\n",
            "Iteration 14111, loss = 0.27413143\n",
            "Iteration 14112, loss = 0.27414890\n",
            "Iteration 14113, loss = 0.27409028\n",
            "Iteration 14114, loss = 0.27410230\n",
            "Iteration 14115, loss = 0.27408636\n",
            "Iteration 14116, loss = 0.27407560\n",
            "Iteration 14117, loss = 0.27406136\n",
            "Iteration 14118, loss = 0.27410713\n",
            "Iteration 14119, loss = 0.27403170\n",
            "Iteration 14120, loss = 0.27401759\n",
            "Iteration 14121, loss = 0.27403326\n",
            "Iteration 14122, loss = 0.27401627\n",
            "Iteration 14123, loss = 0.27400079\n",
            "Iteration 14124, loss = 0.27399515\n",
            "Iteration 14125, loss = 0.27398185\n",
            "Iteration 14126, loss = 0.27397533\n",
            "Iteration 14127, loss = 0.27397377\n",
            "Iteration 14128, loss = 0.27396677\n",
            "Iteration 14129, loss = 0.27395369\n",
            "Iteration 14130, loss = 0.27393549\n",
            "Iteration 14131, loss = 0.27392887\n",
            "Iteration 14132, loss = 0.27391335\n",
            "Iteration 14133, loss = 0.27389819\n",
            "Iteration 14134, loss = 0.27388986\n",
            "Iteration 14135, loss = 0.27387773\n",
            "Iteration 14136, loss = 0.27386429\n",
            "Iteration 14137, loss = 0.27385428\n",
            "Iteration 14138, loss = 0.27384536\n",
            "Iteration 14139, loss = 0.27389182\n",
            "Iteration 14140, loss = 0.27381548\n",
            "Iteration 14141, loss = 0.27389331\n",
            "Iteration 14142, loss = 0.27378928\n",
            "Iteration 14143, loss = 0.27389629\n",
            "Iteration 14144, loss = 0.27378890\n",
            "Iteration 14145, loss = 0.27378206\n",
            "Iteration 14146, loss = 0.27380744\n",
            "Iteration 14147, loss = 0.27377034\n",
            "Iteration 14148, loss = 0.27378957\n",
            "Iteration 14149, loss = 0.27378609\n",
            "Iteration 14150, loss = 0.27376481\n",
            "Iteration 14151, loss = 0.27382871\n",
            "Iteration 14152, loss = 0.27376278\n",
            "Iteration 14153, loss = 0.27380232\n",
            "Iteration 14154, loss = 0.27375372\n",
            "Iteration 14155, loss = 0.27373834\n",
            "Iteration 14156, loss = 0.27376085\n",
            "Iteration 14157, loss = 0.27371995\n",
            "Iteration 14158, loss = 0.27372608\n",
            "Iteration 14159, loss = 0.27372246\n",
            "Iteration 14160, loss = 0.27367880\n",
            "Iteration 14161, loss = 0.27366970\n",
            "Iteration 14162, loss = 0.27365139\n",
            "Iteration 14163, loss = 0.27364120\n",
            "Iteration 14164, loss = 0.27362771\n",
            "Iteration 14165, loss = 0.27361593\n",
            "Iteration 14166, loss = 0.27359714\n",
            "Iteration 14167, loss = 0.27359395\n",
            "Iteration 14168, loss = 0.27357640\n",
            "Iteration 14169, loss = 0.27356703\n",
            "Iteration 14170, loss = 0.27356187\n",
            "Iteration 14171, loss = 0.27357306\n",
            "Iteration 14172, loss = 0.27352551\n",
            "Iteration 14173, loss = 0.27352464\n",
            "Iteration 14174, loss = 0.27357809\n",
            "Iteration 14175, loss = 0.27350009\n",
            "Iteration 14176, loss = 0.27350654\n",
            "Iteration 14177, loss = 0.27350254\n",
            "Iteration 14178, loss = 0.27343545\n",
            "Iteration 14179, loss = 0.27345817\n",
            "Iteration 14180, loss = 0.27342809\n",
            "Iteration 14181, loss = 0.27339828\n",
            "Iteration 14182, loss = 0.27350207\n",
            "Iteration 14183, loss = 0.27340527\n",
            "Iteration 14184, loss = 0.27341678\n",
            "Iteration 14185, loss = 0.27342286\n",
            "Iteration 14186, loss = 0.27340432\n",
            "Iteration 14187, loss = 0.27340024\n",
            "Iteration 14188, loss = 0.27338310\n",
            "Iteration 14189, loss = 0.27337909\n",
            "Iteration 14190, loss = 0.27337309\n",
            "Iteration 14191, loss = 0.27336485\n",
            "Iteration 14192, loss = 0.27336937\n",
            "Iteration 14193, loss = 0.27334047\n",
            "Iteration 14194, loss = 0.27336319\n",
            "Iteration 14195, loss = 0.27333802\n",
            "Iteration 14196, loss = 0.27341248\n",
            "Iteration 14197, loss = 0.27332531\n",
            "Iteration 14198, loss = 0.27333747\n",
            "Iteration 14199, loss = 0.27333325\n",
            "Iteration 14200, loss = 0.27327443\n",
            "Iteration 14201, loss = 0.27329747\n",
            "Iteration 14202, loss = 0.27328475\n",
            "Iteration 14203, loss = 0.27325765\n",
            "Iteration 14204, loss = 0.27326408\n",
            "Iteration 14205, loss = 0.27322080\n",
            "Iteration 14206, loss = 0.27323186\n",
            "Iteration 14207, loss = 0.27322369\n",
            "Iteration 14208, loss = 0.27317907\n",
            "Iteration 14209, loss = 0.27316165\n",
            "Iteration 14210, loss = 0.27316426\n",
            "Iteration 14211, loss = 0.27315665\n",
            "Iteration 14212, loss = 0.27314843\n",
            "Iteration 14213, loss = 0.27313399\n",
            "Iteration 14214, loss = 0.27311472\n",
            "Iteration 14215, loss = 0.27312809\n",
            "Iteration 14216, loss = 0.27310251\n",
            "Iteration 14217, loss = 0.27307562\n",
            "Iteration 14218, loss = 0.27305513\n",
            "Iteration 14219, loss = 0.27302877\n",
            "Iteration 14220, loss = 0.27302177\n",
            "Iteration 14221, loss = 0.27301606\n",
            "Iteration 14222, loss = 0.27300493\n",
            "Iteration 14223, loss = 0.27302147\n",
            "Iteration 14224, loss = 0.27300613\n",
            "Iteration 14225, loss = 0.27300225\n",
            "Iteration 14226, loss = 0.27299762\n",
            "Iteration 14227, loss = 0.27299590\n",
            "Iteration 14228, loss = 0.27297939\n",
            "Iteration 14229, loss = 0.27296217\n",
            "Iteration 14230, loss = 0.27294711\n",
            "Iteration 14231, loss = 0.27297819\n",
            "Iteration 14232, loss = 0.27293665\n",
            "Iteration 14233, loss = 0.27293931\n",
            "Iteration 14234, loss = 0.27293383\n",
            "Iteration 14235, loss = 0.27291322\n",
            "Iteration 14236, loss = 0.27289664\n",
            "Iteration 14237, loss = 0.27288882\n",
            "Iteration 14238, loss = 0.27288650\n",
            "Iteration 14239, loss = 0.27286989\n",
            "Iteration 14240, loss = 0.27285365\n",
            "Iteration 14241, loss = 0.27283202\n",
            "Iteration 14242, loss = 0.27280279\n",
            "Iteration 14243, loss = 0.27280746\n",
            "Iteration 14244, loss = 0.27283058\n",
            "Iteration 14245, loss = 0.27279172\n",
            "Iteration 14246, loss = 0.27279952\n",
            "Iteration 14247, loss = 0.27279095\n",
            "Iteration 14248, loss = 0.27278978\n",
            "Iteration 14249, loss = 0.27277278\n",
            "Iteration 14250, loss = 0.27276458\n",
            "Iteration 14251, loss = 0.27274548\n",
            "Iteration 14252, loss = 0.27273212\n",
            "Iteration 14253, loss = 0.27271984\n",
            "Iteration 14254, loss = 0.27271532\n",
            "Iteration 14255, loss = 0.27270781\n",
            "Iteration 14256, loss = 0.27269389\n",
            "Iteration 14257, loss = 0.27268932\n",
            "Iteration 14258, loss = 0.27267435\n",
            "Iteration 14259, loss = 0.27270604\n",
            "Iteration 14260, loss = 0.27265176\n",
            "Iteration 14261, loss = 0.27263302\n",
            "Iteration 14262, loss = 0.27269359\n",
            "Iteration 14263, loss = 0.27260660\n",
            "Iteration 14264, loss = 0.27270836\n",
            "Iteration 14265, loss = 0.27258247\n",
            "Iteration 14266, loss = 0.27272293\n",
            "Iteration 14267, loss = 0.27260331\n",
            "Iteration 14268, loss = 0.27260437\n",
            "Iteration 14269, loss = 0.27263375\n",
            "Iteration 14270, loss = 0.27257978\n",
            "Iteration 14271, loss = 0.27259481\n",
            "Iteration 14272, loss = 0.27254539\n",
            "Iteration 14273, loss = 0.27258251\n",
            "Iteration 14274, loss = 0.27251842\n",
            "Iteration 14275, loss = 0.27255601\n",
            "Iteration 14276, loss = 0.27250038\n",
            "Iteration 14277, loss = 0.27249927\n",
            "Iteration 14278, loss = 0.27250264\n",
            "Iteration 14279, loss = 0.27248347\n",
            "Iteration 14280, loss = 0.27247364\n",
            "Iteration 14281, loss = 0.27253331\n",
            "Iteration 14282, loss = 0.27244427\n",
            "Iteration 14283, loss = 0.27249897\n",
            "Iteration 14284, loss = 0.27242219\n",
            "Iteration 14285, loss = 0.27245751\n",
            "Iteration 14286, loss = 0.27240541\n",
            "Iteration 14287, loss = 0.27239769\n",
            "Iteration 14288, loss = 0.27240525\n",
            "Iteration 14289, loss = 0.27236398\n",
            "Iteration 14290, loss = 0.27239904\n",
            "Iteration 14291, loss = 0.27233104\n",
            "Iteration 14292, loss = 0.27242691\n",
            "Iteration 14293, loss = 0.27234092\n",
            "Iteration 14294, loss = 0.27233025\n",
            "Iteration 14295, loss = 0.27232513\n",
            "Iteration 14296, loss = 0.27234301\n",
            "Iteration 14297, loss = 0.27232287\n",
            "Iteration 14298, loss = 0.27233038\n",
            "Iteration 14299, loss = 0.27232126\n",
            "Iteration 14300, loss = 0.27230987\n",
            "Iteration 14301, loss = 0.27229566\n",
            "Iteration 14302, loss = 0.27232105\n",
            "Iteration 14303, loss = 0.27228279\n",
            "Iteration 14304, loss = 0.27228422\n",
            "Iteration 14305, loss = 0.27229276\n",
            "Iteration 14306, loss = 0.27226114\n",
            "Iteration 14307, loss = 0.27232029\n",
            "Iteration 14308, loss = 0.27223717\n",
            "Iteration 14309, loss = 0.27225536\n",
            "Iteration 14310, loss = 0.27221476\n",
            "Iteration 14311, loss = 0.27219719\n",
            "Iteration 14312, loss = 0.27219869\n",
            "Iteration 14313, loss = 0.27218266\n",
            "Iteration 14314, loss = 0.27221702\n",
            "Iteration 14315, loss = 0.27217665\n",
            "Iteration 14316, loss = 0.27218289\n",
            "Iteration 14317, loss = 0.27214204\n",
            "Iteration 14318, loss = 0.27211630\n",
            "Iteration 14319, loss = 0.27210062\n",
            "Iteration 14320, loss = 0.27213395\n",
            "Iteration 14321, loss = 0.27208142\n",
            "Iteration 14322, loss = 0.27210347\n",
            "Iteration 14323, loss = 0.27208396\n",
            "Iteration 14324, loss = 0.27209229\n",
            "Iteration 14325, loss = 0.27208489\n",
            "Iteration 14326, loss = 0.27205717\n",
            "Iteration 14327, loss = 0.27201891\n",
            "Iteration 14328, loss = 0.27198522\n",
            "Iteration 14329, loss = 0.27197825\n",
            "Iteration 14330, loss = 0.27199551\n",
            "Iteration 14331, loss = 0.27193933\n",
            "Iteration 14332, loss = 0.27194152\n",
            "Iteration 14333, loss = 0.27193771\n",
            "Iteration 14334, loss = 0.27192847\n",
            "Iteration 14335, loss = 0.27191845\n",
            "Iteration 14336, loss = 0.27190768\n",
            "Iteration 14337, loss = 0.27189637\n",
            "Iteration 14338, loss = 0.27189171\n",
            "Iteration 14339, loss = 0.27188234\n",
            "Iteration 14340, loss = 0.27189113\n",
            "Iteration 14341, loss = 0.27187139\n",
            "Iteration 14342, loss = 0.27187003\n",
            "Iteration 14343, loss = 0.27188112\n",
            "Iteration 14344, loss = 0.27187017\n",
            "Iteration 14345, loss = 0.27191130\n",
            "Iteration 14346, loss = 0.27181704\n",
            "Iteration 14347, loss = 0.27186501\n",
            "Iteration 14348, loss = 0.27178363\n",
            "Iteration 14349, loss = 0.27185599\n",
            "Iteration 14350, loss = 0.27180570\n",
            "Iteration 14351, loss = 0.27180039\n",
            "Iteration 14352, loss = 0.27179218\n",
            "Iteration 14353, loss = 0.27181046\n",
            "Iteration 14354, loss = 0.27178527\n",
            "Iteration 14355, loss = 0.27179059\n",
            "Iteration 14356, loss = 0.27177334\n",
            "Iteration 14357, loss = 0.27179279\n",
            "Iteration 14358, loss = 0.27177439\n",
            "Iteration 14359, loss = 0.27178787\n",
            "Iteration 14360, loss = 0.27180524\n",
            "Iteration 14361, loss = 0.27177230\n",
            "Iteration 14362, loss = 0.27173717\n",
            "Iteration 14363, loss = 0.27172905\n",
            "Iteration 14364, loss = 0.27175267\n",
            "Iteration 14365, loss = 0.27169845\n",
            "Iteration 14366, loss = 0.27168597\n",
            "Iteration 14367, loss = 0.27169037\n",
            "Iteration 14368, loss = 0.27168826\n",
            "Iteration 14369, loss = 0.27167538\n",
            "Iteration 14370, loss = 0.27164713\n",
            "Iteration 14371, loss = 0.27168104\n",
            "Iteration 14372, loss = 0.27161967\n",
            "Iteration 14373, loss = 0.27166103\n",
            "Iteration 14374, loss = 0.27159853\n",
            "Iteration 14375, loss = 0.27170576\n",
            "Iteration 14376, loss = 0.27161745\n",
            "Iteration 14377, loss = 0.27158517\n",
            "Iteration 14378, loss = 0.27157623\n",
            "Iteration 14379, loss = 0.27154443\n",
            "Iteration 14380, loss = 0.27162532\n",
            "Iteration 14381, loss = 0.27156154\n",
            "Iteration 14382, loss = 0.27152994\n",
            "Iteration 14383, loss = 0.27157988\n",
            "Iteration 14384, loss = 0.27148092\n",
            "Iteration 14385, loss = 0.27146984\n",
            "Iteration 14386, loss = 0.27145088\n",
            "Iteration 14387, loss = 0.27142928\n",
            "Iteration 14388, loss = 0.27143551\n",
            "Iteration 14389, loss = 0.27144350\n",
            "Iteration 14390, loss = 0.27144607\n",
            "Iteration 14391, loss = 0.27144199\n",
            "Iteration 14392, loss = 0.27144149\n",
            "Iteration 14393, loss = 0.27145519\n",
            "Iteration 14394, loss = 0.27145627\n",
            "Iteration 14395, loss = 0.27149241\n",
            "Iteration 14396, loss = 0.27143578\n",
            "Iteration 14397, loss = 0.27141957\n",
            "Iteration 14398, loss = 0.27142357\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(10, 5, 3, 2), learning_rate_init=0.0001,\n",
              "              max_iter=20000, random_state=20, tol=1e-05,\n",
              "              validation_fraction=0.2, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-10 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-10 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-10 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-10 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-10 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-10 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-10 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-10 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-10 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-10 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-10 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-10 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(10, 5, 3, 2), learning_rate_init=0.0001,\n",
              "              max_iter=20000, random_state=20, tol=1e-05,\n",
              "              validation_fraction=0.2, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(10, 5, 3, 2), learning_rate_init=0.0001,\n",
              "              max_iter=20000, random_state=20, tol=1e-05,\n",
              "              validation_fraction=0.2, verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(modelo.loss_curve_)\n",
        "plt.xlabel('poca')\n",
        "plt.ylabel('Loss')"
      ],
      "metadata": {
        "id": "c_jOfFs6pz8A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "fd4565e4-28f6-409b-c4bb-973c93c0a55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Dados de treinamento')\n",
        "\n",
        "predicao = modelo.predict(X_train)\n",
        "\n",
        "print(confusion_matrix(y_train,predicao))\n",
        "print(classification_report(y_train,predicao))"
      ],
      "metadata": {
        "id": "ie7Czj49dESt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761c4eea-af95-4694-fe54-16c46745f809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados de treinamento\n",
            "[[40  4]\n",
            " [ 9 97]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Not Placed       0.82      0.91      0.86        44\n",
            "      Placed       0.96      0.92      0.94       106\n",
            "\n",
            "    accuracy                           0.91       150\n",
            "   macro avg       0.89      0.91      0.90       150\n",
            "weighted avg       0.92      0.91      0.91       150\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Dados de teste')\n",
        "\n",
        "predicao = modelo.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test,predicao))\n",
        "print(classification_report(y_test,predicao))"
      ],
      "metadata": {
        "id": "L3UPD1qQqmCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b71446-124a-4a9e-9585-46cdd6ba129c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados de teste\n",
            "[[17  6]\n",
            " [ 7 35]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Not Placed       0.71      0.74      0.72        23\n",
            "      Placed       0.85      0.83      0.84        42\n",
            "\n",
            "    accuracy                           0.80        65\n",
            "   macro avg       0.78      0.79      0.78        65\n",
            "weighted avg       0.80      0.80      0.80        65\n",
            "\n"
          ]
        }
      ]
    }
  ]
}